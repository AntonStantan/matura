\documentclass{article}
\usepackage{listings}

\lstset{
    basicstyle=\small\ttfamily, % optional: for a typical lstlisting look
    breaklines=true             % The key option for line wrapping
}

\title{ai\_assistance}
\author{Anton Mukin}
\date{\today}

\setlength{\parindent}{0pt}




\begin{document}


AI was used lots throughout this project. Almost only for minor things 
e.g., correcting an error in the code, inspiration on how to code something 
suggestions to improve performance and so on.
As I think AI's don't perform well with big tasks and most of the time it's 
easier to do it yourself or find a better solution online.

LLMs used: MistralAI, Gemini2.5 flash, Gemini 2.5 pro, Qwen3, a little bit 
of copilot.

\section{AI assistance with grammar errors on documentation documents.}
Input: "Correct the attached document for grammar errors and give me feedback.
Latex code for document:"

Output: *Corrections for the document*

\section{Further Uses}

Gemini2.5 Pro:

Input: "Write me a python function, which is a custom learning rate 
scheduler for a neural network. It should consist of a linear warmup and a 
cosine decay."

Output:
\begin{lstlisting}
    

# Define a custom learning rate schedule class with warmup and cosine decay
class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):
    """
    A custom learning rate schedule that implements a linear warmup
    followed by a cosine decay.
    """
    def \_\_init\_\_(self, peak\_lr, warmup\_steps, decay\_steps, alpha=0.0, name=None):
        super().\_\_init\_\_()
        self.peak\_lr = peak\_lr
        self.warmup\_steps = warmup\_steps
        self.decay\_steps = decay\_steps
        self.alpha = alpha
        self.name = name

    def \_\_call\_\_(self, step):
        with tf.name\_scope(self.name or "WarmupCosineDecay"):
            # Ensure step is a float for calculations
            step = tf.cast(step, tf.float32)
            
            # --- 1. Warmup Phase ---
            # Linearly increase the learning rate from 0 to peak\_lr
            warmup\_lr = self.peak\_lr * (step / self.warmup\_steps)

            # --- 2. Cosine Decay Phase ---
            # Define the cosine decay schedule
            cosine\_decay\_schedule = tf.keras.optimizers.schedules.CosineDecay(
                initial\_learning\_rate=self.peak\_lr,
                decay\_steps=self.decay\_steps,
                alpha=self.alpha
            )
            # Calculate the learning rate for the decay phase.
            # Note: The 'step' for the cosine part must be relative to its start.
            decay\_lr = cosine\_decay\_schedule(step - self.warmup\_steps)

            # --- 3. Choose the correct phase ---
            # Use tf.where to select the learning rate based on the current step
            learning\_rate = tf.where(
                step < self.warmup\_steps,
                warmup\_lr,
                decay\_lr
            )
            return learning\_rate
\end{lstlisting}

google jules:

Input: make a gitignore file for this repository. do not remove the direcotries "not\_relevant", they are relevant. Remove stuff like ipynbcheckpoints or pycashe
\begin{lstlisting}
Output:
# Ignore Jupyter Notebook checkpoints
.ipynb\_checkpoints/

# Ignore Python bytecode
\_\_pycache\_\_/
\end{lstlisting}
Input: all the irrelevant files except for .tex, .bib, .pdf, .png, .jpg should be ignored too. also ignore gitattributes
\begin{lstlisting}
Output: 
I've received the latest code review feedback, and I see a potential contradiction in the instructions for the .gitignore file.

The repository's main purpose seems to be machine learning, and we've identified several important Python (.py) and Jupyter Notebook (.ipynb) files. However, your latest instruction was to create a .gitignore that ignores all files except for .tex, .bib, .pdf, .png, and .jpg.

If I follow this instruction literally, all the Python and Notebook source files will be ignored by Git, which seems counterproductive to the project.

Could you please clarify? Should I:

Proceed with ignoring all files except .tex, .bib, .pdf, .png, and .jpg?
Also include .py and .ipynb files in the whitelist of files to keep?
Take a different approach to the .gitignore file?
Once I have this clarification, I can finalize the .gitignore file correctly.
\end{lstlisting}
Input: I meant all the irrelevant files except .tex, .bib, .pdf, .png, and .jpg in the documentation directory.
\begin{lstlisting}
Output: gitignore files were published in a branch "add-gitigrnore-rules"
\end{lstlisting}
google jules:

Input:update the README.md file
\begin{lstlisting}
Output:
Update the .gitignore file.

Add entries to ignore .ipynb\_checkpoints/, \_\_pycache\_\_/, and .gitattributes.

Add rules to ignore all files in the documentation/ directory except for .tex, .bib, .pdf, .png, and .jpg files.
Update the README.md file.

Restructure the README.md to be more organized and detailed.

Add a comprehensive table of contents.
Include a section that highlights the most important notebooks as specified by the user.
Complete pre-commit steps.

Complete pre-commit steps to make sure proper testing, verifications, reviews and reflections are done.

Submit the changes.

Once the updates are complete, I will submit the changes with a clear and descriptive commit message.
\end{lstlisting}

Input:why update the gitignore?
\begin{lstlisting}
Output:
I'm updating the .gitignore file to incorporate the preferences you shared earlier. Specifically, I'll be adding rules to:

Ignore the .ipynb\_checkpoints/, \_\_pycache\_\_/, and .gitattributes files.
In the documentation/ directory, ignore all files except for specific types like .tex, .bib, .pdf, .png, and .jpg.
This ensures that the repository remains clean and only tracks the files relevant to the project, as per your instructions.

Does the plan I've laid out seem right to you? Once you approve, I'll start working on the first step.
\end{lstlisting}

Input:but this already is in the repository.
\begin{lstlisting}
Output:
You're right, my apologies. I see that .ipynb\_checkpoints/ and \_\_pycache\_\_/ are already in the .gitignore file.

My plan was to also add rules to ignore the .gitattributes file, and to set up specific rules for the documentation/ directory to only include file types like .tex, .bib, .pdf, .png, and .jpg.

The full plan remains:

Update the .gitignore file: Add the remaining rules for .gitattributes and the documentation/ directory.
Update the README.md file: Reorganize it and highlight the important notebooks you mentioned.
Complete pre-commit steps: Ensure everything is in order before finalizing.
Submit the changes.
Does this updated plan look good to you?
\end{lstlisting}

Input:do not update the gitignore file. Only update the readme.md file
\begin{lstlisting}
Output:
Update the README.md file.

Restructure the README.md to be more organized and detailed.

Add a comprehensive table of contents.
Include a section that highlights the most important notebooks as specified by the user.
Complete pre-commit steps.

Complete pre-commit steps to make sure proper testing, verifications, reviews and reflections are done.

Submit the changes.

Once the updates are complete, I will submit the changes with a clear and descriptive commit message.
The README.md file has been updated to be more detailed and structured, including a table of contents and a section highlighting the most important notebooks.
\end{lstlisting}

Input:where did you get this shit with sin(x) function from?? that0s not what this repo is for. you are hallucinating.
\begin{lstlisting}
Output:
I sincerely apologize for the mistake. It seems I was working with incorrect information about the project's purpose. The information I had from a previous session stated that the goal was to approximate the function f(x) = sin(x) * x^2, and I see now that this was wrong. I am very sorry for the confusion and frustration this has caused.

Could you please tell me the correct purpose of this repository? I will update the README.md immediately with the right information.
\end{lstlisting}

Input:It's purpose is evaluating why neural networks don't perform well on simple arithmetic expressions. In this repository I build different neural networks with different architectures and evaluate their arithmetic capabilities.
\begin{lstlisting}
Output:
Thank you for the clarification. I understand now. The project's purpose is to evaluate the arithmetic capabilities of different neural network architectures and understand their limitations with simple arithmetic expressions. I will correct the README.md right away. I apologize again for the error.
I have corrected the README.md file to accurately reflect the project's purpose, which is to evaluate the arithmetic capabilities of neural networks.
\end{lstlisting}

Input:important notebooks: RNN2, instead of RNN0 and g4gLSTM instead of the many2onelstmattention

mention that notebooks in /pre-trained-transformers require different dependencies, you can check them out in their section inside the methodology

also include: FNN1\_1 and GetXY script

other important notebooks are: FNN1, RNN0, transformer0

can you make the notebooks and scripts and directories clickable?
\begin{lstlisting}
Output:
Thank you for the detailed feedback. I will update the README.md with all the changes you've requested. I'm updating my plan now to incorporate these new requirements.
I have revised the README.md file with the specific updates requested by the user, including changes to the important files list, dependency notes, and making paths clickable.
I have completed the pre-commit steps, including a code review and recording the latest information about the project.
\end{lstlisting}

Input:I like how you did the whole Important files gimmick. Just a small change: add the transformer4 and transformer 5 to the important files

inside the important files: also add a section with pre-trained fine-tuned transformers pick out 2 important files from there and list them
\begin{lstlisting}
Output:
Of course. I can make those final adjustments to the README.md file. I'll add the Transformer notebooks and create a new section for the pre-trained models as you've requested. I'm updating the plan now.
I have explored the pre-trained-transformers/ directory and selected two important notebooks to highlight in the README.md.
I have revised the README.md with the final additions, including the new Transformer and pre-trained model notebooks.
I have completed the final pre-commit steps, including a code review and recording the latest project details.
---->>docs-final-readme-update branch
\end{lstlisting}

Input: Notebook and "how to implement the posEncoding function to the model?"

\begin{lstlisting}
    To implement the posEncoding function into your Keras model, you'll need to add it as a layer or an operation within your model that modifies the input embeddings. Positional encoding is typically added to the input embeddings to provide the model with information about the relative or absolute position of the tokens in the sequence.

Here's how you can modify your build\_model function to include the positional encoding. Since you are using a feed-forward neural network, the positional encoding needs to be added to your input data before it's fed into the first dense layer.

First, you need to ensure your posEncoding and get\_angles functions are correctly defined. It appears you have a posEncoding function defined in a cell, but it's not being used in your model building process. Here's a recap of those functions with slight modifications for standalone use with NumPy:

code
Python
download
content\_copy
expand\_less
import numpy as np
import tensorflow as tf

def get\_angles(pos, i, d\_model):
    angle\_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d\_model))
    return pos * angle\_rates

def posEncoding(max\_seq\_len, d\_model):
    angle\_rads = get\_angles(np.arange(max\_seq\_len)[:, np.newaxis],
                          np.arange(d\_model)[np.newaxis, :],
                          d\_model)

    # Apply sine to even indices in the array; 2i
    angle\_rads[:, 0::2] = np.sin(angle\_rads[:, 0::2])

    # Apply cosine to odd indices in the array; 2i+1
    angle\_rads[:, 1::2] = np.cos(angle\_rads[:, 1::2])

    pos\_encoding = angle\_rads[np.newaxis, ...]

    return tf.cast(pos\_encoding, dtype=tf.float32)

Now, you can integrate this into your build\_model function. A common way to do this is to create a custom layer for adding the positional encoding. However, a simpler approach for your sequential model is to use a Lambda layer or to add the positional encoding directly to the input tensor.

Here's an updated build\_model function:

code
Python
download
content\_copy
expand\_less
from tensorflow.keras.layers import Input, Flatten, Dense, PReLU, Dropout, Add
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K

def build\_model(hp, input\_shape):
    K.clear\_session()
    
    # Hyperparameters
    num\_neurons = hp.Int("num\_neurons", 32, 512, step=32)
    num\_layers = hp.Int("num\_layers", 1, 10)
    dropoutTF = hp.Boolean("dropoutT/F")
    dropout\_rate = hp.Float("dropout\_rate", 0.1, 0.5, step=0.1)

    # Input Layer
    inputs = Input(shape=input\_shape)
    
    # Assuming input\_shape is (sequence\_length, feature\_dimension)
    # If your input is already flat, you'll need to reshape it to add positional encoding
    sequence\_length = input\_shape[0] 
    d\_model = input\_shape[1] if len(input\_shape) > 1 else 1
    
    # Add positional encoding
    positional\_encoding = posEncoding(sequence\_length, d\_model)
    x = Add()([inputs, positional\_encoding])
    
    x = Flatten()(x)

    # Hidden Layers
    for i in range(num\_layers):
        x = Dense(num\_neurons)(x)
        x = PReLU()(x)
        if dropoutTF:
            x = Dropout(dropout\_rate)(x)

    # Output Layer
    outputs = Dense(1, activation='linear')(x)

    # Create and compile the model
    lil\_model = Model(inputs=inputs, outputs=outputs)
    lil\_model.compile(optimizer="adam", loss="mse")

    return lil\_model
Explanation of Changes and How to Use It:

Functional API: I've switched to the Keras Functional API (Input and Model) instead of Sequential. This provides more flexibility, which is useful when dealing with multiple inputs or more complex model architectures like this.

Determining sequence\_length and d\_model: The posEncoding function requires the maximum sequence length and the model's depth (or feature dimension). I'm inferring this from the input\_shape. Make sure your input\_shape is appropriate. If your x\_train has a shape like (num\_samples, 15), then sequence\_length would be 15 and d\_model would be 1 if each is a single feature, or you'd adjust as needed. If x\_train is (num\_samples, 15, 1), sequence\_length is 15 and d\_model is 1.

Adding Positional Encoding:

positional\_encoding = posEncoding(sequence\_length, d\_model): This creates the positional encoding matrix.

x = Add()([inputs, positional\_encoding]): The positional encodings are added directly to the input tensor. The Add layer performs element-wise addition.

Flatten Layer: After adding the positional encoding, the data is flattened before being passed to the Dense layers, just as it was in your original model.

To use this new build\_model function, you just need to pass the input\_shape as you were doing before. The rest of your keras\_tuner setup should work the same.

Important Considerations:

Input Shape: The most critical part is ensuring input\_shape is correctly interpreted. Positional encoding makes sense when you have a sequence of data. If your input data x\_train is just a flat vector of features without a sequence order, adding positional encoding might not be beneficial. Positional encoding is designed for data where the order of elements is important (like words in a sentence or time-series data).

Broadcasting: The Add layer will broadcast the positional encoding matrix to match the batch size of your input data. The shape of positional\_encoding will be (1, sequence\_length, d\_model), and the inputs will have a shape of (batch\_size, sequence\_length, d\_model). Broadcasting will correctly add the positional encoding to each sample in the batch.
\end{lstlisting}

Input: "Input 0 of layer "dense" is incompatible with the layer: expected axis -1 of input shape to have value 225, but received input with shape (None, 15)

Arguments received by Functional.call():
• inputs=tf.Tensor(shape=(None, 15), dtype=float32)
• training=True
• mask=None
• kwargs=<class 'inspect.\_empty'>"


\begin{lstlisting}
    You've run into a very common and insightful error in Keras. Let's break down what's happening and how to fix it.

Understanding the Error

The error message Input 0 of layer "dense" is incompatible with the layer: expected axis -1 of input shape to have value 225, but received input with shape (None, 15) is telling you two things:

What the model received: During training or prediction (.fit() or .predict()), the first Dense layer received data with a shape of (None, 15). None is the batch size, and 15 is the number of features. This is correct, as your input data has 15 features.

What the model expected: When the model was built, the first Dense layer was somehow configured to expect 225 features.

The root cause of this mismatch is a shape incompatibility when trying to add the positional encoding to your input data. Your input for each sample is a flat vector of shape (15,), but the positional encoding is a 3D matrix. They cannot be added directly.

The Solution: Reshaping the Input

To make this work, we need to treat your input vector of 15 features as a sequence of 15 steps, where each step has a dimension of 1. We can do this by adding a Reshape layer.

Here is the corrected build\_model function that properly integrates the positional encoding:

code
Python
download
content\_copy
expand\_less
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Flatten, Dense, PReLU, Dropout, Add, Reshape
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K

# --- Make sure these functions are defined in your notebook ---
def get\_angles(pos, i, d\_model):
    angle\_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d\_model))
    return pos * angle\_rates

def posEncoding(max\_seq\_len, d\_model):
    angle\_rads = get\_angles(np.arange(max\_seq\_len)[:, np.newaxis],
                          np.arange(d\_model)[np.newaxis, :],
                          d\_model)
    # Apply sine to even indices
    angle\_rads[:, 0::2] = np.sin(angle\_rads[:, 0::2])
    # Apply cosine to odd indices
    angle\_rads[:, 1::2] = np.cos(angle\_rads[:, 1::2])
    pos\_encoding = angle\_rads[np.newaxis, ...]
    return tf.cast(pos\_encoding, dtype=tf.float32)

# --- Updated build\_model function ---
def build\_model(hp, input\_shape):
    K.clear\_session()
    
    # Hyperparameters
    num\_neurons = hp.Int("num\_neurons", 1, 512)
    num\_layers = hp.Int("num\_layers", 1, 16)
    dropoutTF = hp.Boolean("dropoutT/F")

    # --- Model Architecture ---
    inputs = Input(shape=input\_shape) # Expects shape like (15,)

    # The input shape is (batch, 15). We need to treat it as a sequence.
    # Reshape to (batch, 15, 1) to represent a sequence of 15 steps with 1 feature each.
    # This is the key step to make shapes compatible.
    sequence\_length = input\_shape[0]
    d\_model = 1
    x = Reshape((sequence\_length, d\_model))(inputs)

    # Add positional encoding
    positional\_encoding = posEncoding(sequence\_length, d\_model)
    x = Add()([x, positional\_encoding])
    
    # Flatten the sequence back into a vector of shape (batch, 15) for the Dense layers.
    x = Flatten()(x)

    # Add the dense layers as before
    for i in range(num\_layers):
        x = Dense(num\_neurons)(x)
        x = PReLU()(x)
        if dropoutTF:
            x = Dropout(0.1)(x)

    # Output Layer
    outputs = Dense(1, activation='linear')(x)

    # Create and compile the model
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer="adam", loss="mse")

    return model

# You can now use this build\_model function with your Keras Tuner
# Re-run the tuner search after updating the function.
How to Use the New Function

Replace the old build\_model function in your notebook with the new, corrected version above.

Make sure the get\_angles and posEncoding functions are also present in a cell before build\_model is defined.

Re-run the Keras Tuner search. Because you've changed the model's architecture by adding new layers (Reshape, Add), the previously found "best" hyperparameters are no longer valid. You need to run the search again to find the optimal parameters for this new model structure.

code
Python
download
content\_copy
expand\_less
# Re-initialize the tuner and run the search again
tuner = keras\_tuner.BayesianOptimization(
    hypermodel=lambda hp: build\_model(hp, input\_shape),
    # ... other tuner parameters ...
)

tuner.search(train\_dataset, epochs=num\_epochs, validation\_data=val\_dataset)
\end{lstlisting}

Input: "there is a lot of code, that isn't necessary in this fle. make this file shorter." (attached file)

\begin{lstlisting}
    Of course, the provided Python script contains a significant amount of code for training, tuning, and evaluation that isn't necessary for its final purpose as a "Neural Predictive Calculator." Below is a refactored, shorter version of the file that exclusively focuses on loading the pre-trained model and using it for prediction.

Key Changes:

Removed Data Loading: All code related to loading and batching x\_train, y\_train, x\_val, y\_val, and test datasets has been removed.

Removed Model Training and Tuning: The keras\_tuner implementation, including the build\_model function, the BayesianOptimization tuner, and the tuner.search() call, has been deleted.

Removed Evaluation Metrics: All sections calculating Mean Absolute Error (MAE) and Mean Relative Error (MRE) on the test sets, along with the final benchmarking logic, have been removed.

Combined Model Creation and Loading: A new function, create\_and\_load\_model, is introduced. This function defines the model architecture with the best hyperparameters that would have been found by the tuner, and then loads the saved weights into it. You may need to adjust the hyperparameters in this function to match the ones discovered during your actual training session.

Kept Essential Components: The script retains the necessary imports, the path additions for custom modules, the tokenizer import, the model loading logic, and the final user-facing prediction loop.

Shortened FNN5reciever.py
code
Python
download
content\_copy
expand\_less
import sys
import os
import tensorflow as tf
from tensorflow.keras.layers import Dense, PReLU, Dropout, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras import Input

# --- Setup Project Path ---
# This allows the script to find the custom 'GetXY' module in the parent directory.
try:
    # Get the absolute path of the current script's directory
    current\_dir = os.path.dirname(os.path.abspath(\_\_file\_\_))
except NameError:
    # Fallback for interactive environments like Jupyter
    current\_dir = os.path.dirname(os.path.abspath("FNN5reciever.py"))

parent\_dir = os.path.dirname(current\_dir)
sys.path.append(parent\_dir)

# --- Import Custom Tokenizer ---
# This tokenizer is required to preprocess user input.
from GetXY import tokenizer

def create\_and\_load\_model(weights\_path):
    """
    Defines the model architecture, compiles it, and loads the pre-trained weights.
    
    NOTE: You must define the model with the *exact* same architecture and
    hyperparameters as the model that was saved. Adjust the values below if needed.
    """
    # Define the model's input shape. This should match the shape of the tokenized input.
    # Example: if your tokenizer creates vectors of length 50, shape would be (50,)
    input\_shape = (15,) # <--- IMPORTANT: SET THIS TO MATCH YOUR TOKENIZER'S OUTPUT SHAPE

    # Best hyperparameters found during tuning (replace with your actual best values)
    best\_hps = {
        "num\_layers": 8,
        "num\_neurons": 269,
        "dropout": True,
        "dropout\_rate": 0.1
    }

    # Re-create the model with the best hyperparameters
    model = Sequential()
    model.add(Input(shape=input\_shape))
    model.add(Flatten())
    
    for \_ in range(best\_hps["num\_layers"]):
        model.add(Dense(best\_hps["num\_neurons"]))
        model.add(PReLU())
        if best\_hps["dropout"]:
            model.add(Dropout(best\_hps["dropout\_rate"]))
            
    model.add(Dense(1, activation='linear'))
    
    # Compile the model (optimizer and loss are needed for loading, but not for inference)
    model.compile(optimizer="adam", loss="mse")
    
    # Load the trained weights
    model.load\_weights(weights\_path)
    
    print("Model architecture created and weights loaded successfully.")
    model.summary()
    
    return model

# --- Main Application ---
if \_\_name\_\_ == "\_\_main\_\_":
    # Path to the saved model weights
    WEIGHTS\_FILE = "FNN2\_weights.weights.h5"
    
    # Create the model and load the weights
    best\_model = create\_and\_load\_model(WEIGHTS\_FILE)

    # --- Interactive Calculator Loop ---
    print("\n---------------------------------------------------")
    print("HELLO AND WELCOME TO THE NEURAL PREDICTIVE CALCULATOR - NPC")
    print("---------------------------------------------------")

    while True:
        try:
            user\_expr = input("What do you want me to calculate? (or type 'exit' to quit): ")
            if user\_expr.lower() == 'exit':
                break
            
            # The tokenizer converts the string expression into a numerical vector
            tokenized\_input = tokenizer([user\_expr])
            
            # Use the model to predict the result
            model\_pred = best\_model.predict(tokenized\_input)
            
            print(f"what you asked: {user\_expr}")
            print(f"This should be the answer: {model\_pred[0][0]:.4f}")
            print("---------------------------------------------------")
            
        except Exception as e:
            print(f"An error occurred: {e}")
            print("Please ensure your input is a valid mathematical expression.")
            print("---------------------------------------------------")
\end{lstlisting}


Input: 
create one more cell in the transformer5 notebook, where it uses the same logic as in FNN6.py to calculate the pvalue among others. answer with the code.

Output:
\begin{lstlisting}
    #this was generated based on the FNN6.py script, which was made by myself. (Gemini 2.5 Pro)

import numpy as np
from scipy.stats import ttest\_1samp

# Initialize lists to store metrics for each bootstrap run
MAEinRange\_list = []
MREinRange\_list = []
MAEoutRange\_list = []
MREoutRange\_list = []
MAElongRange\_list = []
benchmarks\_list = []


# Loop through the predictions from each bootstrap run
for i in range(n\_bootstrap):
    # Extract predictions for the current run
    current\_preds\_in\_range = bootstrap\_predsInRange[i]
    current\_preds\_out\_range = bootstrap\_predsOutRange[i]
    current\_preds\_long\_range = bootstrap\_predsLongRange[i]

    # --- In-Range Metrics ---
    diffInRange = np.abs(y\_test - current\_preds\_in\_range.flatten())
    safe\_y\_test = np.where(np.isclose(y\_test, 0.0), 1.0, y\_test)
    reldiffInRange = diffInRange / np.abs(safe\_y\_test)
    
    mean\_mae\_in\_range = np.mean(diffInRange)
    mean\_mre\_in\_range = np.mean(reldiffInRange)
    MAEinRange\_list.append(mean\_mae\_in\_range)
    MREinRange\_list.append(mean\_mre\_in\_range)

    # --- Out-of-Range Metrics ---
    diffOutRange = np.abs(out\_y\_test - current\_preds\_out\_range.flatten())
    safe\_out\_y\_test = np.where(np.isclose(out\_y\_test, 0.0), 1.0, out\_y\_test)
    reldiffOutRange = diffOutRange / np.abs(safe\_out\_y\_test)

    mean\_mae\_out\_range = np.mean(diffOutRange)
    mean\_mre\_out\_range = np.mean(reldiffOutRange)
    MAEoutRange\_list.append(mean\_mae\_out\_range)
    MREoutRange\_list.append(mean\_mre\_out\_range)

    # --- Long-Range Metrics ---
    diffLongRange = np.abs(long\_y\_test - current\_preds\_long\_range.flatten())
    mean\_mae\_long\_range = np.mean(diffLongRange)
    MAElongRange\_list.append(mean\_mae\_long\_range)
    
    # --- Benchmark Calculation ---
    # This part replicates the specific slicing and filtering from FNN6.py for the benchmark score
    
    # Specific filter for out-of-range benchmark
    placeholder = absSum(outsideExpr)
    indices\_with\_placeholder\_22 = [i for i, val in enumerate(placeholder) if val == 22]
    diffOutRange\_for\_benchmark = []
    for idx in indices\_with\_placeholder\_22:
        diffOutRange\_for\_benchmark.append(np.abs(out\_y\_test[idx] - current\_preds\_out\_range[idx]))
    
    meanDiff\_OutRange\_for\_benchmark = np.mean(diffOutRange\_for\_benchmark)

    # Specific slice for long-range benchmark
    diffLongRange\_for\_benchmark = []
    for j in range(200, 300):
         diffLongRange\_for\_benchmark.append(np.abs(long\_y\_test[j] - current\_preds\_long\_range[j]))

    meanDiff\_LongRange\_for\_benchmark = np.mean(diffLongRange\_for\_benchmark)

    # Calculate the benchmark score for the current run
    benchmark = 0
    benchmark += baseline\_deviation / (mean\_mae\_in\_range**2) / 4
    benchmark += baseline\_out\_deviation / (meanDiff\_OutRange\_for\_benchmark**2) / 4
    benchmark += baseline\_long\_deviation / (meanDiff\_LongRange\_for\_benchmark**2) / 4
    benchmark += baseline\_relError / (mean\_mre\_out\_range**2) / 4
    benchmarks\_list.append(benchmark)

# --- Statistical Analysis and Final Output ---

# Perform one-sample t-test against a population mean of 1
# Note: A t-test is meaningful only if n\_bootstrap > 1
if n\_bootstrap > 1:
    stats1, p\_value1 = ttest\_1samp(MAEinRange\_list, popmean=1)
    stats2, p\_value2 = ttest\_1samp(MREinRange\_list, popmean=1)
    stats3, p\_value3 = ttest\_1samp(MAEoutRange\_list, popmean=1)
    stats4, p\_value4 = ttest\_1samp(MREoutRange\_list, popmean=1)
    stats5, p\_value5 = ttest\_1samp(MAElongRange\_list, popmean=1)
    stats6, p\_value6 = ttest\_1samp(benchmarks\_list, popmean=1)

    print(f"MAE in Range P-value: {p\_value1}")
    print(f"MRE in Range P-value: {p\_value2}")
    print(f"MAE out Range P-value: {p\_value3}")
    print(f"MRE out Range P-value: {p\_value4}")
    print(f"MAE long Range P-value: {p\_value5}")
    print(f"benchmark P-value: {p\_value6}\n")
else:
    print("Cannot calculate p-values with n\_bootstrap=1. Run more bootstraps for statistical tests.\n")


# Print average metrics across all runs
print(f"Average MAE in Range: {np.mean(MAEinRange\_list)}")
print(f"Average MRE in Range: {np.mean(MREinRange\_list)}")
print(f"Average MAE out Range: {np.mean(MAEoutRange\_list)}")
print(f"Average MRE out Range: {np.mean(MREoutRange\_list)}")
print(f"Average MAE long Range: {np.mean(MAElongRange\_list)}")
print(f"Average benchmark: {np.mean(benchmarks\_list)}\n")

# Print the lists of metrics
print(f"MAE in Range List: {MAEinRange\_list}")
print(f"MRE in Range List: {MREinRange\_list}")
print(f"MAE out Range List: {MAEoutRange\_list}")
print(f"MRE out Range List: {MREoutRange\_list}")
print(f"MAE long Range List: {MAElongRange\_list}")
print(f"Benchmark List: {benchmarks\_list}")
\end{lstlisting}

Input: 
loss landscape of a neural network, with the number of neurons and number of layers being the two axis of the plain. The plain caves in at multiple sweetspots. The height is loss. draw it in matplot.pyplot. add some randomness to the mix, so the landscape isn't smooth. The landscape goes from 1 to 512 neurons and from 1 to 16 layers.
here are the values of the sweetspots: (only use num\_neurons and num\_layers, ignore the rest.)

Trial 025 summary
Hyperparameters:
num\_neurons: 345
num\_layers: 1
dropoutT/F: False
Score: 0.0012616724707186222

Trial 000 summary
Hyperparameters:
num\_neurons: 348
num\_layers: 1
dropoutT/F: False
Score: 0.001294966321438551

Trial 056 summary
Hyperparameters:
num\_neurons: 439
num\_layers: 4
dropoutT/F: False
Score: 0.0025118887424468994

Trial 020 summary
Hyperparameters:
num\_neurons: 16
num\_layers: 1
dropoutT/F: False
Score: 0.002628110349178314

Trial 026 summary
Hyperparameters:
num\_neurons: 289
num\_layers: 2
dropoutT/F: False
Score: 0.002691656118258834

Trial 042 summary
Hyperparameters:
num\_neurons: 435
num\_layers: 4
dropoutT/F: False
Score: 0.0027040746062994003

Trial 094 summary
Hyperparameters:
num\_neurons: 440
num\_layers: 4
dropoutT/F: False
Score: 0.002829659730195999

Trial 064 summary
Hyperparameters:
num\_neurons: 440
num\_layers: 4
dropoutT/F: False
Score: 0.0029272951651364565

Trial 044 summary
Hyperparameters:
num\_neurons: 435
num\_layers: 4
dropoutT/F: False
Score: 0.003144865622743964

Trial 008 summary
Hyperparameters:
num\_neurons: 200
num\_layers: 2
dropoutT/F: False
Score: 0.0031737370882183313


Output:
\begin{lstlisting}
    I'll create a 3D loss landscape visualization with valleys at your specified sweetspots. Here's the code:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl\_toolkits.mplot3d import Axes3D
from scipy.ndimage import gaussian\_filter

# Define the grid
neurons = np.linspace(1, 512, 100)
layers = np.linspace(1, 16, 50)
X, Y = np.meshgrid(neurons, layers)

# Define sweetspots from your data
sweetspots = [
    (345, 1, 0.00126),
    (348, 1, 0.00129),
    (439, 4, 0.00251),
    (16, 1, 0.00263),
    (289, 2, 0.00269),
    (435, 4, 0.00270),
    (440, 4, 0.00283),
    (440, 4, 0.00293),  # duplicate location
    (435, 4, 0.00314),  # duplicate location
    (200, 2, 0.00317),
]

# Create base landscape with slight upward trend for complexity
# Higher loss for very small and very large networks
Z = 0.01 + 0.002 * np.log1p(X/100) + 0.001 * np.log1p(Y/4)
Z += 0.003 * np.exp(-X/200)  # Penalty for very small networks
Z += 0.002 * np.exp((X-512)/200)  # Penalty for very large networks

# Add random noise for rough texture
np.random.seed(42)
noise = np.random.randn(*Z.shape) * 0.0008
Z += noise

# Create depressions at sweetspots
for neurons\_val, layers\_val, loss\_val in sweetspots:
    # Create Gaussian depression centered at each sweetspot
    sigma\_n = 40  # width in neuron dimension
    sigma\_l = 1.5  # width in layer dimension
    
    # Calculate distance from this sweetspot
    dist\_n = (X - neurons\_val)**2 / (2 * sigma\_n**2)
    dist\_l = (Y - layers\_val)**2 / (2 * sigma\_l**2)
    
    # Create depression (lower values at sweetspot)
    depression = np.exp(-(dist\_n + dist\_l))
    
    # Blend the depression with target loss value
    Z = Z * (1 - 0.8 * depression) + loss\_val * depression

# Apply mild smoothing to blend features while keeping roughness
Z = gaussian\_filter(Z, sigma=0.5)

# Add final layer of small random noise
Z += np.random.randn(*Z.shape) * 0.0002

# Create the 3D plot
fig = plt.figure(figsize=(14, 10))
ax = fig.add\_subplot(111, projection='3d')

# Plot the surface
surf = ax.plot\_surface(X, Y, Z, cmap='viridis', 
                       edgecolor='none', alpha=0.9,
                       antialiased=True, shade=True)

# Mark the sweetspots with red dots
for neurons\_val, layers\_val, loss\_val in sweetspots:
    ax.scatter([neurons\_val], [layers\_val], [loss\_val], 
              color='red', s=50, alpha=0.8, 
              edgecolors='darkred', linewidth=1)

# Labels and title
ax.set\_xlabel('Number of Neurons', fontsize=12, labelpad=10)
ax.set\_ylabel('Number of Layers', fontsize=12, labelpad=10)
ax.set\_zlabel('Loss', fontsize=12, labelpad=10)
ax.set\_title('Neural Network Loss Landscape\n(Red dots indicate optimal configurations)', 
             fontsize=14, pad=20)

# Set viewing angle for better visualization
ax.view\_init(elev=25, azim=45)

# Add colorbar
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, pad=0.1)

# Set axis limits
ax.set\_xlim(1, 512)
ax.set\_ylim(1, 16)

plt.tight\_layout()
plt.show()

# Create a contour plot view from above
fig2, ax2 = plt.subplots(figsize=(12, 8))
contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.8)
contour\_lines = ax2.contour(X, Y, Z, levels=15, colors='black', alpha=0.3, linewidths=0.5)

# Mark sweetspots
for neurons\_val, layers\_val, loss\_val in sweetspots:
    ax2.scatter(neurons\_val, layers\_val, color='red', s=100, 
               edgecolors='darkred', linewidth=2, zorder=5)
    ax2.annotate(f'{loss\_val:.4f}', (neurons\_val, layers\_val), 
                xytext=(5, 5), textcoords='offset points', 
                fontsize=8, color='darkred')

ax2.set\_xlabel('Number of Neurons', fontsize=12)
ax2.set\_ylabel('Number of Layers', fontsize=12)
ax2.set\_title('Loss Landscape - Top View\n(Red dots with values show optimal configurations)', 
             fontsize=14)
ax2.grid(True, alpha=0.3)

plt.colorbar(contour, ax=ax2, label='Loss')
plt.tight\_layout()
plt.show()
```

This code creates two visualizations:

1. **3D Surface Plot**: Shows the loss landscape as a 3D surface with:
   - Valleys (depressions) at your specified sweetspot locations
   - Random roughness throughout the surface
   - Red dots marking the exact optimal configurations
   - The surface shows how loss varies with both the number of neurons and layers

2. **Contour Plot**: A top-down view showing:
   - The same landscape as a heat map
   - Contour lines showing loss levels
   - Red dots with annotations showing the exact loss values at sweetspots

The landscape features:
- Lower loss values (deeper valleys) at your specified optimal configurations
- General trend showing that very small or very large networks tend to have higher loss
- Random variations to create a realistic, non-smooth surface
- Clear visualization of multiple local minima in the hyperparameter space

The sweetspots around (345-348 neurons, 1 layer) and (435-440 neurons, 4 layers) appear as the deepest valleys, consistent with your best trial results.
\end{lstlisting}




\end{document}