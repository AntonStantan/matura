@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}}

@misc{cho2014propertiesneuralmachinetranslation,
      title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
      year={2014},
      eprint={1409.1259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.1259}, 
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@misc{geeksforgeeks_lstm,
  title = {Deep Learning Introduction to Long Short Term Memory},
  author = {GeeksforGeeks},
  year = {2025},
  month = {4},
  day = {5},
  howpublished = {\url{https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory}},
  note = {Last Updated: 2025-04-05},
  url = {https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory}
}

@misc{geeksforgeeks_gru,
  title = {Gated Recurrent Unit Networks},
  author = {GeeksforGeeks},
  year = {2025},
  month = {10},
  day = {9},
  howpublished = {\url{https://www.geeksforgeeks.org/machine-learning/gated-recurrent-unit-networks}},
  note = {Last Updated: 2025-10-09},
  url = {https://www.geeksforgeeks.org/machine-learning/gated-recurrent-unit-networks}
}

@misc{cristina_2023_bahdanau,
  title = {The Bahdanau Attention Mechanism},
  author = {Cristina, Stefania},
  year = {2023},
  month = {1},
  day = {6},
  howpublished = {\url{https://machinelearningmastery.com/the-bahdanau-attention-mechanism}},
  note = {Accessed: 2025-10-11},
  url = {https://machinelearningmastery.com/the-bahdanau-attention-mechanism}
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{Stryker_LLM,
  author = {Stryker, Cole},
  title = {What are large language models (LLMs)?},
  publisher = {IBM},
  url = {https://www.ibm.com/think/topics/large-language-models}
}

@misc{geeksforgeeks_2024,
title = {Large Language Models (LLMs) vs Transformers - GeeksforGeeks},
url = {https://www.geeksforgeeks.org/nlp/large-language-models-llms-vs-transformers},
journal = {GeeksforGeeks},
author = {GeeksforGeeks},
year = {2024},
month = {8}
}

@misc{srinivasan2024transformer,
  author       = {Srinivasan Anusha, Janani},
  title        = {Transformer Architecture --- The Backbone of LLMs},
  year         = {2024},
  month        = {12},
  publisher    = {Medium},
  url          = {https://jananithinks.medium.com/transformer-architecture-the-backbone-of-llms-1a3d085ca981},
  note         = {Accessed: 2025-10-12}
}

@misc{Bergmann_Fine_Tuning,
  author = {Bergmann, Dave},
  title = {What is Fine-Tuning?},
  publisher = {IBM},
  url = {https://www.ibm.com/think/topics/fine-tuning}
}

@misc{kim2021neuralsequencetogridmodulelearning,
      title={Neural Sequence-to-grid Module for Learning Symbolic Rules}, 
      author={Segwang Kim and Hyoungwook Nam and Joonyoung Kim and Kyomin Jung},
      year={2021},
      eprint={2101.04921},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.04921}, 
}

@inproceedings{inproceedings,
author = {Gawde, Rishikesh},
year = {2021},
month = {04},
pages = {},
title = {Image Caption Generation Methodologies}
}

@misc{emil2020what,
    author = {Emil},
    title = {What is the feedforward network in a transformer trained on?},
    year = {2020},
    month = {02},
    note = {Accessed: 2025-11-03},
    key = {emil2020what},
    url = {https://datascience.stackexchange.com/questions/68020/what-is-the-feedforward-network-in-a-transformer-trained-on}
}

@misc{kili2023,
  author       = {Lee, Kenny},
  title        = {Open-Sourced Training Datasets for Large Language Models (LLMs)},
  year         = {2023},
  publisher    = {Kili Technology},
  url          = {https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models},
  note         = {Accessed on 2025-11-03}
}

@misc{pascanu2013difficultytrainingrecurrentneural,
      title={On the difficulty of training Recurrent Neural Networks}, 
      author={Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
      year={2013},
      eprint={1211.5063},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1211.5063}, 
}

@article{87464e34-37bc-3288-807c-e421e5a0d7a6,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2331554},
 author = {Student},
 journal = {Biometrika},
 number = {1},
 pages = {1--25},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Probable Error of a Mean},
 urldate = {2025-11-09},
 volume = {6},
 year = {1908}
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}