@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}}

@misc{cho2014propertiesneuralmachinetranslation,
      title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
      year={2014},
      eprint={1409.1259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.1259}, 
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@misc{geeksforgeeks_lstm,
  title = {Deep Learning Introduction to Long Short Term Memory},
  author = {GeeksforGeeks},
  year = {2025},
  month = {4},
  day = {5},
  howpublished = {\url{https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory}},
  note = {Last Updated: 2025-04-05},
  url = {https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory}
}

@misc{geeksforgeeks_gru,
  title = {Gated Recurrent Unit Networks},
  author = {GeeksforGeeks},
  year = {2025},
  month = {10},
  day = {9},
  howpublished = {\url{https://www.geeksforgeeks.org/machine-learning/gated-recurrent-unit-networks}},
  note = {Last Updated: 2025-10-09},
  url = {https://www.geeksforgeeks.org/machine-learning/gated-recurrent-unit-networks}
}

@misc{cristina_2023_bahdanau,
  title = {The Bahdanau Attention Mechanism},
  author = {Cristina, Stefania},
  year = {2023},
  month = {1},
  day = {6},
  howpublished = {\url{https://machinelearningmastery.com/the-bahdanau-attention-mechanism}},
  note = {Accessed: 2025-10-11},
  url = {https://machinelearningmastery.com/the-bahdanau-attention-mechanism}
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{Stryker_LLM,
  author = {Stryker, Cole},
  title = {What are large language models (LLMs)?},
  publisher = {IBM},
  url = {https://www.ibm.com/think/topics/large-language-models}
}

@misc{geeksforgeeks_2024,
title = {Large Language Models (LLMs) vs Transformers - GeeksforGeeks},
url = {https://www.geeksforgeeks.org/nlp/large-language-models-llms-vs-transformers},
journal = {GeeksforGeeks},
author = {GeeksforGeeks},
year = {2024},
month = {8}
}

@misc{srinivasan2024transformer,
  author       = {Srinivasan Anusha, Janani},
  title        = {Transformer Architecture --- The Backbone of LLMs},
  year         = {2024},
  month        = {12},
  publisher    = {Medium},
  url          = {https://jananithinks.medium.com/transformer-architecture-the-backbone-of-llms-1a3d085ca981},
  note         = {Accessed: 2025-10-12}
}

@misc{Bergmann_Fine_Tuning,
  author = {Bergmann, Dave},
  title = {What is Fine-Tuning?},
  publisher = {IBM},
  url = {https://www.ibm.com/think/topics/fine-tuning}
}

@misc{kim2021neuralsequencetogridmodulelearning,
      title={Neural Sequence-to-grid Module for Learning Symbolic Rules}, 
      author={Segwang Kim and Hyoungwook Nam and Joonyoung Kim and Kyomin Jung},
      year={2021},
      eprint={2101.04921},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.04921}, 
}

@inproceedings{inproceedings,
author = {Gawde, Rishikesh},
year = {2021},
month = {04},
pages = {},
title = {Image Caption Generation Methodologies}
}

@misc{emil2020what,
    author = {Emil},
    title = {What is the feedforward network in a transformer trained on?},
    year = {2020},
    month = {02},
    note = {Accessed: 2025-11-03},
    key = {emil2020what},
    url = {https://datascience.stackexchange.com/questions/68020/what-is-the-feedforward-network-in-a-transformer-trained-on}
}

@misc{kili2023,
  author       = {Lee, Kenny},
  title        = {Open-Sourced Training Datasets for Large Language Models (LLMs)},
  year         = {2023},
  publisher    = {Kili Technology},
  url          = {https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models},
  note         = {Accessed on 2025-11-03}
}

@misc{pascanu2013difficultytrainingrecurrentneural,
      title={On the difficulty of training Recurrent Neural Networks}, 
      author={Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
      year={2013},
      eprint={1211.5063},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1211.5063}, 
}