@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}}

@misc{cho2014propertiesneuralmachinetranslation,
      title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
      year={2014},
      eprint={1409.1259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.1259}, 
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@misc{geeksforgeeks_lstm,
  title = {Deep Learning Introduction to Long Short Term Memory},
  author = {GeeksforGeeks},
  year = {2025},
  month = {4},
  day = {5},
  howpublished = {\url{https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory}},
  note = {Last Updated: 2025-04-05},
  url = {https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory}
}

@misc{geeksforgeeks_gru,
  title = {Gated Recurrent Unit Networks},
  author = {GeeksforGeeks},
  year = {2025},
  month = {10},
  day = {9},
  howpublished = {\url{https://www.geeksforgeeks.org/machine-learning/gated-recurrent-unit-networks}},
  note = {Last Updated: 2025-10-09},
  url = {https://www.geeksforgeeks.org/machine-learning/gated-recurrent-unit-networks}
}

@misc{cristina_2023_bahdanau,
  title = {The Bahdanau Attention Mechanism},
  author = {Cristina, Stefania},
  year = {2023},
  month = {1},
  day = {6},
  howpublished = {\url{https://machinelearningmastery.com/the-bahdanau-attention-mechanism}},
  note = {Accessed: 2025-10-11},
  url = {https://machinelearningmastery.com/the-bahdanau-attention-mechanism}
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{Stryker_LLM,
  author = {Stryker, Cole},
  title = {What are large language models (LLMs)?},
  publisher = {IBM},
  url = {https://www.ibm.com/think/topics/large-language-models}
}

@misc{geeksforgeeks_2024,
title = {Large Language Models (LLMs) vs Transformers - GeeksforGeeks},
url = {https://www.geeksforgeeks.org/nlp/large-language-models-llms-vs-transformers},
journal = {GeeksforGeeks},
author = {GeeksforGeeks},
year = {2024},
month = {8}
}

@misc{srinivasan2024transformer,
  author       = {Srinivasan Anusha, Janani},
  title        = {Transformer Architecture --- The Backbone of LLMs},
  year         = {2024},
  month        = {12},
  publisher    = {Medium},
  url          = {https://jananithinks.medium.com/transformer-architecture-the-backbone-of-llms-1a3d085ca981},
  note         = {Accessed: 2025-10-12}
}

@misc{Bergmann_Fine_Tuning,
  author = {Bergmann, Dave},
  title = {What is Fine-Tuning?},
  publisher = {IBM},
  url = {https://www.ibm.com/think/topics/fine-tuning}
}