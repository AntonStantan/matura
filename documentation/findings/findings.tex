\documentclass{article}
\usepackage[backend=biber, style=apa]{biblatex}
\usepackage{hyperref}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{wrapfig}
\usepackage{titling}
\usepackage{xurl}

\usepackage[table,xcdraw]{xcolor}

\geometry{margin=2.5cm}
\addbibresource{references_find.bib}
\setlength{\bibitemsep}{0.5\baselineskip}

\setlength{\intextsep}{0pt} 

\title{\huge Neural Predictive Calculator
\\[0em]
{\LARGE Findings}
\\[0em]
{\large Maturitätsarbeit, Kantonsschule Baden}
\\[-0.5em]
{\large Michael Schneider, Julia Smits}}
\author{Anton Mukin G4h}
\date{June 2025}

\setlength{\parindent}{0pt}


\begin{document}

\maketitle

\begin{abstract}
    This study investigates which Neural Network architecture is optimal for predicting 
    simple arithmetic expressions. 
    Various architectures built for regression tasks were evaluated: Feedforward Neural Networks (FNNs), Recurrent Neural Networks (RNNs), transformers, 
    as well as fine-tuned Large Language Models (LLMs). Evaluations were performed to measure 
    if the models were able to learn arithmetic rules by assessing their generalization 
    capabilities with a custom made benchmark. The best performance on the previously 
    defined benchmark was achieved by the simple FNN architecture. Still, no model performed 
    well enough to be classified as \textit{having learned} arithmetic rules.

    The regression models were presented to a colleague in a guided discussion. Though only on a 
    small and insignificant dataset, it was found to be beneficial to explain the basic 
    workings of FNNs and transformers.



\end{abstract}

\subsection{Introduction}
This project began with a simple idea: a calculator that predicts answers using a neural network rather than performing systematic calculations. The central question that propelled this project was determining the most suitable neural network architecture for this task. The current document presents the findings from the "Neural Predictive Calculator" project.

\subsection{Hypothesis}
Following a \href{https://github.com/AntonStantan/matura/blob/main/zwischenProdukt/LiteraturstudieAnton.pdf}{literature review}, the expected results were as follows: 
It was hypothesized that Feed-forward Neural Networks (FNNs) would be the weakest architecture, Recurrent Neural Networks (RNNs) would perform better, and transformers and pre-trained transformers would perform the best. Technologies such as positional embeddings, a seq2grid pre-processor\footnote{Upon further research into the preprocessor; Its architecture uses a neural network for reshaping inputs into a grid, as shown in their paper \cite{kim2021neuralsequencetogridmodulelearning}. This does not fit the requirements for this project. Even though it is not systematic, the preprocessor effectively just increases the model complexity by attaching a RNN in the front. Neural Networks using a seq2grid preprocessor were not evaluated in this project.} and a PReLU activation function were expected to help the model generalize simple arithmetic rules.


\newpage
\listoffigures
\tableofcontents
\newpage

\section{Feed-forward Neural Networks (FNNs)}
The functionality of FNNs has been previously discussed in \href{https://github.com/AntonStantan/matura/blob/main/zwischenProdukt/LiteraturstudieAnton.pdf}{the literature review} and \href{https://github.com/AntonStantan/matura/blob/main/documentation/methodology/methodology.pdf}{methodology document}. It will not be further discussed in here. However, FNNs will be referenced later in this document.

\section{RNN}
Recurrent Neural Networks (RNNs) work similarly to FNNs with one key difference: There is a vector called the hidden-state. This vector contains information about previous time-steps. The hidden-state of the previous time-step, in addition to the input of the current time-step, is fed into a model which computes the hidden-state of the present time-step. The output of each time-step is calculated by feeding the respective hidden-state to a model.

\subsection{Numerical Visualization of a RNN:}

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{images/RNN.png}
    \caption{RNN diagram as described by \cite{inproceedings}}
    \label{fig:RNNarchitecture}
\end{wrapfigure}

Let:
\begin{itemize}
    \item $x_t$: input at time step $t$
    \item $h_t$: hidden-state at time step $t$
    \item $y_t$: output at time step $t$
    \item $W_{xh}$: weight matrix connecting input to hidden-state
    \item $W_{hh}$: weight matrix connecting previous hidden-state to current hidden-state (recurrent weights)
    \item $W_{hy}$: weight matrix connecting hidden-state to output
    \item $b_h$: bias vector for the hidden layer
    \item $b_y$: bias vector for the output layer
    \item $\sigma$: activation function (in our case: PReLU)
    \item $\sigma_{out}$: activation function for the output (linear for the regression task in this project)
\end{itemize}

$$h_t = \sigma(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$$
$$y_t = \sigma_{out}(W_{hy}h_t + b_y)$$

\subsection{Relevant Takeaway}
For this project, it means the RNN processes an expression sequentially, one part at a time, rather than as a whole. Due to the nature of the RNN's formula, tokens that appear later in a sequence have a more significant impact on the model's prediction than earlier tokens. This means the output number will almost always be closer to the last number of the expression than the first. This is a common issue with RNNs. It was well documented by \cite{pascanu2013difficultytrainingrecurrentneural} and is widely known as the vanishing gradient problem.

\section{Other Types of RNNs}
To address the vanishing gradient problem, several architectures have been developed, for example, the Long Short-Term Memory (LSTM) proposed by \cite{6795963}, the Gated Recurrent Unit (GRU) proposed by \cite{cho2014propertiesneuralmachinetranslation} or later, the concept of attention proposed by \cite{bahdanau2016neuralmachinetranslationjointly}.

\subsection{Long Short-Term Memory (LSTM)}
The LSTM architecture solves the gradient vanishing problem by using a memory cell. The model can use this to store \eqref{eq:cell_update_lstm}, forget \eqref{eq:forget_gate_lstm} and pass information from the memory cell to the hidden-state \eqref{eq:hidden_state_lstm}.

Let: 
\begin{itemize}
    \item $\sigma(\cdot)$ denotes the sigmoid activation function,
    \item $\tanh(\cdot)$ is the hyperbolic tangent function,
    \item $\odot$ represents element-wise (Hadamard) multiplication,
    \item $[h_{t-1}, x_t]$ is the concatenation of the previous hidden-state $h_{t-1}$ and the current input $x_t$,
    \item $W_f, W_i, W_C, W_o$ are trainable weight matrices,
    \item $b_f, b_i, b_C, b_o$ are trainable bias vectors,
    \item $C_t$ is the current memory cell state,
    \item $\tilde{C}_t$ is the candidate cell state,
\end{itemize}

\begin{align}
    f_t &= \sigma\!\left(W_f \cdot [h_{t-1}, x_t] + b_f\right) \label{eq:forget_gate_lstm} \\
    i_t &= \sigma\!\left(W_i \cdot [h_{t-1}, x_t] + b_i\right) \label{eq:input_gate_lstm} \\
    \tilde{C}_t &= \tanh\!\left(W_C \cdot [h_{t-1}, x_t] + b_C\right) \label{eq:candidate_cell_state_lstm} \\
    o_t &= \sigma\!\left(W_o \cdot [h_{t-1}, x_t] + b_o\right) \label{eq:output_gate_lstm}\\    
    C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \label{eq:cell_update_lstm}\\
    h_t &= o_t \odot \tanh(C_t) \label{eq:hidden_state_lstm}
\end{align}
Source for the equations: \cite{geeksforgeeks_lstm}
\\[2em]
In the equations one can see the forget gate activation \eqref{eq:forget_gate_lstm}, the input gate activation \eqref{eq:input_gate_lstm}, the candidate cell state \eqref{eq:candidate_cell_state_lstm} and the output gate activation \eqref{eq:output_gate_lstm}.

The cell state is calculated in \eqref{eq:cell_update_lstm}. There, the forget gate which scales the previous cell state is combined with the input gate.

The hidden-state is calculated in \eqref{eq:hidden_state_lstm}, where the output activation is applied to the cell state.

\subsection{Gated Recurrent Unit (GRU)}
The GRU architecture works in a similar way to the LSTM. Instead of utilizing memory cells, GRUs directly use the hidden-state.

Let:
\begin{itemize}
    \item $\sigma(\cdot)$ is the sigmoid activation function,
    \item $\tanh(\cdot)$ is the hyperbolic tangent function,
    \item $\odot$ denotes element-wise (Hadamard) multiplication,
    \item $[h_{t-1}, x_t]$ is the concatenation of the previous hidden-state and current input,
    \item $W_z, W_r, W_h$ are trainable weight matrices,
    \item $b_z, b_r, b_h$ are trainable bias vectors,
    \item $\tilde{h}_t$ is the candidate hidden-state
    \item $h_t$ is the current hidden-state
\end{itemize}

\begin{align}
    z_t &= \sigma\!\left(W_z [h_{t-1}, x_t] + b_z\right) \label{eq:update_gate_gru} \\
    r_t &= \sigma\!\left(W_r [h_{t-1}, x_t] + b_r\right) \label{eq:reset_gate_gru} \\
    \tilde{h}_t &= \tanh\!\left(W_h [r_t \odot h_{t-1}, x_t] + b_h\right) \label{eq:candidate_hidden_state_gru} \\
    h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \label{eq:update_hidden_state_gru}
\end{align}
Source for the equations: \cite{geeksforgeeks_gru}
\\[2em]
Above you can see the update gate activation \eqref{eq:update_gate_gru}, as well as the reset gate activation \eqref{eq:reset_gate_gru}.

Further down, the reset gate activation is applied to the previous hidden-state to calculate the candidate hidden-state \eqref{eq:candidate_hidden_state_gru}. 

Lastly the hidden-state can be calculated by applying (1 - the update gate activation) to the previous hidden-state and combining it with the update gate activation applied to the hidden-state candidate \eqref{eq:update_hidden_state_gru}. Depending on whether the update gate activation is larger or smaller, the previous hidden-state or the candidate hidden-state will weigh in more on the current hidden-state.

\subsection{Bidirectional LSTM with Attention}
An architecture of this type consists in part of a bidirectional LSTM, meaning two LSTMs working in parallel, one of which processes data from front to back, the other from back to front; their results are then concatenated and passed on.
\\[2em]
The other part of this architecture is the attention mechanism. Here it is, as described by \cite{bahdanau2016neuralmachinetranslationjointly}.

Let:
\begin{itemize}
    \item $\mathbf{h}_t \in \mathbb{R}^{128}$ are the hidden-states for each timestep $t$, the output from the bidirectional LSTM,
    \item $\mathbf{W} \in \mathbb{R}^{128 \times 128}$ is a trainable weight matrix,
    \item $\mathbf{b} \in \mathbb{R}^{128}$ is a bias vector,
    \item $\mathbf{u} \in \mathbb{R}^{128}$ is a trainable context vector.
\end{itemize}

For each time step $t$, compute:
\begin{align}
    \mathbf{v}_t &= \tanh\!\left( \mathbf{W} \mathbf{h}_t + \mathbf{b} \right) \label{eq:v_bahdanau} \\
    e_t &= \mathbf{u}^\top \mathbf{v}_t \label{eq:attention_score}
\end{align}

Normalize scores using softmax:
\begin{equation}
    \alpha_t = \frac{\exp(e_t)}{\sum_{j=1}^{T} \exp(e_j)} \label{eq:alpha_score}
\end{equation}
The attention weights $\boldsymbol{\alpha} = [\alpha_1, \dots, \alpha_T]$ indicate the importance of each time step.

Then compute the context vector:
\begin{equation}
    \mathbf{c} = \sum_{t=1}^{T} \alpha_t \mathbf{h}_t \label{eq:attention_context}
\end{equation}

Source for equations: \cite{cristina_2023_bahdanau}
\\[2em]
An attention score is calculated for each timestep in \eqref{eq:v_bahdanau}, \eqref{eq:attention_score}, it is then normalized \eqref{eq:alpha_score}. Finally the attention weights scale their respective hidden-states from the LSTMs, to compute the context vector \eqref{eq:attention_context}.

\section{Transformers} \label{sec:transformers}
The best performing and most widely used architectures for most tasks to date are transformers; they form the basis for LLMs. The key to their success is multi-head self-attention.
\\[2em]
In this document, we will discuss the transformer architecture as first proposed by \cite{vaswani2023attentionneed}.

\subsection{Multi-Head Self-Attention}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\paperwidth]{images/dotproduct_1.png}
    \caption{Two diagrams from \cite{vaswani2023attentionneed} with the scaled dot-product attention described on the left and multi-head attention on the right.}
    \label{fig:MHAttention}
\end{figure}

Data is first split equally into multiple heads, which process it in parallel. There, the tensors are linearly projected with trainable weights to obtain queries (Q), keys (K), and values (V), which can be seen at the bottom of the image.
$$ Q = X W_Q, \quad K = X W_K, \quad V = X W_V $$
Afterwards, the dot-product between the queries and the keys is calculated and scaled\footnote{According to \cite{vaswani2023attentionneed} this is done to counteract dot-products growing too large and later overwhelming the softmax function.} to determine how similar they are. This leaves us with a number between 0 and 1, representing how much attention to pay. The value $V$ represents the actual information of the token for which we just calculated the attention. This means our final step is to scale the value $V$ by its attention. Pay attention to the equation below.\footnote{K has a T, this means the matrix is transposed. This is necessary for multiplication because the Q and K matrices have the same number of rows and columns.} \eqref{eq:sdpAttention}
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V \label{eq:sdpAttention}
\end{equation}

$$\text{where}\footnotemark\text{: }\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i=1, \dots, K$$
\footnotetext{Effectively the softmax function sets the biggest element in a set to 1 and the smallest element in the set to 0. All elements in between are scaled appropriately so all elements add up to 1.}

This is done across all heads in parallel. The results are then concatenated\footnote{This means they are reassembled to have the same dimensions as before they were split up.} back together and they undergo a linear projection. As described below:
\begin{gather*}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O \\
\text{where: } \text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
\end{gather*}

\newpage
\subsection{The Encoder Layer}

\begin{wrapfigure}{l}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{images/encodingLayer.png}
    \caption{Encoder Layer as described by \cite{vaswani2023attentionneed}}
    \label{fig:encodingLayer}
\end{wrapfigure}
In figure \ref{fig:encodingLayer} the multi-head attention first undergoes a residual connection as well as a layer normalization\footnote{Given an input vector, layer normalization works by normalizing its features and making it so their mean is 0.}, as described below:
\begin{gather}
    \text{where Sublayer(x) is the function, the residual connection is formed by:} \nonumber \\
    \text{Output}(x) = \text{LayerNorm}\bigl(x + \text{Sublayer}(x)\bigr) \label{eq:ResConnect}
\end{gather}
Afterwards, all tokens are fed into a point-wise FNN, where they are processed separately and independently. This part is crucial, to introduce non-linearity into the system, as multi-head attention is linear, and non-linearity is needed for a model to be able to learn.

\subsection{Point-wise Feed-Forward Network}
Equation \eqref{eq:pointwiseFNN} and figure \ref{fig:pointwiseFNN}:
The pointwise FNN works by taking in a number of $d\_model$ tokens, then a layer with a number of $d\_FNN$ neurons with weights and biases $W_1$ and $b_1$ is applied to them individually. The activation function ReLU discards all negative values and the output layer with weights and biases $W_2$ and $b_2$ resets the data back to its original dimensionality.

Please turn your attention to figure \ref{fig:encodingLayer}. After the resulting residual connection from the MHAttention \eqref{eq:ResConnect} is processed by a pointwise FNN, its residual connection will be the output of a single Encoder Layer as shown in figure \ref{fig:encodingLayer}. 

A regressive transformer model, like the one used in this project, consists of multiple such encoding layers and last but not least a single neuron, which works as the output layer.
\begin{equation}
    \text{pointwiseFNN}(x) = \max\bigl(0, \, x W_1 + b_1 \bigr) W_2 + b_2 \label{eq:pointwiseFNN}
\end{equation}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\paperwidth]{images/pointwiseFNN.png}
    \caption{A diagram from \cite{emil2020what} representing the structure of a pointwise FNN.}
    \label{fig:pointwiseFNN}
\end{figure}

\newpage

\subsection{Positional Encoding}
A special characteristic of the transformer is that data is passed through it as a whole. This means the model does not have positional understanding on its own. To counteract this effect, a positional encoding is applied to the input sequences after the tokenizer.

Let:
\begin{itemize}
    \item $d_{model}$: dimensionality of the sequences passed to the model
    \item $pos$: index of the token inside the sequence
    \item $i$ an integer that indexes half of the model's dimensions
\end{itemize}
Formulas as described by \cite{vaswani2023attentionneed}: 
\begin{align}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align}

This project implements positional encoding as described by \cite{vaswani2023attentionneed}. Very vaguely put, it assigns a number to each token, based on its position in the sequence.

\section{Fine-Tuned Pre-Trained LLMs}
The last model type used for this project are fine-tuned Large Language Models (LLMs).
For sources and further reading on this section see (in chronological order; left to right): \cite{geeksforgeeks_2024,Stryker_LLM,srinivasan2024transformer,Bergmann_Fine_Tuning}

\subsection{LLM}
LLMs adopt the transformer architecture\footnote{The transformer architecture discussed in the previous section \ref{sec:transformers}. But now it not only consists of an encoder but also has a decoder, which is responsible for the model's output} and adapt it to imense sizes, by increasing the architecture's hyperparameters, as well as employing new technological advancements. The number of parameters used by a LLM varies from model to model, but lies in the hundreds of billions per popular high-end LLM.
The large size allows large architectures to excel at complicated tasks like mostly Natural Language Processing (NLP).
To facilitate training and prediction on such large architectures, massive supercomputers are used, consisting of thousands of GPUs. They mainly work by utilizing parallel processing to distribute the load across multiple powerful GPUs. While by far not a supercomputer, a tiny replica was used for this project: The Nvidia Jetson Orin Nano Super Developer Kit.

\subsection{Fine-Tuning Process}
Most of the time LLMs are trained on huge dumps of unlabeled data. For more details on this view \cite{kili2023}. 
This training approach is called unsupervised learning. When training on unlabelled data from which ground truth can be inferred, the approach is called self-supervised data. 
The fine-tuning process on the other hand is mostly done on smaller datasets with supervised data, meaning they are labeled.
\\[2em]
In the fine-tuning process, a pre-trained model, with its weights and biases already optimized, is trained again on additional data. This means that the model's weights will be slightly adjusted from their pre-trained state. The optimizer will find a new minimum in which the model will have learned additional information from the fine-tuning training data. When evaluated on test data, the newly fine-tuned model is expected to perform better than its pre-trained counterpart.

\subsection{In Addition}
Please note that these are not regression models but sequence-to-sequence models, which means that inside the model, tokens are processed, not numbers, as in the other models discussed. For the task in this project, specifically, fine-tuned models will output an integer rather than a float, because tokens for floats have not been created. This is the reason why the only metric that makes sense for these models is accuracy, which essentially evaluates how many times the model predicted correctly.

\newpage

\section{Findings}
Lastly, the most noteworthy architectures were evaluated and scored on a benchmark. A custom benchmark was defined for this rather unique project, so that it takes into account the different generalization capabilities of models. For details, see the methodology section: 1.5 The Benchmark. 

The results\footnotemark were documented in this table: 
\footnotetext{Mean Absolute Error (MAE) is the average deviation of the predicted answer, to the correct answer. And Mean Relative Error (MRE) is the deviation of the predicted answer, to the correct answer divided by the correct answer.}
\\[0.5em]
\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{|lr|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|l|}{Regression models:}                              & \multicolumn{1}{l|}{FNN2} & \multicolumn{1}{l|}{FNN3}        & \multicolumn{1}{l|}{RNN2} & \multicolumn{1}{l|}{Bidirectional   LSTM} & \multicolumn{1}{l|}{transformer4} & \multicolumn{1}{l|}{transformer5} \\ \hline
\multicolumn{2}{|l|}{Total   Parameters:}                             & 6’211                     & 8’893                            & 222’464                   & 19’535                                    & 114’628                           & 1’494’724                         \\
\multicolumn{2}{|l|}{Architecture   Parameters:}                      & 6’211                     & 8’893                            & 222,464                   & 6’511                                     & 38’209                            & 498’241                           \\
\multicolumn{2}{|l|}{Optimizer   Parameters:}                         & -                         & -                                & -                         & 13’024                                    & 76’419                            & 996’483                           \\
\rowcolor[HTML]{F7C7AC} 
MAE in Range:                              &                          & 0.0389406                 & 0.034600                         & 0.2730647                 & 0.5908327                                 & 0.0448075                         & 0.0381983                         \\
\rowcolor[HTML]{F7C7AC} 
MRE in Range:                              &                          & 0.0155504                 & 0.013438                         & 0.1135200                 & 0.2712299                                 & 0.0188047                         & 0.0151836                         \\
\rowcolor[HTML]{94DCF8} 
MAE out Range:                             &                          & 2.2301364                 & \cellcolor[HTML]{83CCEB}2.504607 & 3.5727410                 & 3.2350100                                 & 4.5702314                         & 4.9287004                         \\
\rowcolor[HTML]{94DCF8} 
MRE out Range:                             &                          & 0.2509270                 & \cellcolor[HTML]{83CCEB}0.290885 & 0.3472042                 & 0.3404062                                 & 0.5201459                         & 0.5431157                         \\
\rowcolor[HTML]{FFDC6D} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{FFDC6D}MAE   long Expressions:} & 6.2929916                 & 6.116872                         & 6.0766930                 & 2.7806435                                 & 6.1630590                         & 6.0680327                         \\ \hline
\rowcolor[HTML]{D86DCD} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{D86DCD}Benchmark   score:}      & 6.6345832                 & 6.672521                         & 0.4793787                 & 1.7374969                                 & 2.8882827                         & 5.2896368                         \\ \hline
\end{tabular}
\end{table}

And the fine-tuned Language Models, evaluated on their accuracy:
\\[0.5em]
\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{|ll|l|l|l|}
\hline
\multicolumn{2}{|l|}{fine-tuned Language Models:}                          & Gemini 2.5 Pro & Gemma 3 1B & Gemma 3 270M \\ \hline
Parameter size:                               &                            & 1.4E+11        & 1.00E+09   & 2.70E+08     \\
\rowcolor[HTML]{F7C7AC} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{F7C7AC}Accuracy   in Range:}         & 95.17          & 93.41      & 54.22        \\
\rowcolor[HTML]{94DCF8} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{94DCF8}Accuracy   out Range:}        & 99.51          & 63.33      & 9.67         \\
\rowcolor[HTML]{FFDC6D} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{FFDC6D}Accuracy   long expressions:} & 90             & 39.57      & 28           \\ \hline
\end{tabular}
\end{table}

Firstly, it is notable that the main hypothesis -- that more complex\footnotemark models would perform better -- is not supported by the results. This is made apparent by the clear outliers, such as the outstanding performance of the FNN2 or the poor performance of the RNN2 model. Also, notice the minimal improvement between FNN2 and FNN3, which includes a positional embedding layer.
\footnotetext{The complexity of a neural network depends in part on the number of parameters, and in part on the type of architecture. Where a large number of parameters and a more advanced architecture correlate to a higher complexity}
It is visible that the FNN models show the best performance on the benchmark, meaning they are crowned the winners.

\subsection{Guided Discussion}

After conducting an open presentation for a colleague, his feedback was collected and his knowledge of the presented topics graded. 

The starting hypothesis and the reason for conducting this discussion was to test if regression models (like the ones in this project) are easier to teach and understand for people interested in neural networks, who already posses some basic knowledge.

The Results from the questionnaire yield that the discussion partner did not fully understand either topic\footnote{The FNN and transformer architectures of regression models were taught.}, but was able to answer basic questions. The topic of FNNs was easier to grasp than transformers.

According to the subject, the direct examples from this project which were delivered in accordance to theory, were the most helpful in forming an understanding. 

The use of Regression models instead of sequence to sequence (seq2seq) models also helped. The reasons listed by the subject were the similarities between the regression in the output layer and linear regression as taught in school, as well as the usage of mostly numbers throughout the whole model architecture, from input to output. The subject also stated that the latter helped with the understanding of tokens or vectors and how they are processed. Prior to this discussion this was not apparent to the subject.

The feedback included criticism about the lack of information and explanation regarding the optimization process. This is understandable, because the focus for this project was placed elsewhere.

In conclusion, though only evaluated on a very small dataset, it can be said that it is beneficial to explain the basic workings of FNNs and transformers on the basis of models for regression, because of the wider use of numbers, as well as better connections to topics like linear regression, discussed as a part of the standard curriculum in school.

\section{Discussion}
\subsection{Regression Models}
All of the 4 architectures discussed here are very different, and all of them have different strengths and weaknesses, as one can tell by their performances on different sets of test data. Recurrent Neural Networks excel at long expressions, due to the way data is passed through these models sequentially in hidden-states. They are designed for handling longer inputs. Similarly, Bidirectional LSTMs with attention are even more effective when adjusting to a longer input. They can confidently predict longer expressions the best out of all the models studied in this project. This is because vanishing gradients are punished heavily when solving arithmetic expressions by nature, and this model combats this well, as opposed to the simplistic RNN architecture. Additionally, the attention mechanism not only helps with vanishing gradients in longer expressions but also emphasizes outstandingly large numbers and their corresponding signs. 

When it comes to transformers, they perform best on data just like the one they are trained on, but not exactly the same; the classical definition of validation data. There are too many different types of different parameters, which all have been trained to process training data; this means even the smallest differences in input will lead to the model processing them wrong. Transformers are bad at generalizing rules from data. transformer5 with more parameters than transformer4 only shows a significant performance improvement on the classical validation data. Thanks to positional encoding, transformers can handle longer inputs better than FNNs.

The FNN architecture performed with the best benchmark. The FNN2 model was good at expressions of the same length with numbers not encountered in the training data. Still, its abilities will not suffice to label it as 'able to generalize', primarily also because of its weakness at longer expressions. Even after adding positional embedding in FNN3, the model struggles at grasping expressions of different sizes.

In addition it can also be said, that the number of parameters in a specific model architecture does not directly correlate to its benchmark performance, rather there are sweetspots for the number of parameters that can be found using the Keras-Tuner or a heatmap of differently sized models and their performances. It can be the case sometimes that there are multiple sweetspots, like it was the case for transformer4 and transformer5, who both were trained with Keras-Tuner.
\subsection{Pre-trained Fine-tuned Transformers}
The models in the lower table of the two are all very large, with Gemini 2.5 being particularly massive. Because we know that these models have the same transformer architecture and the sweet spot for hyperparameters is much smaller than their parameter size, we expect a weaker performance.\footnote{Since these are seq2seq models, their hyper-parameters sweet-spot would not be the same as the regression transformers. And their Decoder multiplies the number of parameters roughly by 2x. This is still not comparable to the jump in complexity between the largest optimized transformer regression model and the smallest pre-trained model, which is a factor of roughly 180x.}. Additionally, they predict on tokens rather than numerical values, which is not optimal for the task in this project. The performance of these models also depends heavily on the pre-training they previously underwent because it also involves arithmetics. This explains why the bigger pre-trained models perform better. The Gemini 2.5 Pro model performed the best out of all models evaluated in this project on generalization tests, but this is due to previous training, including expressions similar to those in the training data. For this reason it cannot be said that it is able to generalize.

\subsection{Drop-Out}
When introducing a Dropout with industry-standard values, contrary to the expectation of reducing overfitting (which is present to some extent, according to literature discussed in the literature review), the models are not able to generalize beyond the training range, this is different to overfitting because validation data is being predicted with a simillar loss as training data.

The MSEs of models with Dropout are higher than those of the previous models without Dropout. This is because dropout effectively decreases the computing capacity of a model during training (when predicting, this is no longer the case) by deactivating a percentage of randomly chosen neurons in each layer. This explains their weak performance and why the KerasTuner prioritizes models without dropout.


\subsection{Conclusion}
Adding positional embedding barely makes a difference for FNNs.
RNNs are weak models, outclassed by bidirectional LSTMs with attention, and should not be used.
Transformers excel at accurately learning the training data but struggle with generalization more than other models.
FNNs are, overall, the best at generalizing and perform best on the defined benchmark. They are the best architecture for solving simple arithmetic expressions with a reasonable\footnotemark supply of computational resources.
\footnotetext{Publicly available, affordable hardware e.g., The Nvidia Jetson Orin Nano Super Developer Kit GPU used in this project.}
If maximum performance with unlimited computational resources is the goal, a fine-tuned, cutting-edge pre-trained model such as Gemini 2.5 Pro will yield the best results.

Despite the chance factor playing a role, the conclusion will hold true. Conclusions were reached with significant margins and are backed by logical explanations. All results are reproducible with \href{https://github.com/AntonStantan/matura/tree/main}{publicly available notebooks}.

\subsection{Possible Future Work}
The topic chosen for this project is very grand and current. A logical and important continuation of this project is using the benchmark as the loss function; this might lead to the training of better-performing models, as well as a struggle with overfitting. 

Another possible direction is improving models by evaluating different activation functions, optimizers, or training data and using the best one, or even designing one's own. Training data and its tokenizer and padding is an area with lots of potential for improvement, i.e., by using an additional embedding, or finding the best training data, to teach a neural network simple arithmetics. 

Also, consider this a proposal to evaluate a model architecture: an FNN with attention seems to be promising. Judging by the results of this study, this model is expected to improve the FNN's performance with longer expressions than in training, as well as strengthen its ability to accurately and confidently predict test data inside of the number range, previously referred to as the classical validation data.

A minimal but still interesting branch can be formed from this project to find the error or investigate the exact reason for the accuracy missmatch observed during and after fine-tuning.

\newpage
\printbibliography[heading=bibintoc]
\end{document}