\documentclass{article}
\usepackage[backend=biber, style=apa]{biblatex}
\usepackage{hyperref}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{wrapfig}
\usepackage{titling}
\usepackage{xurl}
\usepackage[font=small, labelfont=bf]{caption}


\usepackage[table,xcdraw]{xcolor}

\geometry{margin=2.5cm}
\addbibresource{references_find.bib}
\setlength{\bibitemsep}{0.5\baselineskip}

\setlength{\intextsep}{0pt} 

\title{\huge Neural Predictive Calculator
\\[0em]
{\LARGE Findings}
\\[0em]
{\large Maturitätsarbeit, Kantonsschule Baden}
\\[-0.5em]
{\large Erstbetreuer: Michael Schneider, Zweitbetreuer: Julia Smits}}
\author{Anton Mukin}
\date{June 2025}

\setlength{\parindent}{0pt}


\begin{document}

\maketitle

\begin{abstract}
    This study investigates which Neural Network architecture is optimal for predicting 
    simple arithmetic expressions. 
    Various architectures built for regression tasks were evaluated: Feedforward Neural Networks (FNNs), Recurrent Neural Networks (RNNs), transformers, 
    as well as fine-tuned Large Language Models (LLMs). Evaluations were performed to measure 
    if the models were able to learn arithmetic rules by assessing their generalization 
    capabilities with a custom-made benchmark. The best performance on the previously 
    defined benchmark was achieved by the simple FNN architecture.

    The good performance of simple architectures with relatively few parameters suggests that 
    these architectures are better at generalizing than the more sophisticated models.
    Still, no model performed well enough to be classified as \textit{having learned} arithmetic rules.
    Which leads to the conclusion that neural networks are not capable of extrapolating symbolic rules.
    Furthermore, the results point to the fact that neural networks differentiate from our human brain in the way they generalize rules from data\footnote{\cite{marcus2018deeplearningcriticalappraisal} expresses this well in his concerns.};
    While humans typically identify underlying principles, neural networks rely on pattern recognition, primarily analyzing surface-level data.

    The regression models used for this project were presented to a colleague in a guided discussion. Although this conclusion is drawn from a single experiment, the results indicate that it was advantageous to explain the basic workings of FNNs and transformers using models for regression as opposed to sequence-to-sequence models.


\end{abstract}

\newpage
\tableofcontents
\listoffigures
\newpage

\section{Introduction}
This project began with a simple idea: a calculator that predicts answers using a neural network rather than performing systematic calculations. The central question that propelled this project was determining the most suitable neural network architecture for this task. The current document presents the findings from the "Neural Predictive Calculator" project.

\subsection{Hypothesis}
Following a \href{https://github.com/AntonStantan/matura/blob/main/zwischenProdukt/LiteraturstudieAnton.pdf}{literature review}, the expected results were as follows: 
It was hypothesized that Feed-forward Neural Networks (FNNs) would be the weakest architecture, Recurrent Neural Networks (RNNs) would perform better, and transformers and pre-trained transformers would perform the best. Technologies such as positional encoding, a seq2grid pre-processor\footnote{Upon further research into the preprocessor; Its architecture uses a neural network for reshaping inputs into a grid, as shown in their paper \cite{kim2021neuralsequencetogridmodulelearning}. This does not fit the requirements for this project. Even though it is not systematic, the preprocessor effectively just increases the model complexity by attaching a RNN in the front. Neural Networks using a seq2grid preprocessor were not evaluated in this project.} and a PReLU activation function were expected to help the model generalize simple arithmetic rules.


\section{Feed-forward Neural Networks (FNNs)}
The functionality of FNNs has been previously discussed in \href{https://github.com/AntonStantan/matura/blob/main/zwischenProdukt/LiteraturstudieAnton.pdf}{the literature review} and \href{https://github.com/AntonStantan/matura/blob/main/documentation/methodology/methodology.pdf}{methodology document}. It will not be further discussed in here. However, FNNs will be referenced later in this document.

\subsection{Hyper-Parameter Tuning}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/landscape.png}
        \label{fig:landscape}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/landscape_top.png}
        \caption{3D Visualization of the loss landscape from the Keras-Tuner-search of the FNN2 model, with the top-view on the right.}
        \label{fig:landscape_top}
    \end{minipage}
\end{figure}

FNNs as well as other models have hyper-parameters (i.e. the number of layers or neurons per layer) which have to be optimized for the specific task the models are being trained on.
In this project the Keras-Tuner (or a heatmap visualization of different models) is used to find the global minimas of the loss landscape\footnote{The code for drawing the landscape was generated with AI. The data fed into the drawing is real data from the FNN2 notebook. Random noise is added to the landscape to make it look more realistic.}, as shown below. 


\section{RNN}
Recurrent Neural Networks (RNNs) work similarly to FNNs with one key difference: There is a vector called the hidden-state. This vector contains information about previous time-steps. The hidden-state of the previous time-step, in addition to the input of the current time-step, is fed into a model which computes the hidden-state of the present time-step. The output of each time-step is calculated by feeding the respective hidden-state to a model.

\subsection{Numerical Visualization of a RNN:}

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{images/RNN.png}
    \caption{RNN diagram as described by \cite{inproceedings}}
    \label{fig:RNNarchitecture}
\end{wrapfigure}

Let:
\begin{itemize}
    \item $x_t$: input at time step $t$
    \item $h_t$: hidden-state at time step $t$
    \item $y_t$: output at time step $t$
    \item $W_{xh}$: weight matrix connecting input to hidden-state
    \item $W_{hh}$: weight matrix connecting previous hidden-state to current hidden-state (recurrent weights)
    \item $W_{hy}$: weight matrix connecting hidden-state to output
    \item $b_h$: bias vector for the hidden layer
    \item $b_y$: bias vector for the output layer
    \item $\sigma$: activation function (in our case: PReLU)
    \item $\sigma_{out}$: activation function for the output (linear for the regression task in this project)
\end{itemize}

$$h_t = \sigma(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$$
$$y_t = \sigma_{out}(W_{hy}h_t + b_y)$$

Source for equations: \cite{shervine_cheatsheet_rnn}

\subsection{Relevant Takeaway}
For this project, it means the RNN processes an expression sequentially, one part at a time, rather than as a whole. Due to the nature of the RNN's formula, tokens that appear later in a sequence have a more significant impact on the model's prediction than earlier tokens. This means the output number will almost always be closer to the last number of the expression than the first. This is a common issue with RNNs. It was well documented by \cite{pascanu2013difficultytrainingrecurrentneural} and is widely known as the vanishing gradient problem.

\section{Other Types of RNNs}
To address the vanishing gradient problem, several architectures have been developed, for example, the Long Short-Term Memory (LSTM) proposed by \cite{6795963}, the Gated Recurrent Unit (GRU) proposed by \cite{cho2014propertiesneuralmachinetranslation} or later, the concept of attention proposed by \cite{bahdanau2016neuralmachinetranslationjointly}.

\subsection{Long Short-Term Memory (LSTM)}
The LSTM architecture solves the gradient vanishing problem by using a memory cell. The model can use this to store \eqref{eq:cell_update_lstm}, forget \eqref{eq:forget_gate_lstm} and pass information from the memory cell to the hidden-state \eqref{eq:hidden_state_lstm}.

Let: 
\begin{itemize}
    \item $\sigma(\cdot)$ denotes the sigmoid activation function,
    \item $\tanh(\cdot)$ is the hyperbolic tangent function,
    \item $\odot$ represents element-wise (Hadamard) multiplication,
    \item $[h_{t-1}, x_t]$ is the concatenation of the previous hidden-state $h_{t-1}$ and the current input $x_t$,
    \item $W_f, W_i, W_C, W_o$ are trainable weight matrices,
    \item $b_f, b_i, b_C, b_o$ are trainable bias vectors,
    \item $C_t$ is the current memory cell state,
    \item $\tilde{C}_t$ is the candidate cell state,
\end{itemize}

\begin{align}
    f_t &= \sigma\!\left(W_f \cdot [h_{t-1}, x_t] + b_f\right) \label{eq:forget_gate_lstm} \\
    i_t &= \sigma\!\left(W_i \cdot [h_{t-1}, x_t] + b_i\right) \label{eq:input_gate_lstm} \\
    \tilde{C}_t &= \tanh\!\left(W_C \cdot [h_{t-1}, x_t] + b_C\right) \label{eq:candidate_cell_state_lstm} \\
    o_t &= \sigma\!\left(W_o \cdot [h_{t-1}, x_t] + b_o\right) \label{eq:output_gate_lstm}\\    
    C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \label{eq:cell_update_lstm}\\
    h_t &= o_t \odot \tanh(C_t) \label{eq:hidden_state_lstm}
\end{align}
Source for the equations: \cite{geeksforgeeks_lstm}
\\[2em]
In the equations one can see the forget gate activation \eqref{eq:forget_gate_lstm}, the input gate activation \eqref{eq:input_gate_lstm}, the candidate cell state \eqref{eq:candidate_cell_state_lstm} and the output gate activation \eqref{eq:output_gate_lstm}.

The cell state is calculated in \eqref{eq:cell_update_lstm}. There, the forget gate which scales the previous cell state is combined with the input gate.

The hidden-state is calculated in \eqref{eq:hidden_state_lstm}, where the output activation is applied to the cell state.

\subsection{Gated Recurrent Unit (GRU)}
The GRU architecture works in a similar way to the LSTM. Instead of utilizing memory cells, GRUs directly use the hidden-state.

Let:
\begin{itemize}
    \item $\sigma(\cdot)$ is the sigmoid activation function,
    \item $\tanh(\cdot)$ is the hyperbolic tangent function,
    \item $\odot$ denotes element-wise (Hadamard) multiplication,
    \item $[h_{t-1}, x_t]$ is the concatenation of the previous hidden-state and current input,
    \item $W_z, W_r, W_h$ are trainable weight matrices,
    \item $b_z, b_r, b_h$ are trainable bias vectors,
    \item $\tilde{h}_t$ is the candidate hidden-state
    \item $h_t$ is the current hidden-state
\end{itemize}

\begin{align}
    z_t &= \sigma\!\left(W_z [h_{t-1}, x_t] + b_z\right) \label{eq:update_gate_gru} \\
    r_t &= \sigma\!\left(W_r [h_{t-1}, x_t] + b_r\right) \label{eq:reset_gate_gru} \\
    \tilde{h}_t &= \tanh\!\left(W_h [r_t \odot h_{t-1}, x_t] + b_h\right) \label{eq:candidate_hidden_state_gru} \\
    h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \label{eq:update_hidden_state_gru}
\end{align}
Source for the equations: \cite{geeksforgeeks_gru}
\\[2em]
Above you can see the update gate activation \eqref{eq:update_gate_gru}, as well as the reset gate activation \eqref{eq:reset_gate_gru}.

Further down, the reset gate activation is applied to the previous hidden-state to calculate the candidate hidden-state \eqref{eq:candidate_hidden_state_gru}. 

Lastly the hidden-state can be calculated by applying (1 - the update gate activation) to the previous hidden-state and combining it with the update gate activation applied to the hidden-state candidate \eqref{eq:update_hidden_state_gru}. Depending on whether the update gate activation is larger or smaller, the previous hidden-state or the candidate hidden-state will weigh in more on the current hidden-state.

\subsection{Bidirectional LSTM with Attention}
An architecture of this type consists in part of a bidirectional LSTM, meaning two LSTMs working in parallel, one of which processes data from front to back, the other from back to front; their results are then concatenated and passed on.
\\[2em]
The other part of this architecture is the attention mechanism. Here it is, as described by \cite{bahdanau2016neuralmachinetranslationjointly}.

Let:
\begin{itemize}
    \item $\mathbf{h}_t \in \mathbb{R}^{128}$ are the hidden-states for each timestep $t$, the output from the bidirectional LSTM,
    \item $\mathbf{W} \in \mathbb{R}^{128 \times 128}$ is a trainable weight matrix,
    \item $\mathbf{b} \in \mathbb{R}^{128}$ is a bias vector,
    \item $\mathbf{u} \in \mathbb{R}^{128}$ is a trainable context vector.
\end{itemize}

For each time step $t$, compute:
\begin{align}
    \mathbf{v}_t &= \tanh\!\left( \mathbf{W} \mathbf{h}_t + \mathbf{b} \right) \label{eq:v_bahdanau} \\
    e_t &= \mathbf{u}^\top \mathbf{v}_t \label{eq:attention_score}
\end{align}

Normalize scores using softmax:
\begin{equation}
    \alpha_t = \frac{\exp(e_t)}{\sum_{j=1}^{T} \exp(e_j)} \label{eq:alpha_score}
\end{equation}
The attention weights $\boldsymbol{\alpha} = [\alpha_1, \dots, \alpha_T]$ indicate the importance of each time step.

Then compute the context vector:
\begin{equation}
    \mathbf{c} = \sum_{t=1}^{T} \alpha_t \mathbf{h}_t \label{eq:attention_context}
\end{equation}

Source for equations: \cite{cristina_2023_bahdanau}
\\[2em]
An attention score is calculated for each timestep in \eqref{eq:v_bahdanau}, \eqref{eq:attention_score}, it is then normalized \eqref{eq:alpha_score}. Finally the attention weights scale their respective hidden-states from the LSTMs, to compute the context vector \eqref{eq:attention_context}.

\section{Transformers} \label{sec:transformers}
As of the most recent findings by \cite{zhao2025surveylargelanguagemodels}, transformer architectures remain the state-of-the-art and most prevalent models across a majority of tasks; they form the basis for LLMs. The key to their success is multi-head self-attention.
\\[1em]
In this document, we will discuss the transformer architecture as first proposed by \cite{vaswani2023attentionneed}.

\subsection{Multi-Head Self-Attention}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\paperwidth]{images/dotproduct_1.png}
    \caption{Two diagrams from \cite{vaswani2023attentionneed} with the scaled dot-product attention described on the left and multi-head attention on the right.}
    \label{fig:MHAttention}
\end{figure}

Data is first split equally into multiple heads, which process it in parallel. There, the tensors are linearly projected with trainable weights to obtain queries (Q), keys (K), and values (V), which can be seen at the bottom of the image.
$$ Q = X W_Q, \quad K = X W_K, \quad V = X W_V $$
Afterwards, the dot-product between the queries and the keys is calculated and scaled\footnote{According to \cite{vaswani2023attentionneed} this is done to counteract dot-products growing too large and later overwhelming the softmax function.} to determine how similar they are. This leaves us with a number between 0 and 1, representing how much attention to pay. The value $V$ represents the actual information of the token for which we just calculated the attention. This means our final step is to scale the value $V$ by its attention. Pay attention to the equation below.\footnote{K has a T, this means the matrix is transposed. This is necessary for multiplication because the Q and K matrices have the same number of rows and columns.} \eqref{eq:sdpAttention}
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V \label{eq:sdpAttention}
\end{equation}

$$\text{where}\footnotemark\text{: }\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i=1, \dots, K$$
\footnotetext{Effectively the softmax function sets the biggest element in a set to 1 and the smallest element in the set to 0. All elements in between are scaled appropriately so all elements add up to 1.}

This is done across all heads in parallel. The results are then concatenated\footnote{This means they are reassembled to have the same dimensions as before they were split up.} back together and they undergo a linear projection. As described below:
\begin{gather*}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O \\
\text{where: } \text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
\end{gather*}

\newpage
\subsection{The Encoder Layer}

\begin{wrapfigure}{l}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{images/encodingLayer.png}
    \caption{Encoder Layer as described by \cite{vaswani2023attentionneed}}
    \label{fig:encodingLayer}
\end{wrapfigure}
In figure \ref{fig:encodingLayer} the multi-head attention first undergoes a residual connection as well as a layer normalization\footnote{Given an input vector, layer normalization works by normalizing its features and making it so their mean is 0.}, as described below:
\begin{gather}
    \text{where Sublayer(x) is the function, the residual connection is formed by:} \nonumber \\
    \text{Output}(x) = \text{LayerNorm}\bigl(x + \text{Sublayer}(x)\bigr) \label{eq:ResConnect}
\end{gather}
Afterwards, all tokens are fed into a point-wise FNN, where they are processed separately and independently. This part is crucial, to introduce non-linearity into the system, as multi-head attention is linear, and non-linearity is needed for a model to be able to learn.

\subsection{Point-wise Feed-Forward Network}
Equation \eqref{eq:pointwiseFNN} and figure \ref{fig:pointwiseFNN}:
The pointwise FNN works by taking in a number of $d\_model$ tokens, then a layer with a number of $d\_FNN$ neurons with weights and biases $W_1$ and $b_1$ is applied to them individually. The activation function ReLU discards all negative values and the output layer with weights and biases $W_2$ and $b_2$ resets the data back to its original dimensionality.

Please turn your attention to figure \ref{fig:encodingLayer}. After the resulting residual connection from the MHAttention \eqref{eq:ResConnect} is processed by a pointwise FNN, its residual connection will be the output of a single Encoder Layer as shown in figure \ref{fig:encodingLayer}. 

A regressive transformer model, like the one used in this project, consists of multiple such encoding layers and last but not least a single neuron, which works as the output layer.
\begin{equation}
    \text{pointwiseFNN}(x) = \max\bigl(0, \, x W_1 + b_1 \bigr) W_2 + b_2 \label{eq:pointwiseFNN}
\end{equation}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\paperwidth]{images/pointwiseFNN.png}
    \caption{A diagram from \cite{emil2020what} representing the structure of a pointwise FNN.}
    \label{fig:pointwiseFNN}
\end{figure}

\newpage

\subsection{Positional Encoding}
A special characteristic of the transformer is that data is passed through it as a whole. This means the model does not have positional understanding on its own. To counteract this effect, a positional encoding is applied to the input sequences after the tokenizer.

Let:
\begin{itemize}
    \item $d_{model}$: dimensionality of the sequences passed to the model
    \item $pos$: index of the token inside the sequence
    \item $i$ an integer that indexes half of the model's dimensions
\end{itemize}
Formulas as described by \cite{vaswani2023attentionneed}: 
\begin{align}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align}

This project implements positional encoding as described by \cite{vaswani2023attentionneed}. Very vaguely put, it assigns a number to each token, based on its position in the sequence.

\section{Fine-Tuned Pre-Trained LLMs}
The last model type used for this project are fine-tuned Large Language Models (LLMs).
For sources and further reading on this section see (in chronological order; left to right): \cite{geeksforgeeks_2024,Stryker_LLM,srinivasan2024transformer,Bergmann_Fine_Tuning}

\subsection{LLM}
LLMs adopt the transformer architecture\footnote{The transformer architecture discussed in the previous section \ref{sec:transformers}. But now it not only consists of an encoder but also has a decoder, which is responsible for the model's output} and adapt it to imense sizes, by increasing the architecture's hyperparameters, as well as employing new technological advancements. The number of parameters used by a LLM varies from model to model, but lies in the hundreds of billions per popular high-end LLM.
The large size allows large architectures to excel at complicated tasks like mostly Natural Language Processing (NLP).
To facilitate training and prediction on such large architectures, massive supercomputers are used, consisting of thousands of GPUs. They mainly work by utilizing parallel processing to distribute the load across multiple powerful GPUs. While by far not a supercomputer, a tiny replica was used for this project: The Nvidia Jetson Orin Nano Super Developer Kit.

\subsection{Fine-Tuning Process}
Most of the time LLMs are trained on huge dumps of unlabeled data. For more details on this view \cite{kili2023}. 
This training approach is called unsupervised learning. When training on unlabelled data from which ground truth can be inferred, the approach is called self-supervised data. 
The fine-tuning process on the other hand is mostly done on smaller datasets with supervised data, meaning they are labeled.
\\[2em]
In the fine-tuning process, a pre-trained model, with its weights and biases already optimized, is trained again on additional data. This means that the model's weights will be slightly adjusted from their pre-trained state. The optimizer will find a new minimum in which the model will have learned additional information from the fine-tuning training data. When evaluated on test data, the newly fine-tuned model is expected to perform better than its pre-trained counterpart.

\subsection{Regression vs. seq2seq Models}
Please note that the models fine-tuned in this section are not regression models but sequence-to-sequence models, which means that inside the model, tokens are processed, not numbers, like in the other models discussed. For the task in this project, specifically, fine-tuned models will output an integer rather than a float, because for seq2seq models solving an expression is a classification task, rather than a regression task. 

This is the reason why the only metric that makes sense for these models is accuracy, which is the percentage of correctly answered expressions from test data.

\section{Findings}
Lastly, the most noteworthy architectures were evaluated and scored on a benchmark. A custom benchmark was defined for this rather unique project, so that it takes into account the different generalization capabilities of models. For details, see the methodology section: 1.5 The Benchmark. 

An evaluation was performed on 5 training runs of each model, where the average of the 5 performances yielded the results in the following table\footnotemark: 
\footnotetext{Mean Absolute Error (MAE) is the average deviation of the predicted answer, to the correct answer. And Mean Relative Error (MRE) is the deviation of the predicted answer, to the correct answer divided by the correct answer.}
\\[0.5em]
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{|lr|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|l|}{Regression models:}                                 & \multicolumn{1}{l|}{FNN2}                          & \multicolumn{1}{l|}{FNN3}                          & \multicolumn{1}{l|}{RNN2}                          & \multicolumn{1}{l|}{Bidirectional   LSTM}          & \multicolumn{1}{l|}{transformer4}                  & \multicolumn{1}{l|}{transformer5}                  \\ \hline
\multicolumn{2}{|l|}{Total   Parameters:}                                & 6’211                                              & \multicolumn{1}{l|}{8’893}                         & 222’464                                            & 19’535                                             & 114’628                                            & 1’494’724                                          \\
\multicolumn{2}{|l|}{Architecture   Parameters:}                         & 6’211                                              & \multicolumn{1}{l|}{8’893}                         & 222,464                                            & 6’511                                              & 38’209                                             & 498’241                                            \\
\multicolumn{2}{|l|}{Optimizer   Parameters:}                            & -                                                  & \multicolumn{1}{l|}{-}                             & -                                                  & 13’024                                             & 76’419                                             & 996’483                                            \\
\rowcolor[HTML]{F7C7AC} 
MAE in Range:                                &                           & 0.026854                                           & 0.027486                                           & 0.244980                                           & 0.814113                                           & 0.039309                                           & 0.036588                                           \\
\rowcolor[HTML]{F7C7AC} 
MRE in Range:                                &                           & 0.010966                                           & 0.011122                                           & 0.093975                                           & 0.302951                                           & 0.016319                                           & 0.014936                                           \\
\rowcolor[HTML]{94DCF8} 
MAE out Range:                               &                           & 2.178343                                           & \cellcolor[HTML]{83CCEB}2.371700                   & 3.737668                                           & 3.970515                                           & 4.419579                                           & 4.901888                                           \\
\rowcolor[HTML]{94DCF8} 
MRE out Range:                               &                           & 0.250031                                           & \cellcolor[HTML]{83CCEB}0.275299                   & 0.363630                                           & 0.433860                                           & 0.495377                                           & 0.539891                                           \\
\rowcolor[HTML]{FFDC6D} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{FFDC6D}MAE   long Expressions:}    & 6.155671                                           & 5.647490                                           & 5.519240                                           & 2.931804                                           & 3.886400                                           & 3.801507                                           \\ \hline
\rowcolor[HTML]{D86DCD} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{D86DCD}Benchmark   score:}         & 10.702839                                          & 8.908331                                           & 0.665370                                           & 2.202380                                           & 5.023883                                           & 4.771119                                           \\ \hline
\rowcolor[HTML]{B5E6A2} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{B5E6A2}Benchmark   score p-value:} & \multicolumn{1}{l|}{\cellcolor[HTML]{B5E6A2}0.000} & \multicolumn{1}{l|}{\cellcolor[HTML]{B5E6A2}0.000} & \multicolumn{1}{l|}{\cellcolor[HTML]{B5E6A2}0.001} & \multicolumn{1}{l|}{\cellcolor[HTML]{B5E6A2}0.147} & \multicolumn{1}{l|}{\cellcolor[HTML]{B5E6A2}0.000} & \multicolumn{1}{l|}{\cellcolor[HTML]{B5E6A2}0.001} \\ \hline
\end{tabular}
\end{table}

\subsection{p-Value}

The p-values\footnote{Two-sided one-sample t-test was calculated on the natural logarithmic transformation of 5 benchmarks.} calculated symbolize the propability that the next benchmark calculated will be $\le 1$ (or $\ge 1$ if the model's benchmark is smaller than 1, which is the case for RNN2). 
Small p-values ($<0.05$) signify that the models will perform better than the baseline model (or respectively worse in the case of RNN2). This proves the significance of the results.
\\[1em]
To formalize this, let:
\begin{itemize}

    \item $x_1, x_2, ..., x_n$ be the benchmark values where $n = 5$
    \item $y_i = \ln(x_i)$ be the log-transformed values for $i = 1, 2, ..., n$
    \item $\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i$ be the sample mean of the log-transformed values
    \item $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(y_i - \bar{y})^2}$ be the sample standard deviation
    \item $\mu_0 = 0$ be the hypothesized population mean
    \item $t$ be the test statistic
    \item $P(condition)$ be the propability of a condition being true
    \item $T_{n-1}$ follow a t-distribution with $n-1$ degrees of freedom

\end{itemize}
First described by \cite{87464e34-37bc-3288-807c-e421e5a0d7a6}, the p-value is calculated as:

$$t = \frac{\bar{y} - \mu_0}{s/\sqrt{n}} = \frac{\bar{y}}{s/\sqrt{n}}$$

$$p\text{-value} = 2 \cdot P(T_{n-1} \geq |t|)$$

\subsection{Interpretaion}

Firstly, notice how the FNNs dominate the benchmark, with the FNN2 being the best model out of the bunch. 

The bi-directional LSTM with attention is a very interesting model, because it performs bad on most datasets in comparison to other models, but it excels at longer expressions, which only slightly hinder the model's performance.

It is also notable that the main hypothesis -- that more complex\footnotemark models would perform better -- is not supported by the results. This is made apparent by the clear outliers, such as the outstanding performance of the FNN2 (which is small in complexity) or the poor performance of the RNN2 model, which consists of the second to most parameters. 
\footnotetext{The complexity of a neural network depends in part on the number of parameters, and in part on the type of architecture. Where a large number of parameters and a more advanced architecture correlate to a higher complexity}

Also, notice the minimal improvement of the long expressions MAE between FNN2 and FNN3, which includes positional encoding. It can be derived that applying a positional encoding to the input only improves the model's performance when predicting longer expressions, on other test datasets used in this evaluation it only lead to a decrease in performance.

\subsection{Fine-Tuned Models}
The fine-tuned language models were also assessed based on their accuracy, here are the results from one training run each:
\\[0.5em]
\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{|ll|l|l|l|}
\hline
\multicolumn{2}{|l|}{fine-tuned Language Models:}                          & Gemini 2.5 Pro & Gemma 3 1B & Gemma 3 270M \\ \hline
Parameter size:                               &                            & 1.4E+11        & 1.00E+09   & 2.70E+08     \\
\rowcolor[HTML]{F7C7AC} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{F7C7AC}Accuracy   in Range:}         & 95.17          & 93.41      & 54.22        \\
\rowcolor[HTML]{94DCF8} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{94DCF8}Accuracy   out Range:}        & 99.51          & 63.33      & 9.67         \\
\rowcolor[HTML]{FFDC6D} 
\multicolumn{2}{|l|}{\cellcolor[HTML]{FFDC6D}Accuracy   long expressions:} & 90             & 39.57      & 28           \\ \hline
\end{tabular}
\end{table}

Notice how the less complex models with fewer parameters are less accurate. Additionally, only the performance of smaller models seems to be impacted by longer expressions and expressions with numbers outside of the range the models were fine-tuned on, Gemini 2.5 Pro even shows a better performance on the latter.


\subsection{Guided Discussion Experiment}

After conducting an open presentation for a colleague, his feedback was collected and his knowledge of the presented topics graded. 

The starting hypothesis and the reason for conducting this discussion was to test if regression models (like the ones in this project) are easier to teach and understand for people interested in neural networks, who already posses some basic knowledge.

The Results from the questionnaire yield that the discussion partner did not fully understand either topic\footnote{The FNN and transformer architectures of regression models were taught.}, but was able to answer basic questions. The topic of FNNs was easier to grasp than transformers.

According to the subject, the direct examples from this project which were delivered in accordance to theory, were the most helpful in forming an understanding. 

The use of Regression models instead of sequence to sequence (seq2seq) models also helped. The reasons listed by the subject were the similarities between the regression in the output layer and linear regression as taught in school, as well as the usage of mostly numbers throughout the whole model architecture, from input to output. The subject also stated that the latter helped with the understanding of tokens or vectors and how they are processed. Prior to this discussion this was not apparent to the subject.

The feedback included criticism about the lack of information and explanation regarding the optimization process. This is understandable, because the focus for this project was placed elsewhere.

for more details, see \href{https://github.com/AntonStantan/matura/tree/main/documentation/Questionnaire.pdf}{the questionnaire} itself with the participant's responses and some comments by the author.

In conclusion, though only evaluated on a very small dataset, it can be said that it is beneficial to explain the basic workings of FNNs and transformers on the basis of models for regression, because of the wider use of numbers, as well as better connections to topics like linear regression, discussed as a part of the standard curriculum in school.

\section{Discussion}
\subsection{Regression Models}
All of the 4 architectures discussed here are very different, and all of them have different strengths and weaknesses, as one can tell by their performances on different sets of test data. Recurrent Neural Networks excel at long expressions, due to the way data is passed through these models sequentially in hidden-states. They are designed for handling longer inputs. Similarly, Bidirectional LSTMs with attention are even more effective when adjusting to a longer input. They can confidently predict longer expressions the best out of all the models studied in this project. This is because vanishing gradients are punished heavily when solving arithmetic expressions by nature, and this model combats this well, as opposed to the simplistic RNN architecture. Additionally, the attention mechanism not only helps with vanishing gradients in longer expressions but also emphasizes outstandingly large numbers and their corresponding signs. 

When it comes to transformers, they perform best on data just like the one they are trained on, but not exactly the same; the classical definition of validation data. There are too many different types of different parameters, which all have been trained to process training data; this means even the smallest differences in input will lead to the model processing them wrong. Transformers are bad at generalizing rules from data, they perform the worst out of all models on expressions with numbers outside of the training range. Their decent performance on loger expressions shows that the attention is distributed well to numbers inside of the training range. transformer5 with more parameters than transformer4 doesn't show a performance improvement on the benchmark. This underlines that a more complex model doesn't necessarily mean a better performance for this task, rather the loss landscape is shaped with sweet-spots at the hyper parameters of transformer4 and transoformer5. Imagine it looking similar to the loss landscape drawn in figure \ref{fig:landscape_top}.

The FNN architecture performed with the best benchmark. The FNN2 model was the best at expressions of the same length with numbers not encountered in the training data -- The least complex model was best at generalization. Still, its abilities will not suffice to label it as 'able to generalize', primarily also because of its weakness at longer expressions. Even after adding positional encoding in FNN3, the model struggles at grasping expressions of different sizes.

A model's parameter count does not directly correlate with its benchmark performance. Instead, optimal performance is often achieved within specific parameter ranges. These "sweet-spots" can be identified through systematic methods such as hyperparameter optimization (i.e. with Keras-Tuner) or by empirically analyzing the performance of models of varying sizes (i.e. with a heatmap). In some cases, multiple performance optima may exist, as was observed with the transformer4 and transformer5 models.

\subsection{Pre-trained Fine-tuned Transformers}
The models in the lower table of the two are all very large, with Gemini 2.5 being particularly massive. Because we know that these models have the same transformer architecture and the sweet-spot for hyperparameters is much smaller than their parameter size, we expect a weaker performance.\footnote{Since these are seq2seq models, their hyper-parameters sweet-spot would not be the same as the regression transformers. And their Decoder multiplies the number of parameters roughly by 2x. This is still not comparable to the jump in complexity between the largest optimized transformer regression model and the smallest pre-trained model, which is a factor of roughly 180x.}. Additionally, they predict on tokens rather than numerical values, which is not optimal for the task in this project. The performance of these models also depends heavily on the pre-training they previously underwent because it also involves arithmetics \cite{wei2022emergentabilitieslargelanguage}. This explains why the bigger pre-trained models perform better -- due to better pre-training. The Gemini 2.5 Pro model performed the best out of all models evaluated in this project on generalization tests, but this is most likely due to previous training, including expressions similar to those in the test data. For this reason it cannot be said that it is able to generalize.

\subsection{Drop-Out}
When introducing a Dropout with industry-standard values, contrary to the expectation of reducing overfitting (which is present to some extent, according to literature discussed in the literature review), the models are still not able to generalize beyond the training range, this is different to overfitting because validation data is being predicted with a simillar loss as training data.

The MSEs of models with Dropout are higher than those of the previous models without Dropout. This is because dropout effectively decreases the computing capacity of a model during training (when predicting, this is no longer the case) by deactivating a percentage of randomly chosen neurons in each layer. This explains their weak performance and why the KerasTuner usually prioritizes models without dropout.


\subsection{Conclusion}
The benchmark results expose that for generalization, less complex models tend to perform better than more complex ones.
In the results, both the FNN models outperformed the more sophisticated transformer architecture models.
FNNs are, overall, the best at generalizing and perform best on the defined benchmark. The downside is, that they struggle with longer expressions\footnote{and in general with longer, sequential inputs as shown by \cite{Goodfellow-et-al-2016}.}. They are the best architecture for solving simple arithmetic expressions with a reasonable\footnotemark supply of computational resources.
\footnotetext{Publicly available, affordable hardware e.g., The Nvidia Jetson Orin Nano Super Developer Kit GPU used in this project.}
Adding positional encoding barely makes a difference for FNNs, as is shown in the difference between the FNN2 and FNN3 models, it only slightly improves the performance on longer expressions. 
For this type of data sequential processing, like it is done in RNNs is best. While the simple RNN architecture is a weak model, bidirectional LSTMs with attention are better. By solving the vanishing gradient problem, the model solves longer expressions the best.

Transformers on the other hand, excel at learning patterns from training data and achieve strong performance on validation datasets; however, they exhibit reduced generalization capability compared to other model architectures. Optimal deployment requires training on large, diverse datasets that adequately represent the distribution of data the model will encounter during inference. 

If maximum performance with unlimited computational resources is the goal, a fine-tuned, cutting-edge pre-trained transformer model such as Gemini 2.5 Pro will yield the best results.
\\[0.5em]
Throughout evaluation the benchmark proved essential, to assessing the generalization capabilities of different models. This is thanks to the shifts in numbers used in the expressions, as well as the length of the expressions.
\\[0.5em]
From the findings a broader implication for neural networks regardless of size or architecture can be drawn (though admittedly a little far-fetched): 

Neural networks excel at interpolation within a the training distribution and struggle at extrapolating symbolic rules. This means neural networks should be treated as pattern-retrieval algorithms not rule-learners like our human brains.
\\[0.5em]
Despite the chance factor playing a role, the conclusions will hold true. Conclusions were reached with significant margins and are backed by logical explanations. All results are reproducible with \href{https://github.com/AntonStantan/matura/tree/main}{publicly available notebooks}.

\subsection{Possible Future Work}
The topic chosen for this project is very broad and current. There's a lot of possible future work that can be done.
\\[0.5em]
A logical and important continuation of this project is using the benchmark as the loss function; this might lead to the training of better-performing models, as well as a struggle with overfitting. 
\\[0.5em]
The fine-tuned models have been evaluated very sparsly in this study. A possible continuation would be to evaluate them in more detail. (In this study this wasn't done, because of the computational costs.)
\\[0.5em]
Another possible direction is improving models by evaluating different activation functions, optimizers, or training data and using the best one, or even designing one's own. Training data and its tokenizer and padding is an area with lots of potential for improvement, i.e., by using an additional embedding, or finding the best training data, to teach a neural network simple arithmetics. 
\\[0.5em]
Also, consider this a proposal to evaluate a model architecture: an FNN with attention seems to be promising. Judging by the results of this study, this model is expected to improve the FNN's performance with longer expressions than in training, as well as strengthen its ability to accurately and confidently predict test data inside of the number range, previously referred to as the classical validation data.
\\[0.5em]
A minimal but still interesting branch can be formed from this project to find the error or investigate the exact reason for the accuracy missmatch observed during and after fine-tuning.

\newpage
\printbibliography[heading=bibintoc]
\end{document}