\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{apa/global//global/global/global}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1}Brief Summary of the Project}{1}{subsection.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2}Introduction to this document}{1}{subsection.0.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces You can see the aformentioned Nvidia Jetson device booting up.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:JetsonBoot}{{1}{2}{You can see the aformentioned Nvidia Jetson device booting up}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Feed-forward Neural Networks (FNN)}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Train and Test data}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Training a Neural Network Using Tensorflow}{4}{subsection.1.2}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{trask2018neuralarithmeticlogicunits}
\abx@aux@segm{0}{0}{trask2018neuralarithmeticlogicunits}
\abx@aux@page{1}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}FNN1 and FNN2 Notebooks}{6}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}The Benchmark}{6}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Drop-Out}{7}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Recurrent Neural Network (RNN)}{7}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Numerical Visualization of a RNN:}{7}{subsection.2.1}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bowman2015recursiveneuralnetworkslearn}
\abx@aux@segm{0}{0}{bowman2015recursiveneuralnetworkslearn}
\abx@aux@cite{0}{tensorflow_keras_rnn}
\abx@aux@segm{0}{0}{tensorflow_keras_rnn}
\abx@aux@cite{0}{ibm_rnn}
\abx@aux@segm{0}{0}{ibm_rnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}RNN0 and RNN2}{8}{subsection.2.2}\protected@file@percent }
\abx@aux@page{2}{8}
\abx@aux@page{3}{8}
\abx@aux@page{4}{8}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{geeksforgeeks_attention_bilstm}
\abx@aux@segm{0}{0}{geeksforgeeks_attention_bilstm}
\@writefile{toc}{\contentsline {section}{\numberline {3}Attention and Transformers}{9}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Attentional RNNs}{9}{subsection.3.1}\protected@file@percent }
\abx@aux@page{5}{9}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{vaswani2023attentionneed}
\abx@aux@segm{0}{0}{vaswani2023attentionneed}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{vaswani2023attentionneed}
\abx@aux@segm{0}{0}{vaswani2023attentionneed}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{vaswani2023attentionneed}
\abx@aux@segm{0}{0}{vaswani2023attentionneed}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{vaswani2023attentionneed}
\abx@aux@segm{0}{0}{vaswani2023attentionneed}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{loshchilov2019decoupledweightdecayregularization}
\abx@aux@segm{0}{0}{loshchilov2019decoupledweightdecayregularization}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Transformers}{10}{subsection.3.2}\protected@file@percent }
\abx@aux@page{6}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A Seq2Seq model from \blx@tocontentsinit {0}\cite {vaswani2023attentionneed}. The Encoder-only model used in this project, is the same, except the decoder part is skipped, in the image this part has been crossed out with red.}}{10}{figure.2}\protected@file@percent }
\newlabel{fig:transformerSeq2Seq}{{2}{10}{A Seq2Seq model from \cite {vaswani2023attentionneed}. The Encoder-only model used in this project, is the same, except the decoder part is skipped, in the image this part has been crossed out with red}{figure.2}{}}
\abx@aux@page{9}{10}
\abx@aux@page{10}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This is how the learning rate could look like for a model being trained over the period of 200 epochs, when using a cosine decay with a linear warmup, like in this project. (Except for the changing peak-learningrate all other parameters are the same as the ones used in this project.)}}{11}{figure.3}\protected@file@percent }
\newlabel{fig:learningrate}{{3}{11}{This is how the learning rate could look like for a model being trained over the period of 200 epochs, when using a cosine decay with a linear warmup, like in this project. (Except for the changing peak-learningrate all other parameters are the same as the ones used in this project.)}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Pre-trained Transformers}{12}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Gemini 2.5 Pro}{12}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gemma 3}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Weird Results Encountered After Evaluation}{13}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Closing Remark}{15}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{16}{section*.2}\protected@file@percent }
\abx@aux@page{11}{16}
\abx@aux@page{12}{16}
\abx@aux@page{13}{16}
\abx@aux@page{14}{16}
\abx@aux@page{15}{16}
\abx@aux@page{16}{16}
\abx@aux@page{17}{16}
\abx@aux@read@bbl@mdfivesum{DDB62824F977A2CAB6B5A82BAE98B943}
\abx@aux@defaultrefcontext{0}{geeksforgeeks_attention_bilstm}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bowman2015recursiveneuralnetworkslearn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{loshchilov2019decoupledweightdecayregularization}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{trask2018neuralarithmeticlogicunits}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vaswani2023attentionneed}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ibm_rnn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tensorflow_keras_rnn}{apa/global//global/global/global}
\gdef \@abspage@last{16}
