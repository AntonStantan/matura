# -*- coding: utf-8 -*-
"""FNN1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sek7rICQu7h8A-vxXGQ-8zIXsaD1NQKo
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from sklearn.model_selection import train_test_split
import pandas as pd

np.random.seed(42)

x1 = np.random.rand(4000) * 10
x2 = np.random.rand(4000) * 10
x3 = np.random.rand(4000) * 10

x1_int = x1.astype(int) - 5
x2_int = x2.astype(int) - 5
x3_int = x3.astype(int) - 5

x1_str = x1_int.astype(str)
x2_str = x2_int.astype(str)
x3_str = x3_int.astype(str)

unique_expressions = set()

for i in range(len(x1)):
    n = np.random.rand(1)
    if n < 0.25:
        opp1 = " + "
        opp2 = " + "
    elif n > 0.25 and n < 0.5:
        opp1 = " + "
        opp2 = " - "
    elif n > 0.75:
        opp1 = " - "
        opp2 = " + "
    else:
        opp1 = " - "
        opp2 = " - "
    unique_expressions.add(x1_str[i] + opp1 + x2_str[i] + opp2 + x3_str[i])

x = list(unique_expressions)
y = []

for expression in x:
    result = float(eval(expression))
    y.append(result)

def tokenizer(input_list):
    tokenized_x = [expression.split(" ") for expression in input_list]
    for i in range(len(tokenized_x)):
        for j in range(len(tokenized_x[i])):
            if j % 2 == 0:
                tokenized_x[i][j] = np.float32(tokenized_x[i][j])
            else:
                if tokenized_x[i][j] == "+":
                    tokenized_x[i][j] = np.float32(1)
                else:
                    tokenized_x[i][j] = np.float32(0)
        padding_count = 15 - len(tokenized_x[i])
        for _ in range(padding_count):
            tokenized_x[i].append(np.float32(0.5))
    tokenized_x = np.array(tokenized_x)
    return tokenized_x

x = tokenizer(x)

all_possible_expressions = set()
for num1 in range(-5, 5):
    for num2 in range(-5, 5):
        for num3 in range(-5, 5):
            for op1 in [" + ", " - "]:
                for op2 in [" + ", " - "]:
                    expression = str(num1) + op1 + str(num2) + op2 + str(num3)
                    all_possible_expressions.add(expression)

expressions_not_in_x = all_possible_expressions - unique_expressions
expressions_not_in_x = list(expressions_not_in_x)

y_test = []
for i in expressions_not_in_x:
    result = float(eval(i))
    y_test.append(result)
x_test = tokenizer(expressions_not_in_x)

x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.75)
x_train, x_val, y_train, y_val = np.array(x_train), np.array(x_val), np.array(y_train), np.array(y_val)

model = keras.Sequential([
    keras.Input(shape=(len(x[0]),)),
    layers.Dense(30),
    layers.PReLU(),
    layers.Dense(30),
    layers.PReLU(),
    layers.Dense(1, activation="linear")
])

model.compile(optimizer="adam", loss="mse")

early_stopping = keras.callbacks.EarlyStopping(
    patience=20,
    min_delta=0.001,
    restore_best_weights=True,
    monitor='val_loss',
    mode = "min"
)

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=200,
    callbacks=[early_stopping],
    verbose=1
)

# Outside number range test data
outsideExpr = set()
range1 = range(5, 9)
range2 = range(-8, -4)
comboRange = list(range1) + list(range2)
for num1 in comboRange:
    for num2 in comboRange:
        for num3 in comboRange:
            for op1 in [" + ", " - "]:
                for op2 in [" + ", " - "]:
                    expression = str(num1) + op1 + str(num2) + op2 + str(num3)
                    outsideExpr.add(expression)

outsideExpr = list(outsideExpr)
out_x_test = tokenizer(outsideExpr)
out_y_test = []
for i in outsideExpr:
    result = float(eval(i))
    out_y_test.append(result)

# Longer expressions test data
minNums = 2
maxNums = 8
amountNums = 100
num_terms = np.arange(minNums - 1, maxNums)
longer_Exps = []
for j in num_terms:
    for p in range(amountNums):
        longer_Exp = ""
        for i in range(j):
            longer_Exp += str(np.random.randint(-5, 6))
            longer_Exp += np.random.choice([" + ", " - "])
        longer_Exp += str(np.random.randint(-5, 6))
        longer_Exps.append(longer_Exp)

longer_Exps = list(longer_Exps)
long_x_test = tokenizer(longer_Exps)
long_y_test = []
for i in longer_Exps:
    result = float(eval(i))
    long_y_test.append(result)


def posTokenizer(input_list):
    tokenized_x = [expression.split(" ") for expression in input_list]
    max_sequence_length = 15
    positionally_encoded_x = []
    for i in range(len(tokenized_x)):
        current_sequence = []
        for j in range(len(tokenized_x[i])):
            token_val = tokenized_x[i][j]
            if j % 2 == 0:
                encoded_val = np.float32(token_val)
            else:
                if token_val == "+":
                    encoded_val = np.float32(1)
                else:
                    encoded_val = np.float32(0)
            positional_component = np.sin(j / (10000**(0/1)))
            if j % 2 != 0:
                positional_component = np.cos(j / (10000**(0/1)))
            current_sequence.append([encoded_val, np.float32(positional_component)])
        padding_count = max_sequence_length - len(current_sequence)
        for _ in range(padding_count):
            padding_position = len(current_sequence) + _
            padding_positional_component = np.sin(padding_position / (10000**(0/1)))
            if padding_position % 2 != 0:
                padding_positional_component = np.cos(padding_position / (10000**(0/1)))
            current_sequence.append([np.float32(0.5), np.float32(padding_positional_component)])
        positionally_encoded_x.append(current_sequence)
    tokenized_x = np.array(positionally_encoded_x)
    return tokenized_x

def absSum(x):
    expressions = x
    p = []
    for i in range(len(expressions)):
        components = expressions[i].split(" ")
        p.append(abs(int(components[0])) + abs(int(components[2])) + abs(int(components[4])))
    return p

n_bootstrap = 10
predictions = []
out_predictions = []
long_predictions = []

for _ in range(n_bootstrap):
    sample_indices1 = np.random.choice(len(x_train), size=len(x_train), replace=True)
    x_train_bootstrap = x_train[sample_indices1]
    y_train_bootstrap = np.array(y_train)[sample_indices1]
    sample_indices2 = np.random.choice(len(x_val), size=len(x_val), replace=True)
    x_val_bootstrap = x_val[sample_indices2]
    y_val_bootstrap = np.array(y_val)[sample_indices2]
    bootstrap_model = keras.models.clone_model(model)
    bootstrap_model.compile(optimizer="adam", loss="mse")
    bootstrap_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_bootstrap, y_train_bootstrap)).batch(32)
    bootstrap_val_dataset = tf.data.Dataset.from_tensor_slices((x_val_bootstrap, y_val_bootstrap)).batch(32)
    bootstrap_model.fit(
        bootstrap_train_dataset,
        validation_data = bootstrap_val_dataset,
        epochs=50,
        verbose=1,
        callbacks=[]
    )
    predictions.append(bootstrap_model.predict(x_test, verbose=0))
    out_predictions.append(bootstrap_model.predict(out_x_test, verbose=0))
    long_predictions.append(bootstrap_model.predict(long_x_test, verbose=0))

predictions = np.array(predictions)
out_predictions = np.array(out_predictions)
long_predictions = np.array(long_predictions)

meanPredictions = np.mean(predictions, axis=0)
out_meanPredictions = np.mean(out_predictions, axis=0)
long_meanPredictions = np.mean(long_predictions, axis=0)

deviation = np.abs(np.array(y_test).reshape(-1, 1) - meanPredictions)
out_deviation = np.abs(np.array(out_y_test).reshape(-1, 1) - out_meanPredictions)
long_deviation = np.abs(np.array(long_y_test).reshape(-1, 1) - long_meanPredictions)

relativeError = np.where(np.array(y_test) != 0, deviation.flatten() / np.abs(np.array(y_test)), deviation.flatten())
out_relativeError = np.where(np.array(out_y_test) != 0, out_deviation.flatten() / np.abs(np.array(out_y_test)), out_deviation.flatten())
avRelError = np.mean(np.concatenate([relativeError, out_relativeError]))

long_deviations_reshaped = long_deviation.reshape(len(num_terms), amountNums)
long_meanDeviations = np.mean(long_deviations_reshaped, axis=1)

grouped_deviations = {}
for deviation_val, placeholder_val in zip(out_deviation.flatten(), absSum(outsideExpr)):
    if placeholder_val not in grouped_deviations:
        grouped_deviations[placeholder_val] = []
    grouped_deviations[placeholder_val].append(deviation_val)

mean_deviations_by_placeholder = {key: np.mean(val) for key, val in grouped_deviations.items()}

baseline_deviation = np.mean(deviation)**2
baeline_out_deviation = mean_deviations_by_placeholder[22]**2
baseline_long_deviation = long_meanDeviations[2]**2
baseline_relError = avRelError**2