{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7c9865-4871-4cef-a480-767fa2c14844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bitsandbytes GPU Verification Script ---\n",
      "\n",
      "Step 1: Checking for CUDA-enabled GPU...\n",
      "âœ… Success: CUDA is available. Found GPU: Orin\n",
      "\n",
      "Step 2: Loading a model with 8-bit quantization (`load_in_8bit=True`)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: Model loaded in 8-bit without errors.\n",
      "   This indicates that bitsandbytes is correctly installed and communicating with the GPU.\n",
      "\n",
      "Step 3: Verifying model properties...\n",
      "   - Model is on device: cuda:0\n",
      "   âœ… Model is correctly placed on the CUDA device.\n",
      "   - Model memory footprint: 165.54 MB\n",
      "\n",
      "Step 4: Performing a simple inference test (forward pass)...\n",
      "âœ… Success: Forward pass completed without errors.\n",
      "\n",
      "--- Verification Complete ---\n",
      "ðŸŽ‰ All checks passed! Your `bitsandbytes` installation appears to be working correctly with your GPU.\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "#A lot of code from: https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora\n",
    "%run verifyHuggingFacePkgs.py\n",
    "#check if bfloat or float\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    dtype = torch.bfloat16\n",
    "    print(\"nice\")\n",
    "else:\n",
    "    dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3233cb-5fa5-44ee-ba6a-b2f560263511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3 + -5 + -5\n",
      "2543\n",
      "-13.0\n",
      "\n",
      "Expressions not in x:\n",
      "0 - -3 + -1\n",
      "True\n",
      "1457\n",
      "2.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"gemini2.5.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_string, y, expressions_not_in_x, y_test\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751dcfa8-1ae7-4843-867a-1f5ec026e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 2.0\n"
     ]
    }
   ],
   "source": [
    "x_test = expressions_not_in_x[:100]\n",
    "y_test_string = [str(i) for i in y_test]\n",
    "y_test_string = y_test_string[:100]\n",
    "print(y_test[0], y_test_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445ecbb9-6845-42df-a50a-5fd1cbeee18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.0\n"
     ]
    }
   ],
   "source": [
    "y_string = []\n",
    "for entry in y: \n",
    "    y_string.append(str(entry))\n",
    "print(y_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97469934-5138-4649-970d-818dbf971948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b89269-c615-42a2-9f56-6e175a471054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': '-3 + -5 + -5'}, {'role': 'assistant', 'content': '-13.0'}]}\n",
      "[{'content': '-3 + -5 + -5', 'role': 'user'}, {'content': '-13.0', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "def create_conversation(x_list, y_list):\n",
    "    conversations = []\n",
    "    for x, y in zip(x_list, y_list):\n",
    "        conversations.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": x},\n",
    "                {\"role\": \"assistant\", \"content\": y}\n",
    "            ]\n",
    "        })\n",
    "    return conversations\n",
    "\n",
    "# System message for the assistant\n",
    "#system_message = \"\"\n",
    "\n",
    "# User prompt that combines the user query and the schema\n",
    "#user_prompt = \"\"\n",
    "\n",
    "\n",
    "dataset = create_conversation(x_string, y_string)\n",
    "val_dataset = create_conversation(x_test, y_test_string)\n",
    "print(dataset[0])\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(dataset)\n",
    "val_dataset = Dataset.from_list(val_dataset)\n",
    "print(dataset[\"messages\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5377610a-7127-47eb-a689-ca376e147d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3302e646239e454181a2e52725e3d3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40a18cd5d3941af890b77b681ecf9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d12541c91b4a6da592fc0f688343f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a67047796f4e168e4a3807a6d1c12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca5f6bcc5524b6487e5c73f886066c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f3d9e261124fe5a1821dd2df0bebd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3233bc84abf240c1a57fcdfe2fccc409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6def862ca5f4e16b0c94f2a7b160d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", #flash_attention_2 or eager\n",
    "    dtype=dtype, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['dtype'],\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b660cf-b0ae-4697-8177-dbae86b3c793",
   "metadata": {},
   "source": [
    "# if tokenizer.pad_token_id is None:\n",
    "    print(\"new\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#tokenizer.apply_chat_template(dataset, tokenize = False)\n",
    "def format_chat_template(example):\n",
    "    conversation = example['messages']\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation,          # <-- Pass the list of messages for ONE conversation\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False # False for training examples\n",
    "    )\n",
    "    return {'text': formatted_text}\n",
    "formatted_dataset = dataset.map(format_chat_template)\n",
    "formatted_val_dataset = val_dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09caf103-a5f3-4c0d-b9fe-1c2fc4cfa03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"new\")\n",
    "def formatting_func_for_trainer(example):\n",
    "    # This takes one example (a dict with a 'messages' key)\n",
    "    # and returns the formatted string.\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example['messages'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7406bb83-ab82-49c6-9d71-b5e7a82f3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original code from https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.preprocess_logits_for_metrics\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may be a tuple of tensors\n",
    "        # here we take only the first tensor\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8d9015-58aa-414d-8d93-240631b7ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca97d0f7-a7cb-4ef2-b9c7-33bb14f59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"big_output\",         # directory to save and repository id\n",
    "    max_length=64,                         # max sequence length for model and packing of the dataset\n",
    "    packing=False,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    #num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,          # batch size per device during training\n",
    "    per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    eval_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"adamw_torch\",              # use fused adamw optimizer\n",
    "    logging_steps = 10,                       # log every 10 steps\n",
    "    eval_steps = 10,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=1e-4,                     # learning rate, based on QLoRA paper\n",
    "    lr_scheduler_kwargs={\"min_lr\": 1e-6},\n",
    "    fp16=True if dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    },\n",
    "    do_eval = True,\n",
    "    load_best_model_at_end=True, # Recommended: loads the best ckpt at the end of training\n",
    "    metric_for_best_model=\"accuracy\", # <-- Tell the trainer to watch accuracy\n",
    "    greater_is_better=True,         # <-- Tell the trainer that higher accuracy is better\n",
    ")\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1bd5f3b-084f-40be-8995-e43b7104534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback generated with gemini 2.5\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class LearningRateLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Log the learning rate at the end of each epoch.\n",
    "        \"\"\"\n",
    "        if logs is not None and \"learning_rate\" in logs:\n",
    "            print(f\"Epoch: {state.epoch:.2f}, Learning Rate: {logs['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1976f27-6e63-4e0d-8463-574b2f59ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ae39da-246c-4862-a7b0-1eba2c1109d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly generated by mistral\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    preds, labels = eval_preds\n",
    "    # Replace -100 (the ignore index) in labels with the pad_token_id for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "    #print(preds[0], labels[0])\n",
    "\n",
    "    # Decode predicted tokens to text\n",
    "    # We set skip_special_tokens=False to keep the template's special tokens for parsing\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "\n",
    "    # Debugging: Print the first decoded prediction and label to see what they look like\n",
    "    #print(\"----------- DEBUG -----------\\n\")\n",
    "    #print(f\"Sample Decoded Pred:\\n'{decoded_preds[0]}'\\n\")\n",
    "    #print(f\"Sample Decoded Label:\\n'{decoded_labels[0]}'\\n\")\n",
    "    #print(\"---------------------------\\n\")\n",
    "\n",
    "    # This is the separator that Gemma uses to indicate the start of the model's response\n",
    "    turn_separator = \"<start_of_turn>model\\n\"\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        \n",
    "        # We only care about the text that comes *after* the model's turn separator\n",
    "        if turn_separator in label:\n",
    "            total_predictions += 1          \n",
    "            # Extract the response part from the label\n",
    "            label_response = label.split(turn_separator, 1)[1]\n",
    "            clean_label = label_response.replace(\"<end_of_turn>\", \"\").replace(\"<pad>\", \"\").strip()\n",
    "            #print(f\"nice: {clean_label}\")\n",
    "\n",
    "            # Extract the response part from the prediction\n",
    "            if turn_separator in pred:\n",
    "                pred_response = pred.split(turn_separator, 1)[1]\n",
    "                clean_pred = pred_response.replace(\"<end_of_turn>\", \"\") \\\n",
    "                                          .replace(\"<pad>\", \"\") \\\n",
    "                                          .replace(\"<start_of_turn>\", \"\") \\\n",
    "                                          .replace(\"model\", \"\") \\\n",
    "                                          .strip()\n",
    "                #print(f\"ok: {clean_pred} nice: {clean_label}\")\n",
    "            else:\n",
    "                # If the model didn't generate the turn separator, it's an incorrect prediction\n",
    "                clean_pred = \"\"\n",
    "                label_pred = \"1\"\n",
    "            \n",
    "            if clean_pred == clean_label:\n",
    "                correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "compute_metrics = partial(compute_metrics, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5825bc3e-1a13-4069-80c5-dc5f98a8caa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f4230e6dcc46099a9d2050ff5c9803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c047a0a7427f446f865de5d19c73957e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113768ab3c094ba3971bfc8747689652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a871457c28b4f7eb3db07d2c0b88bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062ffa3333ab433a83952713306c52bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3d1e7c5f9944c8a9e151c2936e0346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    formatting_func=formatting_func_for_trainer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    peft_config=peft_config,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    processing_class = tokenizer\n",
    "    #callbacks = [LearningRateLoggerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbcdec4f-444a-46f6-8375-0a163f7d8da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='954' max='954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [954/954 2:12:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.826500</td>\n",
       "      <td>7.876888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.884970</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>0.252115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.876200</td>\n",
       "      <td>3.012336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.948377</td>\n",
       "      <td>3530.000000</td>\n",
       "      <td>0.570034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.048400</td>\n",
       "      <td>1.667517</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.943490</td>\n",
       "      <td>5296.000000</td>\n",
       "      <td>0.698224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.313300</td>\n",
       "      <td>1.020532</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>1.146231</td>\n",
       "      <td>7072.000000</td>\n",
       "      <td>0.704736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.620917</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.880041</td>\n",
       "      <td>8831.000000</td>\n",
       "      <td>0.771265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>0.509277</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.569259</td>\n",
       "      <td>10588.000000</td>\n",
       "      <td>0.753065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.465207</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.443799</td>\n",
       "      <td>12360.000000</td>\n",
       "      <td>0.760434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.456651</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.475915</td>\n",
       "      <td>14124.000000</td>\n",
       "      <td>0.775345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.426300</td>\n",
       "      <td>0.446618</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.429511</td>\n",
       "      <td>15879.000000</td>\n",
       "      <td>0.770154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.444694</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.400550</td>\n",
       "      <td>17654.000000</td>\n",
       "      <td>0.777214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.436318</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.441154</td>\n",
       "      <td>19425.000000</td>\n",
       "      <td>0.774777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.423425</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.421828</td>\n",
       "      <td>21204.000000</td>\n",
       "      <td>0.770003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.451739</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.396718</td>\n",
       "      <td>22977.000000</td>\n",
       "      <td>0.781778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.432169</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.401315</td>\n",
       "      <td>24736.000000</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.429262</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.400493</td>\n",
       "      <td>26512.000000</td>\n",
       "      <td>0.771058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.421564</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.406665</td>\n",
       "      <td>28278.000000</td>\n",
       "      <td>0.775450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.418995</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.400686</td>\n",
       "      <td>30049.000000</td>\n",
       "      <td>0.790430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.391300</td>\n",
       "      <td>0.413388</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.392478</td>\n",
       "      <td>31824.000000</td>\n",
       "      <td>0.781976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.413845</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.396682</td>\n",
       "      <td>33577.000000</td>\n",
       "      <td>0.789404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.400100</td>\n",
       "      <td>0.411285</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.406930</td>\n",
       "      <td>35337.000000</td>\n",
       "      <td>0.783410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.401811</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.394267</td>\n",
       "      <td>37096.000000</td>\n",
       "      <td>0.793095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.414380</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.397753</td>\n",
       "      <td>38858.000000</td>\n",
       "      <td>0.787913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.413349</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.399779</td>\n",
       "      <td>40615.000000</td>\n",
       "      <td>0.792719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.423167</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.389824</td>\n",
       "      <td>42383.000000</td>\n",
       "      <td>0.782869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.407237</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.394708</td>\n",
       "      <td>44147.000000</td>\n",
       "      <td>0.790329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.402753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411977</td>\n",
       "      <td>45903.000000</td>\n",
       "      <td>0.788208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.409416</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.410452</td>\n",
       "      <td>47663.000000</td>\n",
       "      <td>0.783204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.403059</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.396169</td>\n",
       "      <td>49430.000000</td>\n",
       "      <td>0.788911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.407009</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.399568</td>\n",
       "      <td>51188.000000</td>\n",
       "      <td>0.778799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.419808</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.391886</td>\n",
       "      <td>52961.000000</td>\n",
       "      <td>0.783271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.408051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.398123</td>\n",
       "      <td>54729.000000</td>\n",
       "      <td>0.777008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.409272</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.400688</td>\n",
       "      <td>56474.000000</td>\n",
       "      <td>0.783923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.402871</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.411187</td>\n",
       "      <td>58242.000000</td>\n",
       "      <td>0.787912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.407417</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.409725</td>\n",
       "      <td>60012.000000</td>\n",
       "      <td>0.783742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.405034</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.406436</td>\n",
       "      <td>61773.000000</td>\n",
       "      <td>0.784270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.401856</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.402632</td>\n",
       "      <td>63542.000000</td>\n",
       "      <td>0.780682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.413976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414218</td>\n",
       "      <td>65301.000000</td>\n",
       "      <td>0.779109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.390100</td>\n",
       "      <td>0.404703</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.389990</td>\n",
       "      <td>67065.000000</td>\n",
       "      <td>0.788859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.392100</td>\n",
       "      <td>0.416021</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.403181</td>\n",
       "      <td>68832.000000</td>\n",
       "      <td>0.784579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.399794</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.412972</td>\n",
       "      <td>70596.000000</td>\n",
       "      <td>0.794854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.414464</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.380841</td>\n",
       "      <td>72375.000000</td>\n",
       "      <td>0.780740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.403512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.401825</td>\n",
       "      <td>74129.000000</td>\n",
       "      <td>0.785133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.404420</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.402536</td>\n",
       "      <td>75904.000000</td>\n",
       "      <td>0.779260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.405539</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.386322</td>\n",
       "      <td>77677.000000</td>\n",
       "      <td>0.787814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.399157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.408866</td>\n",
       "      <td>79433.000000</td>\n",
       "      <td>0.788865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.402202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.409605</td>\n",
       "      <td>81194.000000</td>\n",
       "      <td>0.787549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.398598</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.395791</td>\n",
       "      <td>82956.000000</td>\n",
       "      <td>0.783694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.403210</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.390808</td>\n",
       "      <td>84732.000000</td>\n",
       "      <td>0.782523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.400454</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.398978</td>\n",
       "      <td>86499.000000</td>\n",
       "      <td>0.783599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.396446</td>\n",
       "      <td>88266.000000</td>\n",
       "      <td>0.788531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.400520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.399732</td>\n",
       "      <td>90034.000000</td>\n",
       "      <td>0.779762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.403292</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388662</td>\n",
       "      <td>91801.000000</td>\n",
       "      <td>0.779243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.402328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386116</td>\n",
       "      <td>93571.000000</td>\n",
       "      <td>0.793785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.403899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387143</td>\n",
       "      <td>95335.000000</td>\n",
       "      <td>0.783676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.401778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395647</td>\n",
       "      <td>97099.000000</td>\n",
       "      <td>0.787402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.401837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395187</td>\n",
       "      <td>98852.000000</td>\n",
       "      <td>0.789774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.398937</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393968</td>\n",
       "      <td>100619.000000</td>\n",
       "      <td>0.787957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>0.396890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395488</td>\n",
       "      <td>102381.000000</td>\n",
       "      <td>0.791718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.400398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391524</td>\n",
       "      <td>104150.000000</td>\n",
       "      <td>0.784037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.405755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.382145</td>\n",
       "      <td>105917.000000</td>\n",
       "      <td>0.776040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.400328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387020</td>\n",
       "      <td>107690.000000</td>\n",
       "      <td>0.784781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.401249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.394489</td>\n",
       "      <td>109443.000000</td>\n",
       "      <td>0.782368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.381500</td>\n",
       "      <td>0.404981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.399407</td>\n",
       "      <td>111197.000000</td>\n",
       "      <td>0.778647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.402035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390988</td>\n",
       "      <td>112955.000000</td>\n",
       "      <td>0.783658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.401382</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387077</td>\n",
       "      <td>114712.000000</td>\n",
       "      <td>0.784905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.397979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388233</td>\n",
       "      <td>116482.000000</td>\n",
       "      <td>0.783717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.397660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391578</td>\n",
       "      <td>118247.000000</td>\n",
       "      <td>0.792827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395642</td>\n",
       "      <td>120006.000000</td>\n",
       "      <td>0.779847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.401188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393322</td>\n",
       "      <td>121765.000000</td>\n",
       "      <td>0.774999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.399686</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393601</td>\n",
       "      <td>123532.000000</td>\n",
       "      <td>0.779819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.396236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.389131</td>\n",
       "      <td>125313.000000</td>\n",
       "      <td>0.791440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.396352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386067</td>\n",
       "      <td>127070.000000</td>\n",
       "      <td>0.794184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.396127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386715</td>\n",
       "      <td>128833.000000</td>\n",
       "      <td>0.790375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.397109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390471</td>\n",
       "      <td>130606.000000</td>\n",
       "      <td>0.793231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.396950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391524</td>\n",
       "      <td>132370.000000</td>\n",
       "      <td>0.794810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.397773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392184</td>\n",
       "      <td>134130.000000</td>\n",
       "      <td>0.787904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.397431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393558</td>\n",
       "      <td>135909.000000</td>\n",
       "      <td>0.785521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.397310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392309</td>\n",
       "      <td>137663.000000</td>\n",
       "      <td>0.784555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.398136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392113</td>\n",
       "      <td>139423.000000</td>\n",
       "      <td>0.780744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.399683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388723</td>\n",
       "      <td>141205.000000</td>\n",
       "      <td>0.776413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.398589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388840</td>\n",
       "      <td>142986.000000</td>\n",
       "      <td>0.782198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.398942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.389762</td>\n",
       "      <td>144747.000000</td>\n",
       "      <td>0.778788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.398324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390321</td>\n",
       "      <td>146514.000000</td>\n",
       "      <td>0.781625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.389127</td>\n",
       "      <td>148280.000000</td>\n",
       "      <td>0.784157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.398311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390438</td>\n",
       "      <td>150045.000000</td>\n",
       "      <td>0.786571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.397546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391071</td>\n",
       "      <td>151806.000000</td>\n",
       "      <td>0.783275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.397228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391631</td>\n",
       "      <td>153572.000000</td>\n",
       "      <td>0.789398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.397732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393152</td>\n",
       "      <td>155355.000000</td>\n",
       "      <td>0.788003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.397702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392055</td>\n",
       "      <td>157116.000000</td>\n",
       "      <td>0.781374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.397971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391426</td>\n",
       "      <td>158869.000000</td>\n",
       "      <td>0.784213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.397952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393392</td>\n",
       "      <td>160642.000000</td>\n",
       "      <td>0.787421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.397884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392753</td>\n",
       "      <td>162413.000000</td>\n",
       "      <td>0.790391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.382100</td>\n",
       "      <td>0.397482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391619</td>\n",
       "      <td>164159.000000</td>\n",
       "      <td>0.781804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.398044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393155</td>\n",
       "      <td>165917.000000</td>\n",
       "      <td>0.779827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.397240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392868</td>\n",
       "      <td>167684.000000</td>\n",
       "      <td>0.784076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=954, training_loss=0.5763607355033826, metrics={'train_runtime': 7947.0746, 'train_samples_per_second': 0.96, 'train_steps_per_second': 0.12, 'total_flos': 747415822218240.0, 'train_loss': 0.5763607355033826, 'entropy': 0.39679994992911816, 'num_tokens': 168369.0, 'mean_token_accuracy': 0.7875087037682533, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa11c264-f9c6-4288-bfc9-54b2c63d9645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e43acb957246e7b4425894fd2bf8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8a948fe0b74302bc7a412b340f6ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4760370ad7449eb55cdad34686e300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...nformers/big_output/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cda3622aa8427e936e5fa02132c7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...anformers/big_output/tokenizer.json: 100%|##########| 33.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1231a625e34284a6ff694128c2f538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ig_output/adapter_model.safetensors:   1%|          |  558kB /  104MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f8aa9096eb45a491afc75b940ed053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1758058328.CapyJetson.1126.0:   1%|1         | 1.35kB / 90.2kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ebad2050f442a48c3ae46f367a772c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ormers/big_output/training_args.bin:   1%|1         |  94.0B / 6.29kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AntonBOOM/big_output/commit/5a0f3a51d173e7b89d8bccb44e3a80bfa19e1a65', commit_message='gemma-finetuned-math', commit_description='', oid='5a0f3a51d173e7b89d8bccb44e3a80bfa19e1a65', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AntonBOOM/big_output', endpoint='https://huggingface.co', repo_type='model', repo_id='AntonBOOM/big_output'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "hub_model_id = \"gemma-finetuned-math\"\n",
    "trainer.push_to_hub(hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d2011-652a-405f-ba1d-975f181df55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
