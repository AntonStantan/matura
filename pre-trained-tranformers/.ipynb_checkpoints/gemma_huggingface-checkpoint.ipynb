{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7c9865-4871-4cef-a480-767fa2c14844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bitsandbytes GPU Verification Script ---\n",
      "\n",
      "Step 1: Checking for CUDA-enabled GPU...\n",
      "âœ… Success: CUDA is available. Found GPU: Orin\n",
      "\n",
      "Step 2: Loading a model with 8-bit quantization (`load_in_8bit=True`)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: Model loaded in 8-bit without errors.\n",
      "   This indicates that bitsandbytes is correctly installed and communicating with the GPU.\n",
      "\n",
      "Step 3: Verifying model properties...\n",
      "   - Model is on device: cuda:0\n",
      "   âœ… Model is correctly placed on the CUDA device.\n",
      "   - Model memory footprint: 165.54 MB\n",
      "\n",
      "Step 4: Performing a simple inference test (forward pass)...\n",
      "âœ… Success: Forward pass completed without errors.\n",
      "\n",
      "--- Verification Complete ---\n",
      "ðŸŽ‰ All checks passed! Your `bitsandbytes` installation appears to be working correctly with your GPU.\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "#A lot of code from: https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora\n",
    "%run verifyHuggingFacePkgs.py\n",
    "#check if bfloat or float\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    dtype = torch.bfloat16\n",
    "    print(\"nice\")\n",
    "else:\n",
    "    dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3233cb-5fa5-44ee-ba6a-b2f560263511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5 - -2 + 3\n",
      "2543\n",
      "0.0\n",
      "\n",
      "Expressions not in x:\n",
      "-5 + 4 + -5\n",
      "True\n",
      "1457\n",
      "-6.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"gemini2.5.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_string, y\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445ecbb9-6845-42df-a50a-5fd1cbeee18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "y_string = []\n",
    "for entry in y: \n",
    "    y_string.append(str(entry))\n",
    "print(y_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97469934-5138-4649-970d-818dbf971948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b89269-c615-42a2-9f56-6e175a471054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': '-5 - -2 + 3'}, {'role': 'assistant', 'content': '0.0'}]}\n",
      "[{'content': '-5 - -2 + 3', 'role': 'user'}, {'content': '0.0', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "def create_conversation(x_list, y_list):\n",
    "    conversations = []\n",
    "    for x, y in zip(x_list, y_list):\n",
    "        conversations.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": x},\n",
    "                {\"role\": \"assistant\", \"content\": y}\n",
    "            ]\n",
    "        })\n",
    "    return conversations\n",
    "\n",
    "# System message for the assistant\n",
    "#system_message = \"\"\n",
    "\n",
    "# User prompt that combines the user query and the schema\n",
    "#user_prompt = \"\"\n",
    "\n",
    "\n",
    "dataset = create_conversation(x_string, y_string)\n",
    "print(dataset[0])\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(dataset)\n",
    "print(dataset[\"messages\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5377610a-7127-47eb-a689-ca376e147d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-3-270m-it\"\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", #flash_attention_2 or eager\n",
    "    dtype=dtype, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['dtype'],\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fee903b-0abb-46ec-986f-d2608e9d10e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0776b111c108486bb6219ad1f97e819b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"new\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#tokenizer.apply_chat_template(dataset, tokenize = False)\n",
    "def format_chat_template(example):\n",
    "    conversation = example['messages']\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation,          # <-- Pass the list of messages for ONE conversation\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False # False for training examples\n",
    "    )\n",
    "    return {'text': formatted_text}\n",
    "formatted_dataset = dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8d9015-58aa-414d-8d93-240631b7ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca97d0f7-a7cb-4ef2-b9c7-33bb14f59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"output\",         # directory to save and repository id\n",
    "    max_length=64,                         # max sequence length for model and packing of the dataset\n",
    "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=4,          # batch size per device during training\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch\",              # use fused adamw optimizer\n",
    "    logging_steps=5,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=5e-4,                     # learning rate, based on QLoRA paper\n",
    "    fp16=True if dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd5f3b-084f-40be-8995-e43b7104534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback generated with gemini 2.5\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class LearningRateLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Log the learning rate at the end of each epoch.\n",
    "        \"\"\"\n",
    "        if logs is not None and \"learning_rate\" in logs:\n",
    "            print(f\"Epoch: {state.epoch:.2f}, Learning Rate: {logs['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5825bc3e-1a13-4069-80c5-dc5f98a8caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2772fe344b1044adaff44f30045980c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1561981461bc47fc869eb8f36c665a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=peft_config,\n",
    "    callbacks = [LearningRateLoggerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcdec4f-444a-46f6-8375-0a163f7d8da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 14:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.534200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.380300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.376000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=213, training_loss=0.632643804863585, metrics={'train_runtime': 851.2538, 'train_samples_per_second': 3.996, 'train_steps_per_second': 0.25, 'total_flos': 105186520403712.0, 'train_loss': 0.632643804863585, 'entropy': 0.39116691301266354, 'num_tokens': 168369.0, 'mean_token_accuracy': 0.7938015460968018, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa11c264-f9c6-4288-bfc9-54b2c63d9645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a8fd0dc2ab4ac8b1498a16c00784a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a289f61f644c2290baa280290afeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83ca8cc67fc4ccd948c2b9a5b2c9d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...fevents.1757103285.CapyJetson.816.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7142f0cc864ada9f6df64cc302a0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757079979.CapyJetson.2201.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4927e854288a48db90804857b787fd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757080067.CapyJetson.2271.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67533b7f76d441c6a0a32390f33fcb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757107076.CapyJetson.1642.0: 100%|##########| 23.1kB / 23.1kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444c8169ddc4430cb8077510d5179c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757106707.CapyJetson.1453.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b27b912ee4a4af4abb3c11f69305007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ranformers/output/training_args.bin: 100%|##########| 6.16kB / 6.16kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80295ece52f44c2d84ea9c448cd9bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105753.CapyJetson.1238.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f4e13ac7d64664b32bef8b74c9953e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105415.CapyJetson.1112.0: 100%|##########| 7.03kB / 7.03kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f9f4c15ca45079987629c9a431be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105415.CapyJetson.1112.0: 100%|##########| 7.03kB / 7.03kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f665db2675c94bb6a3b0ac61a3170dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757106265.CapyJetson.1382.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a83c09b5a34540b9c86db8471787e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757106265.CapyJetson.1382.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf16c4cf46e43c9ba0b74c804cc4231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105910.CapyJetson.1302.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14069ba320440bb939d7a4e7bd8c7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105910.CapyJetson.1302.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-09-05T21:32:28.725772Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'RuntimeError'>, value: RuntimeError('OrderedDict mutated during iteration'), traceback: Some(<traceback object at 0xfffe862e2240>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "hub_model_id = \"YourUsername/gemma-finetuned-math\"\n",
    "trainer.push_to_hub(hub_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
