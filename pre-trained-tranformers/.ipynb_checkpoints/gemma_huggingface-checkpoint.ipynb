{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7c9865-4871-4cef-a480-767fa2c14844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bitsandbytes GPU Verification Script ---\n",
      "\n",
      "Step 1: Checking for CUDA-enabled GPU...\n",
      "âœ… Success: CUDA is available. Found GPU: Orin\n",
      "\n",
      "Step 2: Loading a model with 8-bit quantization (`load_in_8bit=True`)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: Model loaded in 8-bit without errors.\n",
      "   This indicates that bitsandbytes is correctly installed and communicating with the GPU.\n",
      "\n",
      "Step 3: Verifying model properties...\n",
      "   - Model is on device: cuda:0\n",
      "   âœ… Model is correctly placed on the CUDA device.\n",
      "   - Model memory footprint: 165.54 MB\n",
      "\n",
      "Step 4: Performing a simple inference test (forward pass)...\n",
      "âœ… Success: Forward pass completed without errors.\n",
      "\n",
      "--- Verification Complete ---\n",
      "ðŸŽ‰ All checks passed! Your `bitsandbytes` installation appears to be working correctly with your GPU.\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "#A lot of code from: https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora\n",
    "%run verifyHuggingFacePkgs.py\n",
    "#check if bfloat or float\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    dtype = torch.bfloat16\n",
    "    print(\"nice\")\n",
    "else:\n",
    "    dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3233cb-5fa5-44ee-ba6a-b2f560263511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 3 + 1\n",
      "2543\n",
      "8.0\n",
      "\n",
      "Expressions not in x:\n",
      "0 - -5 - 4\n",
      "True\n",
      "1457\n",
      "1.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"gemini2.5.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_string, y, expressions_not_in_x, y_test\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751dcfa8-1ae7-4843-867a-1f5ec026e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 6.0, 6.0, -2.0, -2.0, 10.0, 3.0, 7.0, -7.0, 1.0, -1.0, 0.0, -3.0, 2.0, 5.0, -4.0, -2.0, 1.0, 4.0, 2.0, 7.0, -4.0, -5.0, -5.0, -1.0, -2.0, 0.0, -2.0, 3.0, 1.0, 2.0, -4.0, -4.0, -1.0, 1.0, 4.0, -9.0, -7.0, -4.0, -7.0, -4.0, 6.0, -6.0, 0.0, 6.0, -1.0, 7.0, -2.0, 4.0, 1.0, -7.0, 4.0, -13.0, -5.0, 4.0, 1.0, 3.0, -10.0, -9.0, -4.0, 1.0, 0.0, -1.0, -7.0, 2.0, 1.0, 0.0, 0.0, 4.0, -2.0, -7.0, 3.0, 1.0, -2.0, -2.0, -6.0, 1.0, 3.0, 1.0, -2.0, -9.0, 11.0, -4.0, 0.0, -7.0, -7.0, -2.0, -4.0, -3.0, 4.0, 0.0, -5.0, -2.0, -1.0, -2.0, 3.0, -6.0, 0.0, 3.0, -1.0, 5.0, 3.0, 0.0, 2.0, 0.0, 6.0, -8.0, 7.0, -10.0, 0.0, -7.0, 7.0, -13.0, -3.0, 1.0, -11.0, 6.0, 2.0, -2.0, 7.0, -1.0, 4.0, -3.0, -7.0, -12.0, -4.0, -9.0, 9.0, 3.0, 2.0, 2.0, -6.0, 0.0, 1.0, -4.0, 0.0, 0.0, -1.0, -7.0, -1.0, 0.0, -2.0, -6.0, -4.0, 2.0, -7.0, 0.0, 3.0, 0.0, -1.0, 3.0, -3.0, 1.0, 2.0, 2.0, 7.0, -6.0, -7.0, 1.0, 5.0, -8.0, -10.0, 7.0, -9.0, 5.0, -5.0, 4.0, 1.0, 1.0, -6.0, -1.0, 1.0, -4.0, 9.0, 1.0, -1.0, 0.0, 3.0, 3.0, 1.0, -3.0, 4.0, 3.0, 4.0, 2.0, -7.0, -3.0, 6.0, 8.0, 0.0, 1.0, 0.0, -12.0, 3.0, 8.0, 8.0, -5.0, -5.0, -7.0, 8.0, -2.0, -8.0, 5.0, -1.0, 0.0, -9.0, -3.0, 5.0, 6.0, 5.0, 1.0, 6.0, 11.0, 4.0, -4.0, -1.0, 2.0, 7.0, -4.0, -2.0, 2.0, -5.0, 2.0, 12.0, 6.0, -1.0, -5.0, 3.0, -2.0, -2.0, 4.0, -5.0, -5.0, 1.0, 5.0, -6.0, 2.0, -9.0, -1.0, 8.0, -2.0, 5.0, -7.0, 1.0, 1.0, 1.0, 1.0, -2.0, 6.0, -8.0, 3.0, -4.0, -4.0, -6.0, 10.0, 1.0, -5.0, -2.0, -1.0, 0.0, 3.0, 0.0, 5.0, 2.0, -2.0, -1.0, 6.0, 7.0, 2.0, 6.0, 1.0, -1.0, -3.0, -4.0, -3.0, -3.0, -5.0, -7.0, 2.0, -2.0, -8.0, 5.0, -3.0, 8.0, -4.0, 0.0, -11.0, -4.0, -3.0, 5.0, -10.0, -3.0, -1.0, 4.0, -6.0, 0.0, 7.0, 8.0, -3.0, -6.0, 1.0, -9.0, -4.0, -2.0, -1.0, 5.0, -5.0, 2.0, 2.0, 5.0, 0.0, 9.0, 7.0, -2.0, -7.0, 13.0, 2.0, -1.0, 1.0, -6.0, -5.0, -5.0, -9.0, -6.0, 2.0, 3.0, -4.0, -6.0, -5.0, 1.0, 0.0, 1.0, -1.0, -2.0, 0.0, -4.0, -9.0, 1.0, 5.0, -13.0, -7.0, 8.0, -12.0, 2.0, -2.0, -6.0, 5.0, 1.0, 2.0, 4.0, 3.0, -8.0, 1.0, 1.0, 4.0, 4.0, 7.0, 4.0, -8.0, -1.0, 1.0, 0.0, 0.0, 10.0, -4.0, 5.0, 1.0, -1.0, 6.0, 4.0, 3.0, -9.0, -5.0, 2.0, 3.0, 3.0, -2.0, -2.0, 5.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -3.0, -8.0, -3.0, -5.0, 2.0, -1.0, 0.0, 2.0, -5.0, 4.0, -3.0, 3.0, 11.0, -4.0, 3.0, -1.0, 6.0, -5.0, -10.0, 4.0, -5.0, 3.0, -2.0, 1.0, -1.0, 5.0, 3.0, 9.0, -9.0, 3.0, -2.0, -1.0, 0.0, 0.0, 5.0, -4.0, -4.0, -3.0, 7.0, 6.0, 5.0, -9.0, 5.0, 6.0, -8.0, 2.0, -5.0, -4.0, 2.0, 7.0, 4.0, -1.0, 1.0, 3.0, -8.0, 5.0, -5.0, -3.0, 5.0, 4.0, -6.0, -1.0, -6.0, -4.0, -4.0, -4.0, -3.0, 4.0, -5.0, 2.0, 8.0, 1.0, -4.0, -4.0, -3.0, 7.0, 8.0, -8.0, -2.0, -10.0, 2.0, 2.0, 2.0, -2.0, -11.0, -7.0, 4.0, 0.0, 6.0, -7.0, -4.0, -2.0, -4.0, -8.0, -11.0, -1.0, -3.0, 10.0, 2.0, -3.0, 0.0, 6.0, -1.0, -1.0, 2.0, -3.0, -1.0, -1.0, 0.0, 0.0, -3.0, 11.0, 7.0, 0.0, -4.0, 5.0, -6.0, 1.0, 6.0, 2.0, -9.0, 4.0, -4.0, 1.0, -5.0, -4.0, -5.0, -2.0, -11.0, -4.0, 7.0, 4.0, 4.0, -2.0, 8.0, 9.0, 6.0, -7.0, 2.0, -10.0, 4.0, 1.0, 6.0, 6.0, -2.0, 1.0, 7.0, -4.0, -2.0, -7.0, -10.0, 2.0, 7.0, -5.0, 5.0, -3.0, -10.0, -1.0, -2.0, 2.0, 3.0, -4.0, 3.0, 2.0, -5.0, 5.0, 5.0, 2.0, 5.0, -4.0, 3.0, -2.0, -1.0, -11.0, -1.0, -2.0, 8.0, 2.0, 6.0, -4.0, -5.0, 1.0, 2.0, -7.0, -11.0, 1.0, -1.0, -5.0, -5.0, 1.0, 4.0, 8.0, -8.0, -3.0, 1.0, 6.0, -3.0, -1.0, -4.0, -6.0, 6.0, 3.0, -9.0, 2.0, -2.0, -9.0, -4.0, -9.0, 4.0, 1.0, -3.0, 4.0, -1.0, 4.0, 0.0, -4.0, -2.0, -2.0, 2.0, 0.0, 2.0, 3.0, -7.0, 8.0, -1.0, 3.0, 4.0, 2.0, -11.0, 0.0, -1.0, 6.0, 3.0, 1.0, -2.0, 6.0, 5.0, -5.0, 5.0, -2.0, -2.0, -4.0, -3.0, 1.0, 10.0, 5.0, -1.0, -3.0, -2.0, 8.0, -5.0, -1.0, -7.0, 5.0, -4.0, 5.0, -4.0, -2.0, 2.0, 2.0, 4.0, 1.0, -8.0, -6.0, 5.0, 1.0, -4.0, -2.0, -3.0, 4.0, 5.0, 0.0, 6.0, -5.0, 9.0, -2.0, -3.0, 1.0, 2.0, 2.0, 0.0, -4.0, -2.0, 3.0, 10.0, -4.0, 5.0, -5.0, 5.0, -1.0, 3.0, -8.0, 6.0, 0.0, -1.0, 2.0, -6.0, -2.0, 5.0, 3.0, -4.0, 3.0, -1.0, -1.0, 6.0, -3.0, -14.0, 2.0, 6.0, 5.0, 5.0, 1.0, 0.0, 1.0, 0.0, 4.0, -8.0, -12.0, -10.0, -1.0, -7.0, -3.0, 3.0, 1.0, 0.0, 0.0, -4.0, 1.0, -6.0, -1.0, -2.0, 1.0, 2.0, -7.0, 6.0, 1.0, 9.0, 6.0, -1.0, -3.0, -9.0, 1.0, -2.0, 1.0, 1.0, -3.0, -8.0, 2.0, 0.0, 8.0, 9.0, -3.0, -6.0, 0.0, 3.0, 3.0, 7.0, 6.0, 2.0, 7.0, 9.0, -1.0, -8.0, 0.0, -3.0, -6.0, -10.0, -2.0, -3.0, -1.0, -5.0, 7.0, 7.0, -4.0, -5.0, -7.0, -3.0, -9.0, -4.0, -8.0, -1.0, 6.0, -9.0, 4.0, 0.0, -3.0, 2.0, 5.0, -1.0, -7.0, -1.0, 1.0, -3.0, -2.0, -8.0, -10.0, -10.0, -5.0, 0.0, 4.0, -3.0, -2.0, -3.0, 8.0, 2.0, 5.0, 0.0, 5.0, 3.0, -2.0, -8.0, -5.0, -6.0, 2.0, -2.0, 3.0, -1.0, -4.0, 6.0, -13.0, -1.0, 5.0, -5.0, 6.0, -2.0, -1.0, -2.0, -5.0, -7.0, -9.0, 8.0, 1.0, -4.0, 11.0, -7.0, 10.0, 2.0, 9.0, -3.0, -3.0, -1.0, -4.0, 2.0, -2.0, 4.0, 4.0, -6.0, -10.0, -2.0, 7.0, -7.0, -5.0, -2.0, -1.0, 3.0, -8.0, 1.0, -4.0, -2.0, 2.0, -9.0, 6.0, 4.0, -3.0, 0.0, -8.0, -1.0, 2.0, 3.0, 3.0, 5.0, 2.0, 1.0, 0.0, -9.0, 1.0, -1.0, -4.0, -3.0, 1.0, 7.0, -5.0, -1.0, 6.0, 0.0, 7.0, 1.0, 1.0, -3.0, 4.0, -14.0, -12.0, -2.0, 8.0, -4.0, -7.0, -7.0, -5.0, 0.0, 6.0, -1.0, 1.0, -2.0, -11.0, -3.0, 7.0, 5.0, -12.0, -3.0, 4.0, -2.0, 2.0, 5.0, 4.0, -4.0, 6.0, 2.0, 4.0, 3.0, -11.0, 4.0, -4.0, -1.0, -11.0, -3.0, -1.0, 4.0, 8.0, -2.0, 3.0, -1.0, 1.0, -5.0, -3.0, 3.0, -3.0, -1.0, 3.0, 0.0, 2.0, 4.0, -3.0, 8.0, -5.0, 5.0, 7.0, -3.0, 2.0, -1.0, -3.0, 4.0, -10.0, -4.0, 2.0, 1.0, -4.0, 5.0, -8.0, 2.0, -12.0, -3.0, 5.0, 4.0, 2.0, 0.0, 5.0, 0.0, -3.0, -1.0, -2.0, -7.0, 3.0, -1.0, 4.0, -5.0, -6.0, -5.0, -4.0, -6.0, -10.0, -3.0, -5.0, -5.0, -3.0, -8.0, -4.0, 4.0, 1.0, 3.0, -4.0, -4.0, 2.0, 9.0, -5.0, -4.0, -8.0, 5.0, 9.0, 6.0, -2.0, 3.0, -1.0, 8.0, -7.0, 2.0, 5.0, -3.0, 1.0, 1.0, 4.0, 5.0, -8.0, 8.0, -7.0, 6.0, 7.0, -2.0, -3.0, 10.0, -2.0, 0.0, 7.0, -4.0, 2.0, 1.0, 1.0, 9.0, -3.0, 11.0, 3.0, -9.0, 6.0, 8.0, -9.0, -2.0, -1.0, 6.0, 0.0, -1.0, 5.0, -10.0, 4.0, 1.0, 7.0, 0.0, -6.0, -1.0, 4.0, 5.0, 4.0, 1.0, 2.0, 3.0, 8.0, -3.0, 1.0, -2.0, -10.0, -8.0, -5.0, 1.0, -1.0, 0.0, -3.0, -4.0, 7.0, 3.0, -5.0, 2.0, -1.0, -5.0, 3.0, 9.0, -7.0, -4.0, -8.0, 2.0, -2.0, -2.0, -7.0, 4.0, 0.0, -4.0, 0.0, -6.0, 11.0, -2.0, -3.0, 5.0, -2.0, -9.0, -6.0, 11.0, -2.0, -3.0, -8.0, -3.0, 0.0, 0.0, -3.0, -2.0, -2.0, 6.0, 1.0, 2.0, -4.0, 7.0, -2.0, 5.0, 6.0, 5.0, -8.0, -3.0, -4.0, -6.0, 1.0, -6.0, -2.0, -4.0, -7.0, -11.0, 6.0, 4.0, -3.0, -1.0, 2.0, 7.0, 0.0, 0.0, 0.0, 3.0, 4.0, 2.0, 1.0, 3.0, -2.0, 0.0, -3.0, 3.0, 0.0, -3.0, 4.0, -2.0, -8.0, 1.0, -9.0, 0.0, 1.0, 8.0, -1.0, 11.0, 3.0, -4.0, 3.0, -5.0, 0.0, 5.0, -5.0, -4.0, 4.0, 1.0, -1.0, 8.0, 2.0, 5.0, -5.0, -12.0, 1.0, -2.0, 3.0, -1.0, 3.0, -2.0, -5.0, -1.0, -6.0, 4.0, 0.0, 5.0, -11.0, 0.0, 4.0, 1.0, 1.0, -7.0, -4.0, 4.0, -8.0, -7.0, 6.0, 0.0, 6.0, -2.0, -11.0, -1.0, -12.0, 10.0, 2.0, -5.0, -4.0, 3.0, 1.0, -6.0, -7.0, -4.0, 0.0, 4.0, -2.0, 7.0, -1.0, -2.0, -11.0, 3.0, 3.0, 1.0, -6.0, 1.0, 3.0, -3.0, -4.0, 6.0, -4.0, 2.0, -7.0, -5.0, -5.0, -1.0, 1.0, 8.0, 6.0, 0.0, 5.0, -5.0, 3.0, 1.0, 2.0, -3.0, 2.0, -12.0, 6.0, 2.0, 3.0, 3.0, 4.0, 8.0, 5.0, -3.0, 5.0, -5.0, -1.0, -3.0, -5.0, -1.0, -3.0, 3.0, 3.0, 3.0, 0.0, 2.0, 4.0, 3.0, 1.0, -5.0, 2.0, 2.0, -6.0, 2.0, 5.0, 1.0, -2.0, -1.0, -6.0, -3.0, 4.0, -2.0, 2.0, -4.0, 4.0, 5.0, -1.0, 0.0, -1.0, 0.0, -5.0, -9.0, 4.0, 0.0, 3.0, 4.0, -5.0, 2.0, -6.0, -6.0, -4.0, 4.0, 10.0, -3.0, 0.0, 0.0, -4.0, 2.0, -9.0, -1.0, 12.0, 5.0, -3.0, -8.0, 0.0, 9.0, 5.0, -7.0, 0.0, -4.0, -1.0, 0.0, 5.0, 6.0, 3.0, 3.0, 0.0, 4.0, -1.0, -3.0, -2.0, -1.0, -6.0, -8.0, 2.0, -6.0, 8.0, -3.0, 1.0, -1.0, 1.0, -3.0, -2.0, 2.0, 3.0, 4.0, 0.0, -1.0, 5.0, -3.0, 5.0, -2.0, 1.0, -6.0, 2.0, -2.0, -4.0, -1.0, -2.0, -4.0, 2.0, -7.0, 6.0, -6.0, 7.0, -4.0, -4.0, -1.0, 3.0, 5.0, -3.0, 6.0, 8.0, 4.0, 1.0, -4.0, 0.0, -4.0, -1.0, -2.0, -1.0, 0.0, -7.0, 9.0, 3.0, 2.0, -3.0, -6.0, 5.0, -3.0, 2.0, -5.0, 0.0, 3.0, 5.0, 12.0, 3.0, 7.0, -13.0, -8.0, -5.0, 4.0, -5.0, -5.0, 0.0, 4.0, 0.0, -4.0, -6.0, -9.0, -1.0, -2.0, 7.0, 7.0, 6.0, -2.0, -6.0, 3.0, 10.0, 3.0, 4.0, -5.0, -5.0, 7.0, 6.0, -5.0, 8.0, -2.0, -6.0, 2.0, -7.0, 4.0, -2.0, 2.0, -3.0, 8.0, -7.0, 5.0, 2.0, -6.0, -9.0, -4.0, 0.0, -3.0, 5.0, -11.0, -4.0, -5.0, 0.0, 1.0, -1.0, 3.0, 4.0, -4.0, -5.0, 0.0, 5.0, -2.0, 1.0, -5.0, -2.0, 0.0, -4.0, -7.0, -12.0, -2.0, -3.0, 1.0, -3.0, 4.0, 5.0, 6.0, -2.0, 0.0, -6.0, -8.0, 4.0, 4.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "x_test = expressions_not_in_x[:100]\n",
    "y_test = str(y_test)\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445ecbb9-6845-42df-a50a-5fd1cbeee18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "y_string = []\n",
    "for entry in y: \n",
    "    y_string.append(str(entry))\n",
    "print(y_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97469934-5138-4649-970d-818dbf971948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b89269-c615-42a2-9f56-6e175a471054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': '4 + 3 + 1'}, {'role': 'assistant', 'content': '8.0'}]}\n",
      "[{'content': '4 + 3 + 1', 'role': 'user'}, {'content': '8.0', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "def create_conversation(x_list, y_list):\n",
    "    conversations = []\n",
    "    for x, y in zip(x_list, y_list):\n",
    "        conversations.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": x},\n",
    "                {\"role\": \"assistant\", \"content\": y}\n",
    "            ]\n",
    "        })\n",
    "    return conversations\n",
    "\n",
    "# System message for the assistant\n",
    "#system_message = \"\"\n",
    "\n",
    "# User prompt that combines the user query and the schema\n",
    "#user_prompt = \"\"\n",
    "\n",
    "\n",
    "dataset = create_conversation(x_string, y_string)\n",
    "val_dataset = create_conversation(x_test, y_test)\n",
    "print(dataset[0])\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(dataset)\n",
    "val_dataset = Dataset.from_list(val_dataset)\n",
    "print(dataset[\"messages\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5377610a-7127-47eb-a689-ca376e147d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-3-270m-it\"\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", #flash_attention_2 or eager\n",
    "    dtype=dtype, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['dtype'],\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fee903b-0abb-46ec-986f-d2608e9d10e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00155fdfa4b4b58ae275c827a1c46b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcec76fc84c841eda6c336982a2a9fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"new\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#tokenizer.apply_chat_template(dataset, tokenize = False)\n",
    "def format_chat_template(example):\n",
    "    conversation = example['messages']\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation,          # <-- Pass the list of messages for ONE conversation\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False # False for training examples\n",
    "    )\n",
    "    return {'text': formatted_text}\n",
    "formatted_dataset = dataset.map(format_chat_template)\n",
    "formatted_val_dataset = val_dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8d9015-58aa-414d-8d93-240631b7ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca97d0f7-a7cb-4ef2-b9c7-33bb14f59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"output\",         # directory to save and repository id\n",
    "    max_length=64,                         # max sequence length for model and packing of the dataset\n",
    "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    #num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=4,          # batch size per device during training\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch\",              # use fused adamw optimizer\n",
    "    logging_steps=1,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=1e-4,                     # learning rate, based on QLoRA paper\n",
    "    lr_scheduler_kwargs={\"min_lr\": 1e-6},\n",
    "    fp16=True if dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    },\n",
    "    do_eval = True\n",
    ")\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1bd5f3b-084f-40be-8995-e43b7104534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback generated with gemini 2.5\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class LearningRateLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Log the learning rate at the end of each epoch.\n",
    "        \"\"\"\n",
    "        if logs is not None and \"learning_rate\" in logs:\n",
    "            print(f\"Epoch: {state.epoch:.2f}, Learning Rate: {logs['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5825bc3e-1a13-4069-80c5-dc5f98a8caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44ac323fea84a1f8ca5b408c2ac80a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c5ce53a00d4943b30c0b10b13fd9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bcf94f9c5842bfa77ac42d88947788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca3b7662a134593bb41d4b5eb9b70b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/1457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    eval_dataset = formatted_val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    callbacks = [LearningRateLoggerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbcdec4f-444a-46f6-8375-0a163f7d8da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 14:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.528200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.454100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.470400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.443900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.402800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.389100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.395300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.01, Learning Rate: 0.0\n",
      "Epoch: 0.03, Learning Rate: 1.4285714285714285e-05\n",
      "Epoch: 0.04, Learning Rate: 2.857142857142857e-05\n",
      "Epoch: 0.06, Learning Rate: 4.2857142857142856e-05\n",
      "Epoch: 0.07, Learning Rate: 5.714285714285714e-05\n",
      "Epoch: 0.08, Learning Rate: 7.142857142857143e-05\n",
      "Epoch: 0.10, Learning Rate: 8.571428571428571e-05\n",
      "Epoch: 0.11, Learning Rate: 0.0001\n",
      "Epoch: 0.13, Learning Rate: 9.999424385015958e-05\n",
      "Epoch: 0.14, Learning Rate: 9.99769767393559e-05\n",
      "Epoch: 0.15, Learning Rate: 9.994820268343049e-05\n",
      "Epoch: 0.17, Learning Rate: 9.990792837441476e-05\n",
      "Epoch: 0.18, Learning Rate: 9.985616317897363e-05\n",
      "Epoch: 0.20, Learning Rate: 9.979291913622715e-05\n",
      "Epoch: 0.21, Learning Rate: 9.97182109549506e-05\n",
      "Epoch: 0.23, Learning Rate: 9.963205601015345e-05\n",
      "Epoch: 0.24, Learning Rate: 9.953447433903861e-05\n",
      "Epoch: 0.25, Learning Rate: 9.942548863634226e-05\n",
      "Epoch: 0.27, Learning Rate: 9.930512424905573e-05\n",
      "Epoch: 0.28, Learning Rate: 9.91734091705305e-05\n",
      "Epoch: 0.30, Learning Rate: 9.903037403396768e-05\n",
      "Epoch: 0.31, Learning Rate: 9.887605210529368e-05\n",
      "Epoch: 0.32, Learning Rate: 9.871047927542347e-05\n",
      "Epoch: 0.34, Learning Rate: 9.853369405191333e-05\n",
      "Epoch: 0.35, Learning Rate: 9.834573755000514e-05\n",
      "Epoch: 0.37, Learning Rate: 9.814665348306404e-05\n",
      "Epoch: 0.38, Learning Rate: 9.793648815241212e-05\n",
      "Epoch: 0.39, Learning Rate: 9.771529043655982e-05\n",
      "Epoch: 0.41, Learning Rate: 9.748311177983834e-05\n",
      "Epoch: 0.42, Learning Rate: 9.724000618043508e-05\n",
      "Epoch: 0.44, Learning Rate: 9.698603017783518e-05\n",
      "Epoch: 0.45, Learning Rate: 9.672124283967201e-05\n",
      "Epoch: 0.46, Learning Rate: 9.644570574798981e-05\n",
      "Epoch: 0.48, Learning Rate: 9.615948298492138e-05\n",
      "Epoch: 0.49, Learning Rate: 9.586264111778435e-05\n",
      "Epoch: 0.51, Learning Rate: 9.555524918359957e-05\n",
      "Epoch: 0.52, Learning Rate: 9.523737867303506e-05\n",
      "Epoch: 0.54, Learning Rate: 9.490910351377924e-05\n",
      "Epoch: 0.55, Learning Rate: 9.457050005334747e-05\n",
      "Epoch: 0.56, Learning Rate: 9.422164704132581e-05\n",
      "Epoch: 0.58, Learning Rate: 9.386262561105607e-05\n",
      "Epoch: 0.59, Learning Rate: 9.349351926076647e-05\n",
      "Epoch: 0.61, Learning Rate: 9.311441383415232e-05\n",
      "Epoch: 0.62, Learning Rate: 9.272539750041125e-05\n",
      "Epoch: 0.63, Learning Rate: 9.232656073373752e-05\n",
      "Epoch: 0.65, Learning Rate: 9.191799629228018e-05\n",
      "Epoch: 0.66, Learning Rate: 9.14997991965704e-05\n",
      "Epoch: 0.68, Learning Rate: 9.10720667074221e-05\n",
      "Epoch: 0.69, Learning Rate: 9.063489830331204e-05\n",
      "Epoch: 0.70, Learning Rate: 9.018839565724382e-05\n",
      "Epoch: 0.72, Learning Rate: 8.973266261310172e-05\n",
      "Epoch: 0.73, Learning Rate: 8.92678051614994e-05\n",
      "Epoch: 0.75, Learning Rate: 8.879393141512962e-05\n",
      "Epoch: 0.76, Learning Rate: 8.831115158362028e-05\n",
      "Epoch: 0.77, Learning Rate: 8.781957794790266e-05\n",
      "Epoch: 0.79, Learning Rate: 8.73193248340982e-05\n",
      "Epoch: 0.80, Learning Rate: 8.681050858692937e-05\n",
      "Epoch: 0.82, Learning Rate: 8.62932475426612e-05\n",
      "Epoch: 0.83, Learning Rate: 8.576766200157963e-05\n",
      "Epoch: 0.85, Learning Rate: 8.523387420001312e-05\n",
      "Epoch: 0.86, Learning Rate: 8.469200828190383e-05\n",
      "Epoch: 0.87, Learning Rate: 8.41421902699353e-05\n",
      "Epoch: 0.89, Learning Rate: 8.358454803622319e-05\n",
      "Epoch: 0.90, Learning Rate: 8.301921127257574e-05\n",
      "Epoch: 0.92, Learning Rate: 8.244631146033123e-05\n",
      "Epoch: 0.93, Learning Rate: 8.186598183977907e-05\n",
      "Epoch: 0.94, Learning Rate: 8.127835737917186e-05\n",
      "Epoch: 0.96, Learning Rate: 8.06835747433357e-05\n",
      "Epoch: 0.97, Learning Rate: 8.00817722618857e-05\n",
      "Epoch: 0.99, Learning Rate: 7.947308989705448e-05\n",
      "Epoch: 1.00, Learning Rate: 7.885766921114095e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.01, Learning Rate: 7.823565333358683e-05\n",
      "Epoch: 1.03, Learning Rate: 7.760718692768884e-05\n",
      "Epoch: 1.04, Learning Rate: 7.697241615695417e-05\n",
      "Epoch: 1.06, Learning Rate: 7.633148865110687e-05\n",
      "Epoch: 1.07, Learning Rate: 7.568455347175343e-05\n",
      "Epoch: 1.08, Learning Rate: 7.503176107771521e-05\n",
      "Epoch: 1.10, Learning Rate: 7.437326329003608e-05\n",
      "Epoch: 1.11, Learning Rate: 7.370921325667302e-05\n",
      "Epoch: 1.13, Learning Rate: 7.303976541687827e-05\n",
      "Epoch: 1.14, Learning Rate: 7.23650754652812e-05\n",
      "Epoch: 1.15, Learning Rate: 7.168530031567805e-05\n",
      "Epoch: 1.17, Learning Rate: 7.100059806453829e-05\n",
      "Epoch: 1.18, Learning Rate: 7.031112795423577e-05\n",
      "Epoch: 1.20, Learning Rate: 6.961705033601355e-05\n",
      "Epoch: 1.21, Learning Rate: 6.891852663269068e-05\n",
      "Epoch: 1.23, Learning Rate: 6.821571930111983e-05\n",
      "Epoch: 1.24, Learning Rate: 6.750879179440441e-05\n",
      "Epoch: 1.25, Learning Rate: 6.6797908523884e-05\n",
      "Epoch: 1.27, Learning Rate: 6.608323482089695e-05\n",
      "Epoch: 1.28, Learning Rate: 6.5364936898329e-05\n",
      "Epoch: 1.30, Learning Rate: 6.46431818119568e-05\n",
      "Epoch: 1.31, Learning Rate: 6.39181374215955e-05\n",
      "Epoch: 1.32, Learning Rate: 6.318997235205934e-05\n",
      "Epoch: 1.34, Learning Rate: 6.24588559539442e-05\n",
      "Epoch: 1.35, Learning Rate: 6.172495826424154e-05\n",
      "Epoch: 1.37, Learning Rate: 6.098844996679256e-05\n",
      "Epoch: 1.38, Learning Rate: 6.024950235259196e-05\n",
      "Epoch: 1.39, Learning Rate: 5.95082872799505e-05\n",
      "Epoch: 1.41, Learning Rate: 5.876497713452566e-05\n",
      "Epoch: 1.42, Learning Rate: 5.8019744789229525e-05\n",
      "Epoch: 1.44, Learning Rate: 5.7272763564023556e-05\n",
      "Epoch: 1.45, Learning Rate: 5.652420718560916e-05\n",
      "Epoch: 1.46, Learning Rate: 5.577424974702387e-05\n",
      "Epoch: 1.48, Learning Rate: 5.502306566715212e-05\n",
      "Epoch: 1.49, Learning Rate: 5.427082965016045e-05\n",
      "Epoch: 1.51, Learning Rate: 5.351771664486618e-05\n",
      "Epoch: 1.52, Learning Rate: 5.276390180404933e-05\n",
      "Epoch: 1.54, Learning Rate: 5.2009560443717e-05\n",
      "Epoch: 1.55, Learning Rate: 5.125486800232984e-05\n",
      "Epoch: 1.56, Learning Rate: 5.05e-05\n",
      "Epoch: 1.58, Learning Rate: 4.974513199767017e-05\n",
      "Epoch: 1.59, Learning Rate: 4.899043955628301e-05\n",
      "Epoch: 1.61, Learning Rate: 4.823609819595068e-05\n",
      "Epoch: 1.62, Learning Rate: 4.7482283355133835e-05\n",
      "Epoch: 1.63, Learning Rate: 4.672917034983957e-05\n",
      "Epoch: 1.65, Learning Rate: 4.597693433284789e-05\n",
      "Epoch: 1.66, Learning Rate: 4.522575025297614e-05\n",
      "Epoch: 1.68, Learning Rate: 4.447579281439084e-05\n",
      "Epoch: 1.69, Learning Rate: 4.372723643597645e-05\n",
      "Epoch: 1.70, Learning Rate: 4.2980255210770484e-05\n",
      "Epoch: 1.72, Learning Rate: 4.2235022865474364e-05\n",
      "Epoch: 1.73, Learning Rate: 4.149171272004952e-05\n",
      "Epoch: 1.75, Learning Rate: 4.0750497647408065e-05\n",
      "Epoch: 1.76, Learning Rate: 4.001155003320745e-05\n",
      "Epoch: 1.77, Learning Rate: 3.927504173575848e-05\n",
      "Epoch: 1.79, Learning Rate: 3.854114404605583e-05\n",
      "Epoch: 1.80, Learning Rate: 3.781002764794069e-05\n",
      "Epoch: 1.82, Learning Rate: 3.7081862578404516e-05\n",
      "Epoch: 1.83, Learning Rate: 3.6356818188043225e-05\n",
      "Epoch: 1.85, Learning Rate: 3.5635063101671004e-05\n",
      "Epoch: 1.86, Learning Rate: 3.491676517910306e-05\n",
      "Epoch: 1.87, Learning Rate: 3.4202091476116025e-05\n",
      "Epoch: 1.89, Learning Rate: 3.349120820559562e-05\n",
      "Epoch: 1.90, Learning Rate: 3.278428069888019e-05\n",
      "Epoch: 1.92, Learning Rate: 3.208147336730933e-05\n",
      "Epoch: 1.93, Learning Rate: 3.138294966398646e-05\n",
      "Epoch: 1.94, Learning Rate: 3.068887204576424e-05\n",
      "Epoch: 1.96, Learning Rate: 2.999940193546172e-05\n",
      "Epoch: 1.97, Learning Rate: 2.931469968432196e-05\n",
      "Epoch: 1.99, Learning Rate: 2.8634924534718792e-05\n",
      "Epoch: 2.00, Learning Rate: 2.796023458312172e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.01, Learning Rate: 2.7290786743326985e-05\n",
      "Epoch: 2.03, Learning Rate: 2.6626736709963916e-05\n",
      "Epoch: 2.04, Learning Rate: 2.5968238922284788e-05\n",
      "Epoch: 2.06, Learning Rate: 2.531544652824658e-05\n",
      "Epoch: 2.07, Learning Rate: 2.4668511348893135e-05\n",
      "Epoch: 2.08, Learning Rate: 2.402758384304583e-05\n",
      "Epoch: 2.10, Learning Rate: 2.3392813072311158e-05\n",
      "Epoch: 2.11, Learning Rate: 2.2764346666413187e-05\n",
      "Epoch: 2.13, Learning Rate: 2.2142330788859062e-05\n",
      "Epoch: 2.14, Learning Rate: 2.1526910102945518e-05\n",
      "Epoch: 2.15, Learning Rate: 2.0918227738114308e-05\n",
      "Epoch: 2.17, Learning Rate: 2.031642525666431e-05\n",
      "Epoch: 2.18, Learning Rate: 1.9721642620828153e-05\n",
      "Epoch: 2.20, Learning Rate: 1.913401816022095e-05\n",
      "Epoch: 2.21, Learning Rate: 1.855368853966878e-05\n",
      "Epoch: 2.23, Learning Rate: 1.7980788727424268e-05\n",
      "Epoch: 2.24, Learning Rate: 1.7415451963776824e-05\n",
      "Epoch: 2.25, Learning Rate: 1.6857809730064712e-05\n",
      "Epoch: 2.27, Learning Rate: 1.6307991718096195e-05\n",
      "Epoch: 2.28, Learning Rate: 1.5766125799986897e-05\n",
      "Epoch: 2.30, Learning Rate: 1.523233799842037e-05\n",
      "Epoch: 2.31, Learning Rate: 1.4706752457338818e-05\n",
      "Epoch: 2.32, Learning Rate: 1.418949141307064e-05\n",
      "Epoch: 2.34, Learning Rate: 1.3680675165901798e-05\n",
      "Epoch: 2.35, Learning Rate: 1.3180422052097338e-05\n",
      "Epoch: 2.37, Learning Rate: 1.2688848416379737e-05\n",
      "Epoch: 2.38, Learning Rate: 1.2206068584870393e-05\n",
      "Epoch: 2.39, Learning Rate: 1.173219483850063e-05\n",
      "Epoch: 2.41, Learning Rate: 1.1267337386898307e-05\n",
      "Epoch: 2.42, Learning Rate: 1.0811604342756184e-05\n",
      "Epoch: 2.44, Learning Rate: 1.0365101696687971e-05\n",
      "Epoch: 2.45, Learning Rate: 9.927933292577898e-06\n",
      "Epoch: 2.46, Learning Rate: 9.50020080342961e-06\n",
      "Epoch: 2.48, Learning Rate: 9.082003707719816e-06\n",
      "Epoch: 2.49, Learning Rate: 8.673439266262502e-06\n",
      "Epoch: 2.51, Learning Rate: 8.274602499588747e-06\n",
      "Epoch: 2.52, Learning Rate: 7.885586165847684e-06\n",
      "Epoch: 2.54, Learning Rate: 7.506480739233543e-06\n",
      "Epoch: 2.55, Learning Rate: 7.137374388943941e-06\n",
      "Epoch: 2.56, Learning Rate: 6.7783529586742e-06\n",
      "Epoch: 2.58, Learning Rate: 6.429499946652554e-06\n",
      "Epoch: 2.59, Learning Rate: 6.090896486220787e-06\n",
      "Epoch: 2.61, Learning Rate: 5.762621326964951e-06\n",
      "Epoch: 2.62, Learning Rate: 5.444750816400436e-06\n",
      "Epoch: 2.63, Learning Rate: 5.13735888221566e-06\n",
      "Epoch: 2.65, Learning Rate: 4.840517015078631e-06\n",
      "Epoch: 2.66, Learning Rate: 4.55429425201018e-06\n",
      "Epoch: 2.68, Learning Rate: 4.278757160327995e-06\n",
      "Epoch: 2.69, Learning Rate: 4.013969822164834e-06\n",
      "Epoch: 2.70, Learning Rate: 3.7599938195649177e-06\n",
      "Epoch: 2.72, Learning Rate: 3.5168882201616556e-06\n",
      "Epoch: 2.73, Learning Rate: 3.2847095634401816e-06\n",
      "Epoch: 2.75, Learning Rate: 3.0635118475878932e-06\n",
      "Epoch: 2.76, Learning Rate: 2.853346516935964e-06\n",
      "Epoch: 2.77, Learning Rate: 2.654262449994884e-06\n",
      "Epoch: 2.79, Learning Rate: 2.466305948086668e-06\n",
      "Epoch: 2.80, Learning Rate: 2.28952072457653e-06\n",
      "Epoch: 2.82, Learning Rate: 2.1239478947063145e-06\n",
      "Epoch: 2.83, Learning Rate: 1.9696259660323216e-06\n",
      "Epoch: 2.85, Learning Rate: 1.8265908294695052e-06\n",
      "Epoch: 2.86, Learning Rate: 1.6948757509442647e-06\n",
      "Epoch: 2.87, Learning Rate: 1.5745113636577424e-06\n",
      "Epoch: 2.89, Learning Rate: 1.4655256609614026e-06\n",
      "Epoch: 2.90, Learning Rate: 1.367943989846561e-06\n",
      "Epoch: 2.92, Learning Rate: 1.2817890450494084e-06\n",
      "Epoch: 2.93, Learning Rate: 1.2070808637728417e-06\n",
      "Epoch: 2.94, Learning Rate: 1.143836821026384e-06\n",
      "Epoch: 2.96, Learning Rate: 1.092071625585255e-06\n",
      "Epoch: 2.97, Learning Rate: 1.0517973165695117e-06\n",
      "Epoch: 2.99, Learning Rate: 1.0230232606440996e-06\n",
      "Epoch: 3.00, Learning Rate: 1.005756149840427e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=213, training_loss=0.7204744058875411, metrics={'train_runtime': 861.3704, 'train_samples_per_second': 3.95, 'train_steps_per_second': 0.247, 'total_flos': 109022294880000.0, 'train_loss': 0.7204744058875411, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa11c264-f9c6-4288-bfc9-54b2c63d9645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f28af7ed4f40e09cde5c97bc1c70a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955f6308757c49a88c79200faf0997af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eee0c226cb4f5aaba5adaf267af73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105753.CapyJetson.1238.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348483bf72244170bf4b2129df55b932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757105415.CapyJetson.1112.0: 100%|##########| 7.03kB / 7.03kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3326067029a4f838c886fdcad50f876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757107076.CapyJetson.1642.0: 100%|##########| 23.1kB / 23.1kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3993258acd314ee59b48e9d130db77d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757080067.CapyJetson.2271.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ed919507ae408fab41c222d6c7dbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757106803.CapyJetson.1522.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc2ac135aff4648a599f837fd49fa3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757106707.CapyJetson.1453.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669130ec8a3b4c5ab630b611cc0a0c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...fevents.1757103285.CapyJetson.816.0: 100%|##########| 6.66kB / 6.66kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93904e334a94ff38770ec8d0f728537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757414946.CapyJetson.2230.0: 100%|##########| 15.3kB / 15.3kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f72272fc4742a5b8f235dc6bae4087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757106889.CapyJetson.1578.0: 100%|##########| 7.41kB / 7.41kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04e49a029b043acbe711cb7cce223cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...events.1757444979.CapyJetson.2917.0: 100%|##########| 23.2kB / 23.2kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AntonBOOM/output/commit/864a0a65aec883f176de9b2fe1dd50c8820a8598', commit_message='YourUsername/gemma-finetuned-math', commit_description='', oid='864a0a65aec883f176de9b2fe1dd50c8820a8598', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AntonBOOM/output', endpoint='https://huggingface.co', repo_type='model', repo_id='AntonBOOM/output'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "hub_model_id = \"YourUsername/gemma-finetuned-math\"\n",
    "trainer.push_to_hub(hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d2011-652a-405f-ba1d-975f181df55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
