{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4bf303-33c4-4fe5-a7e5-2fd7cac9a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capybara/Desktop/matura_project_python/HFvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Model output shape: torch.Size([1, 10, 768])\n",
      "Running on: GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Load a model (will automatically use GPU if available)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Test inference\n",
    "text = \"Hello, this is a test sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to GPU\n",
    "if torch.cuda.is_available():\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Run model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"Model output shape:\", outputs.last_hidden_state.shape)\n",
    "print(\"Running on:\", \"GPU\" if outputs.last_hidden_state.device.type == \"cuda\" else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8e42f-a9ee-416b-85ab-a02c05995ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bitsandbytes GPU Verification Script ---\n",
      "\n",
      "Step 1: Checking for CUDA-enabled GPU...\n",
      "‚úÖ Success: CUDA is available. Found GPU: Orin\n",
      "\n",
      "Step 2: Loading a model with 8-bit quantization (`load_in_8bit=True`)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Error named symbol not found at line 448 in file /src/csrc/ops.cu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def verify_bitsandbytes_gpu():\n",
    "    \"\"\"\n",
    "    This script verifies the functionality of the bitsandbytes library on a GPU.\n",
    "\n",
    "    It performs the following checks:\n",
    "    1. Confirms that PyTorch can detect the CUDA-enabled GPU.\n",
    "    2. Attempts to load a small language model ('facebook/opt-125m') using\n",
    "       8-bit quantization (`load_in_8bit=True`). This is a core feature of\n",
    "       bitsandbytes and will fail if the library is not correctly configured\n",
    "       for your GPU.\n",
    "    3. Prints the memory footprint of the 8-bit quantized model to show\n",
    "       that quantization has occurred.\n",
    "    4. Performs a simple forward pass (inference) to ensure the quantized\n",
    "       model can execute operations on the GPU.\n",
    "    \"\"\"\n",
    "    print(\"--- Bitsandbytes GPU Verification Script ---\")\n",
    "    print(\"\\nStep 1: Checking for CUDA-enabled GPU...\")\n",
    "\n",
    "    # Check 1: Is CUDA available?\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"\\n‚ùå ERROR: PyTorch cannot find a CUDA-enabled GPU.\")\n",
    "        print(\"Please ensure you have installed the correct PyTorch version for your Jetson device.\")\n",
    "        print(\"Verification failed.\")\n",
    "        return\n",
    "\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úÖ Success: CUDA is available. Found GPU: {cuda_device_name}\")\n",
    "\n",
    "    # Check 2: Load a model with 8-bit quantization\n",
    "    print(\"\\nStep 2: Loading a model with 8-bit quantization (`load_in_8bit=True`)...\")\n",
    "    model_name = \"facebook/opt-125m\"\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\" # Automatically places the model on the GPU\n",
    "        )\n",
    "        print(\"‚úÖ Success: Model loaded in 8-bit without errors.\")\n",
    "        print(\"   This indicates that bitsandbytes is correctly installed and communicating with the GPU.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: Failed to load the model in 8-bit.\")\n",
    "        print(\"   This is likely an issue with your bitsandbytes installation or its CUDA compatibility.\")\n",
    "        print(f\"   Error details: {e}\")\n",
    "        print(\"Verification failed.\")\n",
    "        return\n",
    "\n",
    "    # Check 3: Verify model is on GPU and quantized\n",
    "    print(\"\\nStep 3: Verifying model properties...\")\n",
    "    model_device = next(model.parameters()).device\n",
    "    print(f\"   - Model is on device: {model_device}\")\n",
    "\n",
    "    if model_device.type != 'cuda':\n",
    "        print(f\"   ‚ùå WARNING: Model is on {model_device.type}, not 'cuda'. Check the `device_map`.\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Model is correctly placed on the CUDA device.\")\n",
    "\n",
    "    # Print memory footprint\n",
    "    mem_footprint = model.get_memory_footprint()\n",
    "    print(f\"   - Model memory footprint: {mem_footprint / 1e6:.2f} MB\")\n",
    "\n",
    "    # Check 4: Perform a simple forward pass\n",
    "    print(\"\\nStep 4: Performing a simple inference test (forward pass)...\")\n",
    "    try:\n",
    "        # Create a dummy input tensor on the GPU\n",
    "        dummy_input = torch.randint(0, 1000, (1, 10)).to(\"cuda\")\n",
    "        \n",
    "        # Perform a forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(dummy_input)\n",
    "\n",
    "        print(\"‚úÖ Success: Forward pass completed without errors.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: The forward pass failed.\")\n",
    "        print(\"   There might be an issue with the quantized operations on your GPU.\")\n",
    "        print(f\"   Error details: {e}\")\n",
    "        print(\"Verification failed.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Verification Complete ---\")\n",
    "    print(\"üéâ All checks passed! Your `bitsandbytes` installation appears to be working correctly with your GPU.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_bitsandbytes_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cb2eb-92b9-4f6d-8065-859a03be8f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hugging Face venv",
   "language": "python",
   "name": "hfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
