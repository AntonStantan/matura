{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee93da6-b3d6-4fc2-8776-b66eb26a9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfe0cc3-9929-4476-a4cb-3bdd073d6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - 1 + -2\n",
      "2543\n",
      "-1.0\n",
      "\n",
      "Expressions not in x:\n",
      "1 - -2 - -3\n",
      "True\n",
      "1457\n",
      "6.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"gemini2.5.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import expressions_not_in_x, outsideExpr, longer_Exps, y_test, out_y_test, long_y_test, x_string, y\n",
    "\n",
    "x_test = expressions_not_in_x\n",
    "out_x_test = outsideExpr\n",
    "long_x_test = longer_Exps\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00126f4d-1c66-425d-9753-a8199ac9c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./big_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f22ea6-4884-484f-afcd-4f3ce71d19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model ID, as indicated by the error message\n",
    "base_model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Your fine-tuned adapter\n",
    "adapter_id = \"./big_output\" \n",
    "\n",
    "# 1. Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 2. Load the Peft model and merge it with the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_id)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# The 'model' variable now holds your fully fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc3cbad-a454-4ed0-b0ea-7f27cb956739",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"added padding\")\n",
    "\n",
    "def encodePreds(x):\n",
    "    out = []\n",
    "    for i in x:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": i}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        out.append(tokenizer(prompt, return_tensors=\"pt\").to(model.device))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10855081-3089-4176-a959-4acc65230b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = encodePreds(x_test)\n",
    "inputs2 = encodePreds(x_string)\n",
    "inputs3 = encodePreds(out_x_test)\n",
    "inputs4 = encodePreds(long_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f53c7b3-6579-4391-80c2-11b7deff6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodePreds(output):\n",
    "    responses = []\n",
    "    turn_separator = \"<start_of_turn>model\\n\"\n",
    "    decoded_preds = tokenizer.batch_decode(output, skip_special_tokens=False)\n",
    "    for i in decoded_preds:\n",
    "        pred_response = i.split(turn_separator, 1)[1]\n",
    "        clean_pred = pred_response.replace(\"<end_of_turn>\", \"\") \\\n",
    "                                  .replace(\"<pad>\", \"\") \\\n",
    "                                  .replace(\"<start_of_turn>\", \"\") \\\n",
    "                                  .replace(\"model\", \"\") \\\n",
    "                                  .replace(\"\\n\", \"\") \\\n",
    "                                  .strip()\n",
    "    \n",
    "    #print(clean_pred)\n",
    "    responses.append(clean_pred)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a12705-f67a-4790-8ff9-9ec6ef2e1bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses1 = []\n",
    "for input in inputs1:\n",
    "    output = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=20, \n",
    "        do_sample=False,\n",
    "        temperature=None, # Explicitly unset\n",
    "        top_p=None,       # Explicitly unset\n",
    "        top_k=None        # Explicitly unset\n",
    "    )\n",
    "\n",
    "    responses1.append(decodePreds(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4ec280-3152-4431-a675-6539b392b10e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses2 = []\n",
    "for input in inputs2:\n",
    "    output = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=20, \n",
    "        do_sample=False,\n",
    "        temperature=None, # Explicitly unset\n",
    "        top_p=None,       # Explicitly unset\n",
    "        top_k=None        # Explicitly unset\n",
    "    )\n",
    "    responses2.append(decodePreds(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad645167-b7c3-4f01-93ab-23d96a139788",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses3 = []\n",
    "for input in inputs3:\n",
    "    output = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=20, \n",
    "        do_sample=False,\n",
    "        temperature=None, # Explicitly unset\n",
    "        top_p=None,       # Explicitly unset\n",
    "        top_k=None        # Explicitly unset\n",
    "    )\n",
    "    responses3.append(decodePreds(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f93750-422f-45a0-92b6-bd8c64435110",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses4 = []\n",
    "for input in inputs4:\n",
    "    output = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=20, \n",
    "        do_sample=False,\n",
    "        temperature=None, # Explicitly unset\n",
    "        top_p=None,       # Explicitly unset\n",
    "        top_k=None        # Explicitly unset\n",
    "    )\n",
    "    responses4.append(decodePreds(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "170bb0fb-eb66-4e04-8724-b8cf73b16d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2.0'], ['-2.0'], ['4.0'], ['1.0'], ['-11.0']]\n",
      "[['-1.0'], ['-2.0'], ['-10.0'], ['-7.0'], ['-6.0']]\n"
     ]
    }
   ],
   "source": [
    "print(responses1[:5])\n",
    "print(responses2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d6e88d-ee5c-4ce8-ae0c-411df1b2fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0  # Default fallback; adjust as needed\n",
    "\n",
    "def safe_str(x):\n",
    "    try:\n",
    "        return str(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"  # Default fallback; adjust as needed\n",
    "\n",
    "# Process responses1-4\n",
    "float_preds1 = [safe_float(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else 0.0 for i in responses1]\n",
    "string_preds1 = [safe_str(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else \"\" for i in responses1]\n",
    "\n",
    "float_preds2 = [safe_float(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else 0.0 for i in responses2]\n",
    "string_preds2 = [safe_str(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else \"\" for i in responses2]\n",
    "\n",
    "float_preds3 = [safe_float(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else 0.0 for i in responses3]\n",
    "string_preds3 = [safe_str(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else \"\" for i in responses3]\n",
    "\n",
    "float_preds4 = [safe_float(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else 0.0 for i in responses4]\n",
    "string_preds4 = [safe_str(i[0]) if isinstance(i, (list, tuple)) and len(i) > 0 else \"\" for i in responses4]\n",
    "\n",
    "# Process y_test and y\n",
    "y_test_float = [safe_float(i) for i in y_test]\n",
    "y_test_string = [safe_str(i) for i in y_test]\n",
    "\n",
    "y_float = [safe_float(i) for i in y]\n",
    "y_string = [safe_str(i) for i in y]\n",
    "\n",
    "#this is part of the reason why using abs error in pretrained models with token outputs doesn't work.\n",
    "#this is an improvement by ai on my code, to catch outlying tokens. This isn't the best way to do it because of the replacement with 0.0, but this is only going to be the case sometimes, so wont play that big a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2db464-ae3a-4d55-aa78-e4fd5783ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out abs error: 2.19990234375, accuracy: 0.63330078125\n",
      "long abs error: 2.400142857142857, accuracy: 0.39571428571428574\n"
     ]
    }
   ],
   "source": [
    "out_diffs = []\n",
    "long_diffs = []\n",
    "out_acc = 0\n",
    "long_acc = 0\n",
    "for i in range(len(out_y_test)):\n",
    "    diff = float(out_y_test[i]) - float_preds3[i]\n",
    "    if diff == 0:\n",
    "        out_acc += 1\n",
    "    out_diffs.append(abs(diff))\n",
    "out_mean_diff = np.mean(out_diffs)\n",
    "out_acc = out_acc/len(out_y_test)\n",
    "print(f\"out abs error: {out_mean_diff}, accuracy: {out_acc}\")\n",
    "\n",
    "for i in range(len(long_y_test)):\n",
    "    diff = float(long_y_test[i]) - float_preds4[i]\n",
    "    if diff == 0:\n",
    "        long_acc += 1\n",
    "    long_diffs.append(abs(diff))\n",
    "long_mean_diff = np.mean(long_diffs)\n",
    "long_acc = long_acc/len(long_y_test)\n",
    "print(f\"long abs error: {long_mean_diff}, accuracy: {long_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80662193-541b-463e-ab57-5099d131a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2330130404941661\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "for i in range(len(y_test)):\n",
    "    diff = y_test_float[i] - float_preds1[i]\n",
    "    diffs.append(abs(diff))\n",
    "print(np.mean(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4061f42-f424-4b94-bd91-d08b4b973c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2609123082972867\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "for i in range(len(y_float)):\n",
    "    diff = y_float[i] - float_preds2[i]\n",
    "    diffs.append(abs(diff))\n",
    "print(np.mean(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2922bc6-ff0f-4d16-a54e-2c6a549a3359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9341111873713109\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test_string)):\n",
    "    if y_test_string[i] == string_preds1[i]:\n",
    "        count += 1\n",
    "print(count/len(y_test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4108c4e-481b-44dd-88b8-c1fe38377324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939441604404247\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_string)):\n",
    "    if y_string[i] == string_preds2[i]:\n",
    "        count += 1\n",
    "print(count/len(y_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a5b4d-ce03-47d3-8722-c81a867860e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93600c1-6d4f-4d90-a2ec-74e6fc40a109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
