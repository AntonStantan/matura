{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e458af-c009-452d-ba64-b19ab9311aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported libraries!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Layer, Dropout\n",
    "\n",
    "print(\"Successfully imported libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfae1ac-dca8-441d-b1c3-0e4096b967c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow Version: 2.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 22:57:56.165425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:00.892023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:00.892338: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:00.896889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:00.897217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:00.897376: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:01.041409: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:01.041773: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:01.041846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-08-12 22:58:01.042007: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-12 22:58:01.042164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4244 MB memory:  -> device: 0, name: Orin, pci bus id: 0000:00:00.0, compute capability: 8.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test computation done on GPU\n"
     ]
    }
   ],
   "source": [
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\", gpus)\n",
    "\n",
    "# Check if TensorFlow will place operations on the GPU\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Run a quick test\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"Test computation done on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9435ec-aa9e-4f68-97f2-066e660de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 + -4 - -2\n",
      "2543\n",
      "-3.0\n",
      "\n",
      "Expressions not in x:\n",
      "3 + 3 + -1\n",
      "True\n",
      "1457\n",
      "5.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"transformer0.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_train, y_train, x_val, y_val, early_stopping\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe57a9d-1c20-47ec-88c6-51cc140f0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a cls token at the beginning of x_train and x_val\n",
    "pad_value = 15\n",
    "x_train = np.pad(x_train, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "x_val = np.pad(x_val, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c9a70b-4f59-4f7d-b98e-a45c24430049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the positional encoder modelled after the formula in the paper that was cited. (generated by gemini)\n",
    "def posEncoding(max_seq_len, d_model):\n",
    "    # Create a matrix of angles according to the formula\n",
    "    angle_rads = get_angles(np.arange(max_seq_len)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    \n",
    "    # Apply sine to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cosine to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358a7c4d-7c43-47f3-8332-b0f3516f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the point-wise FNN\n",
    "#d_ff = 2048 #(original transformer size)\n",
    "def point_wise_fnn(d_model, d_ff):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(d_ff, activation = \"relu\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\"),\n",
    "        Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c3f8aa-c268-40bc-93e4-8e3f89576ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled dot-product attention\n",
    "class MH_Attention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        #for the split_heads function:\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #for the call function:\n",
    "        #This allows the model to learn the best way to project the input embeddings. (linear projection)\n",
    "        self.wq = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wk = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wv = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "\n",
    "        #it's important to initialize this aswell as the ones above here, so that the model saves the previous weights and is able to learn.\n",
    "        self.finalDense = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def SDP_Attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) #calculate the dotproduct, between the query and a transposed key.\n",
    "        d_k = tf.shape(k)[-1] #read the dimensionality of the key tensor (here d_model/num_heads = depth)\n",
    "        d_k = tf.cast(d_k, tf.float32) #convert to float type\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(d_k) #scale for purposes discussed in their paper.        \n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9) #masking to a big negative number\n",
    "        \n",
    "        softmaxed_qk = tf.nn.softmax(scaled_qk, axis = -1) #apply softmax function (axis = -1) for softmaxing all the different keys. The last entry is the number of keys (not the dimensionality of them, like it was befre.)\n",
    "        output = tf.matmul(softmaxed_qk, v) #multiply the attention-weights with the values corresponding to the keys, in respect to the query.\n",
    "        return output, softmaxed_qk\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) #splits up the x data which is gonna be q, k, or v, into the individual heads. effectively adding a dimension (self.num_heads), after splitting up self.d_model\n",
    "        return tf.transpose(x, perm =[0,2,1,3]) #reorganizes the dimensions into the expected order (batch_size, num_heads, seq_len, depth(the new d_model \"fractions\"))\n",
    "\n",
    "    def call(self, q, k ,v, mask = None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        #(linear projection)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        #split them all up into the individual heads. (add a dimension basically)\n",
    "        q = self.split_heads(q , batch_size)\n",
    "        k = self.split_heads(k , batch_size)\n",
    "        v = self.split_heads(v , batch_size)\n",
    "\n",
    "        sdp_attention, attention_weights = self.SDP_Attention(q,k,v, mask = mask) #applies the sdp-attention to all of them. sdp_attention at the end has a shape of: (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        sdp_attention = tf.transpose(sdp_attention, perm=[0, 2, 1, 3]) #swap the 2nd and 3rd dimensions\n",
    "        combined_attention = tf.reshape(sdp_attention, (batch_size, -1, self.d_model)) #combine back the two last dimnensions (num_heads and depth) into the original d_model\n",
    "\n",
    "        output = self.finalDense(combined_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a9f37d-9fd5-4d54-b70a-b34a6bf99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, rate):\n",
    "        super().__init__()\n",
    "        #define all the components of a Layer so the model will learn them properly here.\n",
    "        self.mha = MH_Attention(d_model, num_heads)\n",
    "        self.fnn = point_wise_fnn(d_model, d_ff)\n",
    "\n",
    "        #initiate the 2 normalizations\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        \n",
    "    def call(self,x, training, mask = None):\n",
    "        mha_out, attention_weights = self.mha(x,x,x,mask = mask) #for self-attention: q,k,v = x\n",
    "        mha_out = self.dropout1(mha_out, training = training) #they apply a small dropout of 0.1 after every residual step in the paper.\n",
    "\n",
    "        norm_out = self.norm1(x + mha_out) #first, add the vectors, then normalize them.\n",
    "\n",
    "        fnn_out = self.fnn(norm_out) #2nd sub-layer with fnn\n",
    "        fnn_out = self.dropout2(fnn_out, training = training) #again apply drop out\n",
    "\n",
    "        norm2_out = self.norm2(norm_out + fnn_out) #again add and norm\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042ae57b-c5b7-44f9-a681-4a929f99252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #amount of encoding layers\n",
    "        self.layers = [EncodingLayer(d_model, num_heads, d_ff, rate) for i in range(num_layers)] #define multiple diffferent encoding layers here.\n",
    "\n",
    "        self.dropout = Dropout(rate)\n",
    "            \n",
    "    def call(self, x, training, mask = None):\n",
    "        x = self.dropout(x, training = training) #we want to drop out before the first layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, training = training, mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dafb55e-1d0e-4338-a9ff-12023f91fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, embedding_layer, d_model, max_seq_len, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.d_model = d_model\n",
    "        self.pos_enc = posEncoding(max_seq_len, d_model)\n",
    "        self.Encoder = Encoder(d_model, num_heads, num_layers, d_ff, rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.finalDense = Dense(1, activation = \"linear\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def call(self, x, training, mask = None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.expand_dims(x, axis=-1) #add a dimension to x\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #scale with âˆšd_model\n",
    "        x += self.pos_enc[:, :seq_len, :]\n",
    "        \n",
    "        out_Encoder = self.Encoder(x, training = training, mask = mask)\n",
    "\n",
    "        output = out_Encoder[:,0,:] #pooling: to the first token.\n",
    "        output = self.dropout(output, training = training) #another dropout\n",
    "\n",
    "        final = self.finalDense(output) #now we can reduce back to a single neuron. This is the opposite of what we did in the embedding layer.\n",
    "\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563b50-5de7-4e5f-a365-7e1daf5e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom learning rate schedule class with warmup and cosine decay\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    A custom learning rate schedule that implements a linear warmup\n",
    "    followed by a cosine decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, peak_lr, warmup_steps, decay_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmupCosineDecay\"):\n",
    "            # Ensure step is a float for calculations\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            \n",
    "            # --- 1. Warmup Phase ---\n",
    "            # Linearly increase the learning rate from 0 to peak_lr\n",
    "            warmup_lr = self.peak_lr * (step / self.warmup_steps)\n",
    "\n",
    "            # --- 2. Cosine Decay Phase ---\n",
    "            # Define the cosine decay schedule\n",
    "            cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.peak_lr,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            # Calculate the learning rate for the decay phase.\n",
    "            # Note: The 'step' for the cosine part must be relative to its start.\n",
    "            decay_lr = cosine_decay_schedule(step - self.warmup_steps)\n",
    "\n",
    "            # --- 3. Choose the correct phase ---\n",
    "            # Use tf.where to select the learning rate based on the current step\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps,\n",
    "                warmup_lr,\n",
    "                decay_lr\n",
    "            )\n",
    "            return learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"peak_lr\": self.peak_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97550c6-783b-4fa8-8f66-cd24fd6c3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=False>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "def build_model(hp):\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [64, 128, 256, 512])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.2 \n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = 5\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse'\n",
    "    )\n",
    "    return transformer_model\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3343ff78-3947-49fa-935f-09dcb9c57d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1895cb-38d5-4c54-b3ff-88a7624757d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 39m 23s]\n",
      "val_loss: 11.010191440582275\n",
      "\n",
      "Best val_loss So Far: 11.010191440582275\n",
      "Total elapsed time: 01h 18m 09s\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "\n",
    "tuner.search(train_dataset, epochs = num_epochs, validation_data = (val_dataset), verbose = 0, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2fbd62-fcb6-4f71-87ec-4608e98ccce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras-tunerResults/results\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 2 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 1e-06\n",
      "Score: 11.010191440582275\n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 32\n",
      "num_layers: 5\n",
      "d_ff: 128\n",
      "dropout: False\n",
      "peak learning rate: 1e-06\n",
      "Score: 16.062340259552002\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 2\n",
      "d_ff: 64\n",
      "dropout: True\n",
      "peak learning rate: 1e-06\n",
      "Score: 25.62373638153076\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e74da-7bcd-49e9-aa43-9c550f8a18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e31fa-53d2-49e0-b5ee-c28036fe4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(\n",
    "    train_dataset, # Pass the TensorFlow Dataset\n",
    "    validation_data=val_dataset, # Pass the TensorFlow Dataset\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Training and Validation MSE per Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ae6b1-f153-468f-8e80-04ee9224040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take one batch from the dataset to inspect it or use it for prediction.\n",
    "first_batch = val_dataset.take(1)\n",
    "\n",
    "# To print the contents of that first batch, you can iterate over it.\n",
    "# (Note: .take(1) creates a new dataset with only one element, so this loop will run once)\n",
    "print(\"Contents of the first batch:\")\n",
    "for batch in first_batch:\n",
    "    # A batch is typically a tuple of (inputs, labels)\n",
    "    inputs, labels = batch\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "print(inputs[0])\n",
    "# 2. Run prediction on that single batch.\n",
    "# The model's predict method can directly accept the dataset object created by .take(1).\n",
    "print(\"\\nRunning prediction on the first batch...\")\n",
    "predictions = best_model.predict(first_batch)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(predictions[0])\n",
    "print(\"--------------\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNN keras env",
   "language": "python",
   "name": "myproject_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
