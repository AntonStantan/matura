{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e458af-c009-452d-ba64-b19ab9311aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported libraries!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Layer, Dropout\n",
    "\n",
    "print(\"Successfully imported libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfae1ac-dca8-441d-b1c3-0e4096b967c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow Version: 2.16.1\n",
      "Test computation done on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 22:32:43.057589: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.104324: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.104513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.108065: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.108282: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.108376: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.190214: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.190532: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.190588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-08-15 22:32:43.190738: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-15 22:32:43.190851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2443 MB memory:  -> device: 0, name: Orin, pci bus id: 0000:00:00.0, compute capability: 8.7\n"
     ]
    }
   ],
   "source": [
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\", gpus)\n",
    "\n",
    "# Check if TensorFlow will place operations on the GPU\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Run a quick test\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"Test computation done on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9435ec-aa9e-4f68-97f2-066e660de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 0 + 4\n",
      "2543\n",
      "8.0\n",
      "\n",
      "Expressions not in x:\n",
      "-2 - -4 - 1\n",
      "True\n",
      "1457\n",
      "1.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"transformer0.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_train, y_train, x_val, y_val, early_stopping\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe57a9d-1c20-47ec-88c6-51cc140f0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a cls token at the beginning of x_train and x_val\n",
    "pad_value = 15\n",
    "x_train = np.pad(x_train, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "x_val = np.pad(x_val, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c9a70b-4f59-4f7d-b98e-a45c24430049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the positional encoder modelled after the formula in the paper that was cited. (generated by gemini)\n",
    "def posEncoding(max_seq_len, d_model):\n",
    "    # Create a matrix of angles according to the formula\n",
    "    angle_rads = get_angles(np.arange(max_seq_len)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    \n",
    "    # Apply sine to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cosine to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358a7c4d-7c43-47f3-8332-b0f3516f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the point-wise FNN\n",
    "#d_ff = 2048 #(original transformer size)\n",
    "def point_wise_fnn(d_model, d_ff):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(d_ff, activation = \"relu\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\"),\n",
    "        Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c3f8aa-c268-40bc-93e4-8e3f89576ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled dot-product attention\n",
    "class MH_Attention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        #for the split_heads function:\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #for the call function:\n",
    "        #This allows the model to learn the best way to project the input embeddings. (linear projection)\n",
    "        self.wq = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wk = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wv = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "\n",
    "        #it's important to initialize this aswell as the ones above here, so that the model saves the previous weights and is able to learn.\n",
    "        self.finalDense = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def SDP_Attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) #calculate the dotproduct, between the query and a transposed key.\n",
    "        d_k = tf.shape(k)[-1] #read the dimensionality of the key tensor (here d_model/num_heads = depth)\n",
    "        d_k = tf.cast(d_k, tf.float32) #convert to float type\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(d_k) #scale for purposes discussed in their paper.        \n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9) #masking to a big negative number\n",
    "        \n",
    "        softmaxed_qk = tf.nn.softmax(scaled_qk, axis = -1) #apply softmax function (axis = -1) for softmaxing all the different keys. The last entry is the number of keys (not the dimensionality of them, like it was befre.)\n",
    "        output = tf.matmul(softmaxed_qk, v) #multiply the attention-weights with the values corresponding to the keys, in respect to the query.\n",
    "        return output, softmaxed_qk\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) #splits up the x data which is gonna be q, k, or v, into the individual heads. effectively adding a dimension (self.num_heads), after splitting up self.d_model\n",
    "        return tf.transpose(x, perm =[0,2,1,3]) #reorganizes the dimensions into the expected order (batch_size, num_heads, seq_len, depth(the new d_model \"fractions\"))\n",
    "\n",
    "    def call(self, q, k ,v, mask = None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        #(linear projection)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        #split them all up into the individual heads. (add a dimension basically)\n",
    "        q = self.split_heads(q , batch_size)\n",
    "        k = self.split_heads(k , batch_size)\n",
    "        v = self.split_heads(v , batch_size)\n",
    "\n",
    "        sdp_attention, attention_weights = self.SDP_Attention(q,k,v, mask = mask) #applies the sdp-attention to all of them. sdp_attention at the end has a shape of: (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        sdp_attention = tf.transpose(sdp_attention, perm=[0, 2, 1, 3]) #swap the 2nd and 3rd dimensions\n",
    "        combined_attention = tf.reshape(sdp_attention, (batch_size, -1, self.d_model)) #combine back the two last dimnensions (num_heads and depth) into the original d_model\n",
    "\n",
    "        output = self.finalDense(combined_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a9f37d-9fd5-4d54-b70a-b34a6bf99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, rate):\n",
    "        super().__init__()\n",
    "        #define all the components of a Layer so the model will learn them properly here.\n",
    "        self.mha = MH_Attention(d_model, num_heads)\n",
    "        self.fnn = point_wise_fnn(d_model, d_ff)\n",
    "\n",
    "        #initiate the 2 normalizations\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        \n",
    "    def call(self,x, training, mask = None):\n",
    "        mha_out, attention_weights = self.mha(x,x,x,mask = mask) #for self-attention: q,k,v = x\n",
    "        mha_out = self.dropout1(mha_out, training = training) #they apply a small dropout of 0.1 after every residual step in the paper.\n",
    "\n",
    "        norm_out = self.norm1(x + mha_out) #first, add the vectors, then normalize them.\n",
    "\n",
    "        fnn_out = self.fnn(norm_out) #2nd sub-layer with fnn\n",
    "        fnn_out = self.dropout2(fnn_out, training = training) #again apply drop out\n",
    "\n",
    "        norm2_out = self.norm2(norm_out + fnn_out) #again add and norm\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042ae57b-c5b7-44f9-a681-4a929f99252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #amount of encoding layers\n",
    "        self.layers = [EncodingLayer(d_model, num_heads, d_ff, rate) for i in range(num_layers)] #define multiple diffferent encoding layers here.\n",
    "\n",
    "        self.dropout = Dropout(rate)\n",
    "            \n",
    "    def call(self, x, training, mask = None):\n",
    "        x = self.dropout(x, training = training) #we want to drop out before the first layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, training = training, mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dafb55e-1d0e-4338-a9ff-12023f91fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, embedding_layer, d_model, max_seq_len, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.d_model = d_model\n",
    "        self.pos_enc = posEncoding(max_seq_len, d_model)\n",
    "        self.Encoder = Encoder(d_model, num_heads, num_layers, d_ff, rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.finalDense = Dense(1, activation = \"linear\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def call(self, x, training, mask = None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.expand_dims(x, axis=-1) #add a dimension to x\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #scale with √d_model\n",
    "        x += self.pos_enc[:, :seq_len, :]\n",
    "        \n",
    "        out_Encoder = self.Encoder(x, training = training, mask = mask)\n",
    "\n",
    "        output = out_Encoder[:,0,:] #pooling: to the first token.\n",
    "        output = self.dropout(output, training = training) #another dropout\n",
    "\n",
    "        final = self.finalDense(output) #now we can reduce back to a single neuron. This is the opposite of what we did in the embedding layer.\n",
    "\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563b50-5de7-4e5f-a365-7e1daf5e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom learning rate schedule class with warmup and cosine decay\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    A custom learning rate schedule that implements a linear warmup\n",
    "    followed by a cosine decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, peak_lr, warmup_steps, decay_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmupCosineDecay\"):\n",
    "            # Ensure step is a float for calculations\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            \n",
    "            # --- 1. Warmup Phase ---\n",
    "            # Linearly increase the learning rate from 0 to peak_lr\n",
    "            warmup_lr = self.peak_lr * (step / self.warmup_steps)\n",
    "\n",
    "            # --- 2. Cosine Decay Phase ---\n",
    "            # Define the cosine decay schedule\n",
    "            cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.peak_lr,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            # Calculate the learning rate for the decay phase.\n",
    "            # Note: The 'step' for the cosine part must be relative to its start.\n",
    "            decay_lr = cosine_decay_schedule(step - self.warmup_steps)\n",
    "\n",
    "            # --- 3. Choose the correct phase ---\n",
    "            # Use tf.where to select the learning rate based on the current step\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps,\n",
    "                warmup_lr,\n",
    "                decay_lr\n",
    "            )\n",
    "            return learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"peak_lr\": self.peak_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97550c6-783b-4fa8-8f66-cd24fd6c3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=False>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "from tensorflow.keras import backend as K\n",
    "def build_model(hp):\n",
    "    K.clear_session()\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [64, 128, 256, 512])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.2 \n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = 25\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = 3\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse'\n",
    "    )\n",
    "    return transformer_model\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3343ff78-3947-49fa-935f-09dcb9c57d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ./untitled_project/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1895cb-38d5-4c54-b3ff-88a7624757d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "\n",
    "tuner.search(train_dataset, epochs = num_epochs, validation_data = (val_dataset), verbose = 0, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2fbd62-fcb6-4f71-87ec-4608e98ccce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 23 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 32\n",
      "num_layers: 3\n",
      "d_ff: 128\n",
      "dropout: False\n",
      "peak learning rate: 0.0014188984633416947\n",
      "Score: 0.13932304084300995\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.00023727638415800082\n",
      "Score: 0.5695855617523193\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.00017753963838400855\n",
      "Score: 1.641616940498352\n",
      "\n",
      "Trial 42 summary\n",
      "Hyperparameters:\n",
      "num_heads: 4\n",
      "d_model: 128\n",
      "num_layers: 3\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0004034381760009418\n",
      "Score: 6.7436699867248535\n",
      "\n",
      "Trial 20 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0002221139550977517\n",
      "Score: 7.766565322875977\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 32\n",
      "num_layers: 2\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.0018620036258997228\n",
      "Score: 10.484166145324707\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 5\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 6.836644645712527e-05\n",
      "Score: 10.627899169921875\n",
      "\n",
      "Trial 18 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0006572830203098331\n",
      "Score: 11.503336906433105\n",
      "\n",
      "Trial 38 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 7.544146171497126e-06\n",
      "Score: 15.012101173400879\n",
      "\n",
      "Trial 47 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 32\n",
      "num_layers: 3\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0017666057033955919\n",
      "Score: 15.705245018005371\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041e74da-7bcd-49e9-aa43-9c550f8a18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b6745e-8bcf-4610-90d8-5c7cacb0a2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_best_model(hp, num_epochs):\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [64, 128, 256, 512])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.2 \n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = num_epochs\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = np.floor(num_epochs/10) + 1\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse'\n",
    "    )\n",
    "    return transformer_model\n",
    "num_epochs_best_model = 200\n",
    "best_model = build_best_model(best_hps, num_epochs_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3e31fa-53d2-49e0-b5ee-c28036fe4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755289984.195224   19637 service.cc:145] XLA service 0xfffe6c002cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1755289984.195314   19637 service.cc:153]   StreamExecutor device (0): Orin, Compute Capability 8.7\n",
      "2025-08-15 22:33:04.954647: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-15 22:33:07.941520: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/60\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 27.9247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755289998.891784   19637 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 338ms/step - loss: 25.9114 - val_loss: 24.9496\n",
      "Epoch 2/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 23.6377 - val_loss: 22.2951\n",
      "Epoch 3/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 21.1356 - val_loss: 19.1666\n",
      "Epoch 4/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 18.2383 - val_loss: 16.2498\n",
      "Epoch 5/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 16.4174 - val_loss: 15.5344\n",
      "Epoch 6/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 15.3729 - val_loss: 14.2005\n",
      "Epoch 7/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 16.2148 - val_loss: 13.7088\n",
      "Epoch 8/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 14.5101 - val_loss: 11.8499\n",
      "Epoch 9/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 13.2370 - val_loss: 11.0068\n",
      "Epoch 10/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 13.3209 - val_loss: 12.3039\n",
      "Epoch 11/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 11.8602 - val_loss: 7.8798\n",
      "Epoch 12/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 10.1903 - val_loss: 6.5684\n",
      "Epoch 13/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 5.7919 - val_loss: 5.8970\n",
      "Epoch 14/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.2371 - val_loss: 4.9304\n",
      "Epoch 15/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3.2689 - val_loss: 3.1038\n",
      "Epoch 16/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.3452 - val_loss: 4.9484\n",
      "Epoch 17/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.9900 - val_loss: 2.1832\n",
      "Epoch 18/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3.6948 - val_loss: 11.4281\n",
      "Epoch 19/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3.7780 - val_loss: 3.9994\n",
      "Epoch 20/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3.6684 - val_loss: 1.9113\n",
      "Epoch 21/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.6894 - val_loss: 16.2483\n",
      "Epoch 22/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3.4473 - val_loss: 1.1704\n",
      "Epoch 23/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.9480 - val_loss: 0.9953\n",
      "Epoch 24/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2.2590 - val_loss: 1.5546\n",
      "Epoch 25/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.2030 - val_loss: 3.9784\n",
      "Epoch 26/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2.4888 - val_loss: 1.0254\n",
      "Epoch 27/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.9807 - val_loss: 1.1614\n",
      "Epoch 28/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.1209 - val_loss: 1.1609\n",
      "Epoch 29/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.4553 - val_loss: 0.7321\n",
      "Epoch 30/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.1699 - val_loss: 1.0390\n",
      "Epoch 31/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.5715 - val_loss: 0.6251\n",
      "Epoch 32/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.5971 - val_loss: 0.3256\n",
      "Epoch 33/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.6096 - val_loss: 0.7048\n",
      "Epoch 34/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4875 - val_loss: 0.2816\n",
      "Epoch 35/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.3005 - val_loss: 0.3399\n",
      "Epoch 36/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.5457 - val_loss: 1.1451\n",
      "Epoch 37/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.6284 - val_loss: 0.2978\n",
      "Epoch 38/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1901 - val_loss: 0.3798\n",
      "Epoch 39/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2275 - val_loss: 0.1957\n",
      "Epoch 40/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2206 - val_loss: 0.2315\n",
      "Epoch 41/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1735 - val_loss: 0.1479\n",
      "Epoch 42/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2557 - val_loss: 0.4316\n",
      "Epoch 43/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.6613 - val_loss: 0.6616\n",
      "Epoch 44/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.5727 - val_loss: 0.1482\n",
      "Epoch 45/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1833 - val_loss: 0.1830\n",
      "Epoch 46/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1569 - val_loss: 0.1741\n",
      "Epoch 47/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2580 - val_loss: 0.1333\n",
      "Epoch 48/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1558 - val_loss: 0.1099\n",
      "Epoch 49/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1551 - val_loss: 0.1655\n",
      "Epoch 50/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3176 - val_loss: 0.1300\n",
      "Epoch 51/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1030 - val_loss: 0.0729\n",
      "Epoch 52/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1126 - val_loss: 0.3523\n",
      "Epoch 53/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1368 - val_loss: 0.0839\n",
      "Epoch 54/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0852 - val_loss: 0.1126\n",
      "Epoch 55/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1387 - val_loss: 0.3938\n",
      "Epoch 56/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1542 - val_loss: 0.1089\n",
      "Epoch 57/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1518 - val_loss: 0.1452\n",
      "Epoch 58/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1542 - val_loss: 0.1024\n",
      "Epoch 59/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1067 - val_loss: 0.0904\n",
      "Epoch 60/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1933 - val_loss: 0.0843\n",
      "Epoch 61/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0887 - val_loss: 0.0821\n",
      "Epoch 62/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0451 - val_loss: 0.0315\n",
      "Epoch 63/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0458 - val_loss: 0.0820\n",
      "Epoch 64/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0742 - val_loss: 0.0521\n",
      "Epoch 65/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.7877 - val_loss: 0.1980\n",
      "Epoch 66/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1565 - val_loss: 0.0925\n",
      "Epoch 67/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1321 - val_loss: 0.0893\n",
      "Epoch 68/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0618 - val_loss: 0.0442\n",
      "Epoch 69/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0306 - val_loss: 0.0510\n",
      "Epoch 70/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0454 - val_loss: 0.0391\n",
      "Epoch 71/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0322 - val_loss: 0.3395\n",
      "Epoch 72/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0771 - val_loss: 0.0510\n",
      "Epoch 73/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0391 - val_loss: 0.0310\n",
      "Epoch 74/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0378 - val_loss: 0.0254\n",
      "Epoch 75/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0747 - val_loss: 0.0587\n",
      "Epoch 76/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0428 - val_loss: 0.0725\n",
      "Epoch 77/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0280 - val_loss: 0.0298\n",
      "Epoch 78/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0274 - val_loss: 0.0285\n",
      "Epoch 79/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0244 - val_loss: 0.0129\n",
      "Epoch 80/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0269 - val_loss: 0.0231\n",
      "Epoch 81/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0393 - val_loss: 0.0455\n",
      "Epoch 82/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0419 - val_loss: 0.0266\n",
      "Epoch 83/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0248 - val_loss: 0.0441\n",
      "Epoch 84/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0346 - val_loss: 0.0486\n",
      "Epoch 85/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0278 - val_loss: 0.0904\n",
      "Epoch 86/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0455 - val_loss: 0.0475\n",
      "Epoch 87/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0250 - val_loss: 0.0446\n",
      "Epoch 88/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0300 - val_loss: 0.0226\n",
      "Epoch 89/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0208 - val_loss: 0.0277\n",
      "Epoch 90/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0133 - val_loss: 0.0192\n",
      "Epoch 91/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0188 - val_loss: 0.0254\n",
      "Epoch 92/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0244 - val_loss: 0.0209\n",
      "Epoch 93/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0170 - val_loss: 0.0158\n",
      "Epoch 94/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0133 - val_loss: 0.0105\n",
      "Epoch 95/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0093 - val_loss: 0.0113\n",
      "Epoch 96/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0101 - val_loss: 0.0271\n",
      "Epoch 97/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 98/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0140 - val_loss: 0.0183\n",
      "Epoch 99/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0196 - val_loss: 0.0251\n",
      "Epoch 100/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0220 - val_loss: 0.0231\n",
      "Epoch 101/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 102/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0142 - val_loss: 0.0145\n",
      "Epoch 103/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0090 - val_loss: 0.0174\n",
      "Epoch 104/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0078 - val_loss: 0.0097\n",
      "Epoch 105/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0078 - val_loss: 0.0123\n",
      "Epoch 106/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 107/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0155 - val_loss: 0.0095\n",
      "Epoch 108/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0081 - val_loss: 0.0105\n",
      "Epoch 109/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0057 - val_loss: 0.0123\n",
      "Epoch 110/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0093\n",
      "Epoch 111/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 112/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0073 - val_loss: 0.0142\n",
      "Epoch 113/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 114/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0143\n",
      "Epoch 115/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0159 - val_loss: 0.0179\n",
      "Epoch 116/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0111 - val_loss: 0.0082\n",
      "Epoch 117/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 118/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0055\n",
      "Epoch 119/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0059 - val_loss: 0.0084\n",
      "Epoch 120/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0071 - val_loss: 0.0087\n",
      "Epoch 121/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0072 - val_loss: 0.0081\n",
      "Epoch 122/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0058 - val_loss: 0.0062\n",
      "Epoch 123/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0105 - val_loss: 0.0164\n",
      "Epoch 124/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0139 - val_loss: 0.0316\n",
      "Epoch 125/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0115 - val_loss: 0.0595\n",
      "Epoch 126/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0063\n",
      "Epoch 127/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0092\n",
      "Epoch 128/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0062 - val_loss: 0.0154\n",
      "Epoch 129/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0074 - val_loss: 0.0053\n",
      "Epoch 130/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0070\n",
      "Epoch 131/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0051\n",
      "Epoch 132/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0041\n",
      "Epoch 133/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 134/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0051\n",
      "Epoch 135/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0046\n",
      "Epoch 136/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0056\n",
      "Epoch 137/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0033\n",
      "Epoch 138/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0039\n",
      "Epoch 139/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0037\n",
      "Epoch 140/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0071\n",
      "Epoch 141/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0062\n",
      "Epoch 142/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0072\n",
      "Epoch 143/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0048\n",
      "Epoch 144/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0036\n",
      "Epoch 145/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0050\n",
      "Epoch 146/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0042\n",
      "Epoch 147/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0040\n",
      "Epoch 148/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0039\n",
      "Epoch 149/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0042\n",
      "Epoch 150/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 151/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0060\n",
      "Epoch 152/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0032\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = best_model.fit(\n",
    "    train_dataset, # Pass the TensorFlow Dataset\n",
    "    validation_data=val_dataset, # Pass the TensorFlow Dataset\n",
    "    epochs=num_epochs_best_model,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Training and Validation MSE per Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdc42eb-b271-4160-aea5-3dd9ef377a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │        \u001b[38;5;34m38,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,628</span> (447.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m114,628\u001b[0m (447.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,209</span> (149.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,209\u001b[0m (149.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,419</span> (298.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m76,419\u001b[0m (298.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa9ae6b1-f153-468f-8e80-04ee9224040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the first batch:\n",
      "Inputs shape: (32, 16)\n",
      "Labels shape: (32,)\n",
      "tf.Tensor(\n",
      "[15.   4.   0.  -5.   0.  -5.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5  0.5], shape=(16,), dtype=float32)\n",
      "\n",
      "Running prediction on the first batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 22:36:01.223455: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predictions shape: (32, 1)\n",
      "[13.288919]\n",
      "--------------\n",
      "[[ 1.3288919e+01]\n",
      " [-1.0249513e+00]\n",
      " [-2.9264622e+00]\n",
      " [-6.0107427e+00]\n",
      " [-9.8061329e-01]\n",
      " [-1.9711741e+00]\n",
      " [ 2.0516894e+00]\n",
      " [ 7.8633094e+00]\n",
      " [-9.5711529e-01]\n",
      " [ 1.0088371e+00]\n",
      " [-4.9608655e+00]\n",
      " [ 8.9442253e+00]\n",
      " [-4.9349351e+00]\n",
      " [ 1.0160669e+00]\n",
      " [-2.4901513e-02]\n",
      " [ 9.9882132e-01]\n",
      " [-2.9563727e+00]\n",
      " [ 3.0083988e+00]\n",
      " [-3.8919444e+00]\n",
      " [-9.0002508e+00]\n",
      " [-3.9580483e+00]\n",
      " [ 2.9846687e+00]\n",
      " [ 1.9960011e+00]\n",
      " [-1.1012344e+01]\n",
      " [-2.0112092e+00]\n",
      " [ 9.0181761e+00]\n",
      " [-4.9540725e+00]\n",
      " [-7.6944865e-03]\n",
      " [-2.9935086e+00]\n",
      " [-4.9706135e+00]\n",
      " [ 3.0246150e+00]\n",
      " [-6.9407458e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Take one batch from the dataset to inspect it or use it for prediction.\n",
    "first_batch = val_dataset.take(1)\n",
    "\n",
    "# To print the contents of that first batch, you can iterate over it.\n",
    "# (Note: .take(1) creates a new dataset with only one element, so this loop will run once)\n",
    "print(\"Contents of the first batch:\")\n",
    "for batch in first_batch:\n",
    "    # A batch is typically a tuple of (inputs, labels)\n",
    "    inputs, labels = batch\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "print(inputs[0])\n",
    "# 2. Run prediction on that single batch.\n",
    "# The model's predict method can directly accept the dataset object created by .take(1).\n",
    "print(\"\\nRunning prediction on the first batch...\")\n",
    "predictions = best_model.predict(first_batch)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(predictions[0])\n",
    "print(\"--------------\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632d837-05ff-46a3-bb6e-3f4e8e7ffdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbd56c-6d64-41cf-8bef-13a06fb0737e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old versions and stuff (TF GPU)",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
