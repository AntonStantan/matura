{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e458af-c009-452d-ba64-b19ab9311aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported libraries!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Layer, Dropout\n",
    "\n",
    "print(\"Successfully imported libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfae1ac-dca8-441d-b1c3-0e4096b967c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: []\n",
      "TensorFlow Version: 2.20.0-rc0\n",
      "Test computation done on GPU\n"
     ]
    }
   ],
   "source": [
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\", gpus)\n",
    "\n",
    "# Check if TensorFlow will place operations on the GPU\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Run a quick test\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"Test computation done on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9435ec-aa9e-4f68-97f2-066e660de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5 - 0 + 0\n",
      "2543\n",
      "-5.0\n",
      "\n",
      "Expressions not in x:\n",
      "3 - 2 + 4\n",
      "True\n",
      "1457\n",
      "5.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"transformer0.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_train, y_train, x_val, y_val, early_stopping\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe57a9d-1c20-47ec-88c6-51cc140f0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a cls token at the beginning of x_train and x_val\n",
    "pad_value = 15\n",
    "x_train = np.pad(x_train, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "x_val = np.pad(x_val, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c9a70b-4f59-4f7d-b98e-a45c24430049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the positional encoder modelled after the formula in the paper that was cited. (generated by gemini)\n",
    "def posEncoding(max_seq_len, d_model):\n",
    "    # Create a matrix of angles according to the formula\n",
    "    angle_rads = get_angles(np.arange(max_seq_len)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    \n",
    "    # Apply sine to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cosine to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358a7c4d-7c43-47f3-8332-b0f3516f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the point-wise FNN\n",
    "#d_ff = 2048 #(original transformer size)\n",
    "def point_wise_fnn(d_model, d_ff):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(d_ff, activation = \"relu\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\"),\n",
    "        Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c3f8aa-c268-40bc-93e4-8e3f89576ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled dot-product attention\n",
    "class MH_Attention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        #for the split_heads function:\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #for the call function:\n",
    "        #This allows the model to learn the best way to project the input embeddings. (linear projection)\n",
    "        self.wq = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wk = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wv = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "\n",
    "        #it's important to initialize this aswell as the ones above here, so that the model saves the previous weights and is able to learn.\n",
    "        self.finalDense = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def SDP_Attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) #calculate the dotproduct, between the query and a transposed key.\n",
    "        d_k = tf.shape(k)[-1] #read the dimensionality of the key tensor (here d_model/num_heads = depth)\n",
    "        d_k = tf.cast(d_k, tf.float32) #convert to float type\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(d_k) #scale for purposes discussed in their paper.        \n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9) #masking to a big negative number\n",
    "        \n",
    "        softmaxed_qk = tf.nn.softmax(scaled_qk, axis = -1) #apply softmax function (axis = -1) for softmaxing all the different keys. The last entry is the number of keys (not the dimensionality of them, like it was befre.)\n",
    "        output = tf.matmul(softmaxed_qk, v) #multiply the attention-weights with the values corresponding to the keys, in respect to the query.\n",
    "        return output, softmaxed_qk\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) #splits up the x data which is gonna be q, k, or v, into the individual heads. effectively adding a dimension (self.num_heads), after splitting up self.d_model\n",
    "        return tf.transpose(x, perm =[0,2,1,3]) #reorganizes the dimensions into the expected order (batch_size, num_heads, seq_len, depth(the new d_model \"fractions\"))\n",
    "\n",
    "    def call(self, q, k ,v, mask = None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        #(linear projection)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        #split them all up into the individual heads. (add a dimension basically)\n",
    "        q = self.split_heads(q , batch_size)\n",
    "        k = self.split_heads(k , batch_size)\n",
    "        v = self.split_heads(v , batch_size)\n",
    "\n",
    "        sdp_attention, attention_weights = self.SDP_Attention(q,k,v, mask = mask) #applies the sdp-attention to all of them. sdp_attention at the end has a shape of: (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        sdp_attention = tf.transpose(sdp_attention, perm=[0, 2, 1, 3]) #swap the 2nd and 3rd dimensions\n",
    "        combined_attention = tf.reshape(sdp_attention, (batch_size, -1, self.d_model)) #combine back the two last dimnensions (num_heads and depth) into the original d_model\n",
    "\n",
    "        output = self.finalDense(combined_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a9f37d-9fd5-4d54-b70a-b34a6bf99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, rate):\n",
    "        super().__init__()\n",
    "        #define all the components of a Layer so the model will learn them properly here.\n",
    "        self.mha = MH_Attention(d_model, num_heads)\n",
    "        self.fnn = point_wise_fnn(d_model, d_ff)\n",
    "\n",
    "        #initiate the 2 normalizations\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        \n",
    "    def call(self,x, training, mask = None):\n",
    "        mha_out, attention_weights = self.mha(x,x,x,mask = mask) #for self-attention: q,k,v = x\n",
    "        mha_out = self.dropout1(mha_out, training = training) #they apply a small dropout of 0.1 after every residual step in the paper.\n",
    "\n",
    "        norm_out = self.norm1(x + mha_out) #first, add the vectors, then normalize them.\n",
    "\n",
    "        fnn_out = self.fnn(norm_out) #2nd sub-layer with fnn\n",
    "        fnn_out = self.dropout2(fnn_out, training = training) #again apply drop out\n",
    "\n",
    "        norm2_out = self.norm2(norm_out + fnn_out) #again add and norm\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042ae57b-c5b7-44f9-a681-4a929f99252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #amount of encoding layers\n",
    "        self.layers = [EncodingLayer(d_model, num_heads, d_ff, rate) for i in range(num_layers)] #define multiple diffferent encoding layers here.\n",
    "\n",
    "        self.dropout = Dropout(rate)\n",
    "            \n",
    "    def call(self, x, training, mask = None):\n",
    "        x = self.dropout(x, training = training) #we want to drop out before the first layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, training = training, mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dafb55e-1d0e-4338-a9ff-12023f91fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, embedding_layer, d_model, max_seq_len, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.d_model = d_model\n",
    "        self.pos_enc = posEncoding(max_seq_len, d_model)\n",
    "        self.Encoder = Encoder(d_model, num_heads, num_layers, d_ff, rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.finalDense = Dense(1, activation = \"linear\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def call(self, x, training, mask = None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.expand_dims(x, axis=-1) #add a dimension to x\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #scale with √d_model\n",
    "        x += self.pos_enc[:, :seq_len, :]\n",
    "        \n",
    "        out_Encoder = self.Encoder(x, training = training, mask = mask)\n",
    "\n",
    "        output = out_Encoder[:,0,:] #pooling: to the first token.\n",
    "        output = self.dropout(output, training = training) #another dropout\n",
    "\n",
    "        final = self.finalDense(output) #now we can reduce back to a single neuron. This is the opposite of what we did in the embedding layer.\n",
    "\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563b50-5de7-4e5f-a365-7e1daf5e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom learning rate schedule class with warmup and cosine decay\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    A custom learning rate schedule that implements a linear warmup\n",
    "    followed by a cosine decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, peak_lr, warmup_steps, decay_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmupCosineDecay\"):\n",
    "            # Ensure step is a float for calculations\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            \n",
    "            # --- 1. Warmup Phase ---\n",
    "            # Linearly increase the learning rate from 0 to peak_lr\n",
    "            warmup_lr = self.peak_lr * (step / self.warmup_steps)\n",
    "\n",
    "            # --- 2. Cosine Decay Phase ---\n",
    "            # Define the cosine decay schedule\n",
    "            cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.peak_lr,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            # Calculate the learning rate for the decay phase.\n",
    "            # Note: The 'step' for the cosine part must be relative to its start.\n",
    "            decay_lr = cosine_decay_schedule(step - self.warmup_steps)\n",
    "\n",
    "            # --- 3. Choose the correct phase ---\n",
    "            # Use tf.where to select the learning rate based on the current step\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps,\n",
    "                warmup_lr,\n",
    "                decay_lr\n",
    "            )\n",
    "            return learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"peak_lr\": self.peak_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97550c6-783b-4fa8-8f66-cd24fd6c3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=False>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "def build_model(hp):\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [64, 128, 256, 512])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.2 \n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = 25\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = 3\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse'\n",
    "    )\n",
    "    return transformer_model\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3343ff78-3947-49fa-935f-09dcb9c57d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1895cb-38d5-4c54-b3ff-88a7624757d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\A_118784\\Desktop\\matura_github\\myenv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "\n",
    "tuner.search(train_dataset, epochs = num_epochs, validation_data = (val_dataset), verbose = 0, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2fbd62-fcb6-4f71-87ec-4608e98ccce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 23 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 32\n",
      "num_layers: 3\n",
      "d_ff: 128\n",
      "dropout: False\n",
      "peak learning rate: 0.0014188984633416947\n",
      "Score: 0.13932304084300995\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.00023727638415800082\n",
      "Score: 0.5695855617523193\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.00017753963838400855\n",
      "Score: 1.641616940498352\n",
      "\n",
      "Trial 42 summary\n",
      "Hyperparameters:\n",
      "num_heads: 4\n",
      "d_model: 128\n",
      "num_layers: 3\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0004034381760009418\n",
      "Score: 6.7436699867248535\n",
      "\n",
      "Trial 20 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0002221139550977517\n",
      "Score: 7.766565322875977\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 32\n",
      "num_layers: 2\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.0018620036258997228\n",
      "Score: 10.484166145324707\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 5\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 6.836644645712527e-05\n",
      "Score: 10.627899169921875\n",
      "\n",
      "Trial 18 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 64\n",
      "num_layers: 4\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0006572830203098331\n",
      "Score: 11.503336906433105\n",
      "\n",
      "Trial 38 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 7.544146171497126e-06\n",
      "Score: 15.012101173400879\n",
      "\n",
      "Trial 47 summary\n",
      "Hyperparameters:\n",
      "num_heads: 2\n",
      "d_model: 32\n",
      "num_layers: 3\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0017666057033955919\n",
      "Score: 15.705245018005371\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041e74da-7bcd-49e9-aa43-9c550f8a18fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with best hyperparameters\n",
    "best_model = build_model(best_hps)\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3e31fa-53d2-49e0-b5ee-c28036fe4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 56ms/step - loss: 23.7313 - val_loss: 20.4483\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 19.6743 - val_loss: 19.0151\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 18.9644 - val_loss: 13.0276\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 9.8153 - val_loss: 9.9391\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 8.2833 - val_loss: 6.9374\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 3.8404 - val_loss: 5.6706\n",
      "Epoch 7/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 2.8058 - val_loss: 13.5605\n",
      "Epoch 8/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 3.3477 - val_loss: 0.7473\n",
      "Epoch 9/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 1.9873 - val_loss: 0.6802\n",
      "Epoch 10/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.7164 - val_loss: 0.3653\n",
      "Epoch 11/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.4366 - val_loss: 0.2714\n",
      "Epoch 12/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.3462 - val_loss: 0.3418\n",
      "Epoch 13/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.4634 - val_loss: 0.4911\n",
      "Epoch 14/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.2861 - val_loss: 0.1996\n",
      "Epoch 15/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.1432 - val_loss: 0.1017\n",
      "Epoch 16/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0933 - val_loss: 0.0840\n",
      "Epoch 17/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0882 - val_loss: 0.1495\n",
      "Epoch 18/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0817 - val_loss: 0.0676\n",
      "Epoch 19/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0450 - val_loss: 0.0430\n",
      "Epoch 20/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0370 - val_loss: 0.0429\n",
      "Epoch 21/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0264 - val_loss: 0.0261\n",
      "Epoch 22/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0252 - val_loss: 0.0278\n",
      "Epoch 23/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0199 - val_loss: 0.0198\n",
      "Epoch 24/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 0.0184 - val_loss: 0.0222\n",
      "Epoch 25/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0169 - val_loss: 0.0185\n",
      "Epoch 26/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0163 - val_loss: 0.0179\n",
      "Epoch 27/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0167 - val_loss: 0.0174\n",
      "Epoch 28/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0143 - val_loss: 0.0186\n",
      "Epoch 29/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0147 - val_loss: 0.0156\n",
      "Epoch 30/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0145 - val_loss: 0.0166\n",
      "Epoch 31/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0139 - val_loss: 0.0171\n",
      "Epoch 32/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - loss: 0.0123 - val_loss: 0.0125\n",
      "Epoch 33/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0144 - val_loss: 0.0157\n",
      "Epoch 34/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0119 - val_loss: 0.0133\n",
      "Epoch 35/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0122 - val_loss: 0.0138\n",
      "Epoch 36/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 37/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0120 - val_loss: 0.0159\n",
      "Epoch 38/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 39/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 40/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 41/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - loss: 0.0097 - val_loss: 0.0122\n",
      "Epoch 42/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 43/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 44/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 45/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 46/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 0.0093 - val_loss: 0.0087\n",
      "Epoch 47/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 48/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 0.0090 - val_loss: 0.0126\n",
      "Epoch 49/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0106 - val_loss: 0.0079\n",
      "Epoch 50/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.0104 - val_loss: 0.0100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = best_model.fit(\n",
    "    train_dataset, # Pass the TensorFlow Dataset\n",
    "    validation_data=val_dataset, # Pass the TensorFlow Dataset\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Training and Validation MSE per Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9ae6b1-f153-468f-8e80-04ee9224040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the first batch:\n",
      "Inputs shape: (32, 16)\n",
      "Labels shape: (32,)\n",
      "tf.Tensor(\n",
      "[15.  -5.   0.   0.   1.  -1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5  0.5], shape=(16,), dtype=float32)\n",
      "\n",
      "Running prediction on the first batch...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step\n",
      "Predictions shape: (32, 1)\n",
      "[-6.06027]\n",
      "--------------\n",
      "[[-6.0602698e+00]\n",
      " [-1.9249308e+00]\n",
      " [ 7.9277949e+00]\n",
      " [-3.9551051e+00]\n",
      " [-9.9639883e+00]\n",
      " [ 1.8710394e+00]\n",
      " [ 7.0357237e+00]\n",
      " [-8.9382334e+00]\n",
      " [-8.0277910e+00]\n",
      " [ 6.9623327e+00]\n",
      " [ 4.0522084e+00]\n",
      " [-6.9374614e+00]\n",
      " [ 6.0358171e+00]\n",
      " [-4.0214577e+00]\n",
      " [-2.9439240e+00]\n",
      " [-5.9228024e+00]\n",
      " [-2.1150894e+00]\n",
      " [-3.0126839e+00]\n",
      " [ 3.0080953e+00]\n",
      " [ 2.9002728e+00]\n",
      " [-1.2029700e+01]\n",
      " [ 8.8589258e+00]\n",
      " [-5.9931660e+00]\n",
      " [-3.9897926e+00]\n",
      " [-4.0853415e+00]\n",
      " [ 7.1195886e-04]\n",
      " [ 2.0442543e+00]\n",
      " [-8.8900537e+00]\n",
      " [ 3.9514408e+00]\n",
      " [ 8.0285616e+00]\n",
      " [ 6.0691133e+00]\n",
      " [ 8.0487432e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Take one batch from the dataset to inspect it or use it for prediction.\n",
    "first_batch = val_dataset.take(1)\n",
    "\n",
    "# To print the contents of that first batch, you can iterate over it.\n",
    "# (Note: .take(1) creates a new dataset with only one element, so this loop will run once)\n",
    "print(\"Contents of the first batch:\")\n",
    "for batch in first_batch:\n",
    "    # A batch is typically a tuple of (inputs, labels)\n",
    "    inputs, labels = batch\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "print(inputs[0])\n",
    "# 2. Run prediction on that single batch.\n",
    "# The model's predict method can directly accept the dataset object created by .take(1).\n",
    "print(\"\\nRunning prediction on the first batch...\")\n",
    "predictions = best_model.predict(first_batch)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(predictions[0])\n",
    "print(\"--------------\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632d837-05ff-46a3-bb6e-3f4e8e7ffdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNN keras env",
   "language": "python",
   "name": "myproject_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
