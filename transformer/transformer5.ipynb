{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e458af-c009-452d-ba64-b19ab9311aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported libraries!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Layer, Dropout\n",
    "\n",
    "print(\"Successfully imported libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfae1ac-dca8-441d-b1c3-0e4096b967c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow Version: 2.16.1\n",
      "Test computation done on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 13:34:08.228505: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.276293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.276472: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.280344: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.280541: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.280635: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.387498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.387863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.387917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-08-17 13:34:08.388090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-17 13:34:08.388189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3558 MB memory:  -> device: 0, name: Orin, pci bus id: 0000:00:00.0, compute capability: 8.7\n"
     ]
    }
   ],
   "source": [
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\", gpus)\n",
    "\n",
    "# Check if TensorFlow will place operations on the GPU\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Run a quick test\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"Test computation done on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9435ec-aa9e-4f68-97f2-066e660de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - -4 + -2\n",
      "2543\n",
      "2.0\n",
      "\n",
      "Expressions not in x:\n",
      "-5 - 2 + -5\n",
      "True\n",
      "1457\n",
      "-12.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"transformer0.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_train, y_train, x_val, y_val, early_stopping\n",
    "\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe57a9d-1c20-47ec-88c6-51cc140f0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a cls token at the beginning of x_train and x_val\n",
    "pad_value = 15\n",
    "x_train = np.pad(x_train, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "x_val = np.pad(x_val, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c9a70b-4f59-4f7d-b98e-a45c24430049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the positional encoder modelled after the formula in the paper that was cited. (generated by gemini)\n",
    "def posEncoding(max_seq_len, d_model):\n",
    "    # Create a matrix of angles according to the formula\n",
    "    angle_rads = get_angles(np.arange(max_seq_len)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    \n",
    "    # Apply sine to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cosine to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358a7c4d-7c43-47f3-8332-b0f3516f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the point-wise FNN\n",
    "#d_ff = 2048 #(original transformer size)\n",
    "def point_wise_fnn(d_model, d_ff):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(d_ff, activation = \"relu\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\"),\n",
    "        Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c3f8aa-c268-40bc-93e4-8e3f89576ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled dot-product attention\n",
    "class MH_Attention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        #for the split_heads function:\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #for the call function:\n",
    "        #This allows the model to learn the best way to project the input embeddings. (linear projection)\n",
    "        self.wq = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wk = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wv = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "\n",
    "        #it's important to initialize this aswell as the ones above here, so that the model saves the previous weights and is able to learn.\n",
    "        self.finalDense = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def SDP_Attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) #calculate the dotproduct, between the query and a transposed key.\n",
    "        d_k = tf.shape(k)[-1] #read the dimensionality of the key tensor (here d_model/num_heads = depth)\n",
    "        d_k = tf.cast(d_k, tf.float32) #convert to float type\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(d_k) #scale for purposes discussed in their paper.        \n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9) #masking to a big negative number\n",
    "        \n",
    "        softmaxed_qk = tf.nn.softmax(scaled_qk, axis = -1) #apply softmax function (axis = -1) for softmaxing all the different keys. The last entry is the number of keys (not the dimensionality of them, like it was befre.)\n",
    "        output = tf.matmul(softmaxed_qk, v) #multiply the attention-weights with the values corresponding to the keys, in respect to the query.\n",
    "        return output, softmaxed_qk\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) #splits up the x data which is gonna be q, k, or v, into the individual heads. effectively adding a dimension (self.num_heads), after splitting up self.d_model\n",
    "        return tf.transpose(x, perm =[0,2,1,3]) #reorganizes the dimensions into the expected order (batch_size, num_heads, seq_len, depth(the new d_model \"fractions\"))\n",
    "\n",
    "    def call(self, q, k ,v, mask = None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        #(linear projection)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        #split them all up into the individual heads. (add a dimension basically)\n",
    "        q = self.split_heads(q , batch_size)\n",
    "        k = self.split_heads(k , batch_size)\n",
    "        v = self.split_heads(v , batch_size)\n",
    "\n",
    "        sdp_attention, attention_weights = self.SDP_Attention(q,k,v, mask = mask) #applies the sdp-attention to all of them. sdp_attention at the end has a shape of: (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        sdp_attention = tf.transpose(sdp_attention, perm=[0, 2, 1, 3]) #swap the 2nd and 3rd dimensions\n",
    "        combined_attention = tf.reshape(sdp_attention, (batch_size, -1, self.d_model)) #combine back the two last dimnensions (num_heads and depth) into the original d_model\n",
    "\n",
    "        output = self.finalDense(combined_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a9f37d-9fd5-4d54-b70a-b34a6bf99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, rate):\n",
    "        super().__init__()\n",
    "        #define all the components of a Layer so the model will learn them properly here.\n",
    "        self.mha = MH_Attention(d_model, num_heads)\n",
    "        self.fnn = point_wise_fnn(d_model, d_ff)\n",
    "\n",
    "        #initiate the 2 normalizations\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        \n",
    "    def call(self,x, training, mask = None):\n",
    "        mha_out, attention_weights = self.mha(x,x,x,mask = mask) #for self-attention: q,k,v = x\n",
    "        mha_out = self.dropout1(mha_out, training = training) #they apply a small dropout of 0.1 after every residual step in the paper.\n",
    "\n",
    "        norm_out = self.norm1(x + mha_out) #first, add the vectors, then normalize them.\n",
    "\n",
    "        fnn_out = self.fnn(norm_out) #2nd sub-layer with fnn\n",
    "        fnn_out = self.dropout2(fnn_out, training = training) #again apply drop out\n",
    "\n",
    "        norm2_out = self.norm2(norm_out + fnn_out) #again add and norm\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042ae57b-c5b7-44f9-a681-4a929f99252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #amount of encoding layers\n",
    "        self.layers = [EncodingLayer(d_model, num_heads, d_ff, rate) for i in range(num_layers)] #define multiple diffferent encoding layers here.\n",
    "\n",
    "        self.dropout = Dropout(rate)\n",
    "            \n",
    "    def call(self, x, training, mask = None):\n",
    "        x = self.dropout(x, training = training) #we want to drop out before the first layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, training = training, mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dafb55e-1d0e-4338-a9ff-12023f91fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, embedding_layer, d_model, max_seq_len, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.d_model = d_model\n",
    "        self.pos_enc = posEncoding(max_seq_len, d_model)\n",
    "        self.Encoder = Encoder(d_model, num_heads, num_layers, d_ff, rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.finalDense = Dense(1, activation = \"linear\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def call(self, x, training, mask = None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.expand_dims(x, axis=-1) #add a dimension to x\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #scale with âˆšd_model\n",
    "        x += self.pos_enc[:, :seq_len, :]\n",
    "        \n",
    "        out_Encoder = self.Encoder(x, training = training, mask = mask)\n",
    "\n",
    "        output = out_Encoder[:,0,:] #pooling: to the first token.\n",
    "        output = self.dropout(output, training = training) #another dropout\n",
    "\n",
    "        final = self.finalDense(output) #now we can reduce back to a single neuron. This is the opposite of what we did in the embedding layer.\n",
    "\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563b50-5de7-4e5f-a365-7e1daf5e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom learning rate schedule class with warmup and cosine decay\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    A custom learning rate schedule that implements a linear warmup\n",
    "    followed by a cosine decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, peak_lr, warmup_steps, decay_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmupCosineDecay\"):\n",
    "            # Ensure step is a float for calculations\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            \n",
    "            # --- 1. Warmup Phase ---\n",
    "            # Linearly increase the learning rate from 0 to peak_lr\n",
    "            warmup_lr = self.peak_lr * (step / self.warmup_steps)\n",
    "\n",
    "            # --- 2. Cosine Decay Phase ---\n",
    "            # Define the cosine decay schedule\n",
    "            cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.peak_lr,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            # Calculate the learning rate for the decay phase.\n",
    "            # Note: The 'step' for the cosine part must be relative to its start.\n",
    "            decay_lr = cosine_decay_schedule(step - self.warmup_steps)\n",
    "\n",
    "            # --- 3. Choose the correct phase ---\n",
    "            # Use tf.where to select the learning rate based on the current step\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps,\n",
    "                warmup_lr,\n",
    "                decay_lr\n",
    "            )\n",
    "            return learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"peak_lr\": self.peak_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97550c6-783b-4fa8-8f66-cd24fd6c3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=False>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "from tensorflow.keras import backend as K\n",
    "def build_model(hp):\n",
    "    K.clear_session()\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [128, 256, 512, 1024])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.05\n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = 25\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = 3\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse'\n",
    "    )\n",
    "    return transformer_model\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3343ff78-3947-49fa-935f-09dcb9c57d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from 2ndTuner/tuner_2/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=False,\n",
    "    directory=\"2ndTuner\",\n",
    "    project_name=\"tuner_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1895cb-38d5-4c54-b3ff-88a7624757d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 02m 44s]\n",
      "val_loss: 18.585067749023438\n",
      "\n",
      "Best val_loss So Far: 18.585067749023438\n",
      "Total elapsed time: 00h 17m 33s\n",
      "\n",
      "Search: Running Trial #9\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |num_heads\n",
      "128               |64                |d_model\n",
      "5                 |6                 |num_layers\n",
      "256               |128               |d_ff\n",
      "False             |False             |dropout\n",
      "0.001631          |8.785e-06         |peak learning rate\n",
      "\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "\n",
    "tuner.search(train_dataset, epochs = num_epochs, validation_data = (val_dataset), verbose = 1, callbacks = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fbd62-fcb6-4f71-87ec-4608e98ccce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e74da-7bcd-49e9-aa43-9c550f8a18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6745e-8bcf-4610-90d8-5c7cacb0a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_best_model(hp, num_epochs):\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [64, 128, 256, 512])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.2 \n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = num_epochs\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = np.floor(num_epochs/10) + 1\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "    return transformer_model, lr_schedule\n",
    "num_epochs_best_model = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ae6b1-f153-468f-8e80-04ee9224040a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from FNN1_1 import baseline_deviation, baeline_out_deviation, baseline_long_deviation, baseline_relError, absSum\n",
    "baseline_out_deviation = baeline_out_deviation\n",
    "from GetXY import x_test, y_test, out_x_test, out_y_test, long_x_test, long_y_test, outsideExpr, absSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70bcea-2acf-4d21-8894-9cee7fdc6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_value = 15\n",
    "x_test = np.pad(x_test, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "out_x_test = np.pad(out_x_test, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "long_x_test = np.pad(long_x_test, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "\n",
    "x_test_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size)\n",
    "out_x_test_dataset = tf.data.Dataset.from_tensor_slices(out_x_test).batch(batch_size)\n",
    "long_x_test_dataset = tf.data.Dataset.from_tensor_slices(long_x_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33590d-f683-4ef2-85cb-92f125512f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debuggng: \n",
    "# Add a custom callback to track the best epoch\n",
    "class BestEpochTracker(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_epoch = epoch + 1\n",
    "            print(f\"New best validation loss: {val_loss:.4f} at epoch {self.best_epoch}\")\n",
    "\n",
    "best_tracker = BestEpochTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e31fa-53d2-49e0-b5ee-c28036fe4e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_bootstrap = 5\n",
    "count = 0\n",
    "bootstrap_predsInRange = []\n",
    "bootstrap_predsOutRange = []\n",
    "bootstrap_predsLongRange = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=20,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "        monitor='val_loss',\n",
    "        mode=\"min\", \n",
    "        verbose=1\n",
    "    )\n",
    "    tf.random.set_seed(i * 12345)  # Different seed each iteration\n",
    "    best_model, lr_schedule = build_best_model(best_hps, num_epochs_best_model)\n",
    "    best_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    best_model.fit(\n",
    "        train_dataset, # Pass the TensorFlow Dataset\n",
    "        validation_data=val_dataset, # Pass the TensorFlow Dataset\n",
    "        epochs=num_epochs_best_model,\n",
    "        callbacks=[early_stopping, best_tracker],\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    bootstrap_predsInRange.append(best_model.predict(x_test_dataset))\n",
    "    bootstrap_predsOutRange.append(best_model.predict(out_x_test_dataset))\n",
    "    bootstrap_predsLongRange.append(best_model.predict(long_x_test_dataset))\n",
    "    print(f\"Done: {count}\")\n",
    "    count += 1\n",
    "\n",
    "bootstrap_predsInRange = np.array(bootstrap_predsInRange)\n",
    "bootstrap_predsOutRange = np.array(bootstrap_predsOutRange)\n",
    "bootstrap_predsLongRange = np.array(bootstrap_predsLongRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc42eb-b271-4160-aea5-3dd9ef377a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f1ea3-7b7c-40fb-99f2-41f68005243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_predsInRange.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632d837-05ff-46a3-bb6e-3f4e8e7ffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predsInRange = []\n",
    "predsOutRange = []\n",
    "predsLongRange = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    counter = 0\n",
    "    for j in range(n_bootstrap):\n",
    "        counter += bootstrap_predsInRange[j][i]\n",
    "    predsInRange.append(counter/n_bootstrap)\n",
    "\n",
    "for i in range(len(out_x_test)):\n",
    "    counter = 0\n",
    "    for j in range(n_bootstrap):\n",
    "        counter += bootstrap_predsOutRange[j][i]\n",
    "    predsOutRange.append(counter/n_bootstrap)\n",
    "\n",
    "for i in range(len(long_x_test)):\n",
    "    counter = 0\n",
    "    for j in range(n_bootstrap):\n",
    "        counter += bootstrap_predsLongRange[j][i]\n",
    "    predsLongRange.append(counter/n_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbd56c-6d64-41cf-8bef-13a06fb0737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffInRange = []\n",
    "for i in range(len(y_test)):\n",
    "    diffInRange.append(abs(y_test[i] - predsInRange[i]))\n",
    "print(len(diffInRange))\n",
    "diffLongRange = []\n",
    "for i in range(200, 300):\n",
    "    diffLongRange.append(np.array(np.abs(long_y_test[i]) - np.array(predsInRange[i])))\n",
    "    long_y_test[i]\n",
    "    predsLongRange[i]\n",
    "\n",
    "diffOutRange = []\n",
    "for i in range(len(out_y_test)):\n",
    "    diffOutRange.append(abs(out_y_test[i] - predsOutRange[i]))\n",
    "safe_out_y_test = np.where(out_y_test == 0, 1, out_y_test)\n",
    "diff_out_relError = []\n",
    "for i in range(len(out_y_test)):\n",
    "    diff_out_relError.append(diffOutRange[i] / safe_out_y_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59665095-8f64-48a6-8126-757700d50aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder = absSum(outsideExpr)\n",
    "diffOutRange = []\n",
    "indices_with_placeholder_22 = [i for i, val in enumerate(placeholder) if val == 22] \n",
    "\n",
    "for i in indices_with_placeholder_22:\n",
    "    diffOutRange.append(np.abs(out_y_test[i]-predsOutRange[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b36b63-6c0a-43db-b375-97a8a7cb6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDiff_InRange = np.mean(diffInRange)\n",
    "meanDiff_OutRange = np.mean(diffOutRange)\n",
    "meanDiff_LongRange = np.mean(diffLongRange)\n",
    "meanDiff_OutRelRange = np.mean(diff_out_relError)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae46e3-0413-4e6b-879b-2b280245d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = 0\n",
    "benchmark += baseline_deviation / (meanDiff_InRange**2) / 4\n",
    "print(baseline_deviation / (meanDiff_InRange**2) / 4)\n",
    "\n",
    "benchmark += baseline_out_deviation / (meanDiff_OutRange**2) / 4\n",
    "print(baseline_out_deviation / (meanDiff_OutRange**2) / 4)\n",
    "\n",
    "benchmark += baseline_long_deviation / (meanDiff_LongRange**2) / 4\n",
    "print(baseline_long_deviation / (meanDiff_LongRange**2) / 4)\n",
    "\n",
    "benchmark += baseline_relError / (meanDiff_OutRelRange**2) / 4\n",
    "print(baseline_relError / (meanDiff_OutRelRange**2) / 4)\n",
    "\n",
    "print(f\"Benchmark: {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18978947-44cc-449c-a4c9-8bae99c162f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bootstrap_predsInRange[2][2])\n",
    "print(x_test[0])\n",
    "print(y_test[2], predsInRange[0])\n",
    "\n",
    "\n",
    "print(baseline_deviation, baseline_out_deviation, baseline_long_deviation, baseline_relError)\n",
    "print(meanDiff_InRange**2, meanDiff_OutRange**2, meanDiff_LongRange**2, meanDiff_OutRelRange**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fdf34-1945-4aae-b670-03c540df22f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e6d46-0876-4945-87e0-408346febc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a1dd0-e426-428e-8401-b7e52c16d815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old versions and stuff (TF GPU)",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
