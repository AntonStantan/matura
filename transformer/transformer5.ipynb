{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e458af-c009-452d-ba64-b19ab9311aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported libraries!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Layer, Dropout\n",
    "\n",
    "print(\"Successfully imported libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfae1ac-dca8-441d-b1c3-0e4096b967c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow Version: 2.16.1\n",
      "Test computation done on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 21:32:29.663757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.711228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.711412: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.715280: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.715415: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.715511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.821968: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.822315: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.822383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-10-02 21:32:29.822530: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-02 21:32:29.822648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2372 MB memory:  -> device: 0, name: Orin, pci bus id: 0000:00:00.0, compute capability: 8.7\n"
     ]
    }
   ],
   "source": [
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\", gpus)\n",
    "\n",
    "# Check if TensorFlow will place operations on the GPU\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Run a quick test\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"Test computation done on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9435ec-aa9e-4f68-97f2-066e660de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 + 1 + 1\n",
      "2543\n",
      "5.0\n",
      "\n",
      "Expressions not in x:\n",
      "-1 - -5 - -3\n",
      "True\n",
      "1457\n",
      "7.0\n",
      "15\n",
      "-4.0\n",
      "[-5.   1.   1.   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5]\n",
      "Successfully imported variables!\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the current script's directory\n",
    "current_dir = os.path.dirname(os.path.abspath(\"transformer0.ipynb\"))\n",
    "\n",
    "# Get the absolute path of the parent directory (project_folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from GetXY.py\n",
    "from GetXY import x_train, y_train, x_val, y_val\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    "    monitor='mse'\n",
    ")\n",
    "# ... rest of your code\n",
    "print(\"Successfully imported variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe57a9d-1c20-47ec-88c6-51cc140f0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a cls token at the beginning of x_train and x_val\n",
    "pad_value = 15\n",
    "x_train = np.pad(x_train, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "x_val = np.pad(x_val, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c9a70b-4f59-4f7d-b98e-a45c24430049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the positional encoder modelled after the formula in the paper that was cited. (generated by gemini)\n",
    "def posEncoding(max_seq_len, d_model):\n",
    "    # Create a matrix of angles according to the formula\n",
    "    angle_rads = get_angles(np.arange(max_seq_len)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    \n",
    "    # Apply sine to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cosine to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358a7c4d-7c43-47f3-8332-b0f3516f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the point-wise FNN\n",
    "#d_ff = 2048 #(original transformer size)\n",
    "def point_wise_fnn(d_model, d_ff):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(d_ff, activation = \"relu\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\"),\n",
    "        Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c3f8aa-c268-40bc-93e4-8e3f89576ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled dot-product attention\n",
    "class MH_Attention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        #for the split_heads function:\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #for the call function:\n",
    "        #This allows the model to learn the best way to project the input embeddings. (linear projection)\n",
    "        self.wq = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wk = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        self.wv = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "\n",
    "        #it's important to initialize this aswell as the ones above here, so that the model saves the previous weights and is able to learn.\n",
    "        self.finalDense = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def SDP_Attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) #calculate the dotproduct, between the query and a transposed key.\n",
    "        d_k = tf.shape(k)[-1] #read the dimensionality of the key tensor (here d_model/num_heads = depth)\n",
    "        d_k = tf.cast(d_k, tf.float32) #convert to float type\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(d_k) #scale for purposes discussed in their paper.        \n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9) #masking to a big negative number\n",
    "        \n",
    "        softmaxed_qk = tf.nn.softmax(scaled_qk, axis = -1) #apply softmax function (axis = -1) for softmaxing all the different keys. The last entry is the number of keys (not the dimensionality of them, like it was befre.)\n",
    "        output = tf.matmul(softmaxed_qk, v) #multiply the attention-weights with the values corresponding to the keys, in respect to the query.\n",
    "        return output, softmaxed_qk\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) #splits up the x data which is gonna be q, k, or v, into the individual heads. effectively adding a dimension (self.num_heads), after splitting up self.d_model\n",
    "        return tf.transpose(x, perm =[0,2,1,3]) #reorganizes the dimensions into the expected order (batch_size, num_heads, seq_len, depth(the new d_model \"fractions\"))\n",
    "\n",
    "    def call(self, q, k ,v, mask = None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        #(linear projection)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        #split them all up into the individual heads. (add a dimension basically)\n",
    "        q = self.split_heads(q , batch_size)\n",
    "        k = self.split_heads(k , batch_size)\n",
    "        v = self.split_heads(v , batch_size)\n",
    "\n",
    "        sdp_attention, attention_weights = self.SDP_Attention(q,k,v, mask = mask) #applies the sdp-attention to all of them. sdp_attention at the end has a shape of: (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        sdp_attention = tf.transpose(sdp_attention, perm=[0, 2, 1, 3]) #swap the 2nd and 3rd dimensions\n",
    "        combined_attention = tf.reshape(sdp_attention, (batch_size, -1, self.d_model)) #combine back the two last dimnensions (num_heads and depth) into the original d_model\n",
    "\n",
    "        output = self.finalDense(combined_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a9f37d-9fd5-4d54-b70a-b34a6bf99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, rate):\n",
    "        super().__init__()\n",
    "        #define all the components of a Layer so the model will learn them properly here.\n",
    "        self.mha = MH_Attention(d_model, num_heads)\n",
    "        self.fnn = point_wise_fnn(d_model, d_ff)\n",
    "\n",
    "        #initiate the 2 normalizations\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        \n",
    "    def call(self,x, training, mask = None):\n",
    "        mha_out, attention_weights = self.mha(x,x,x,mask = mask) #for self-attention: q,k,v = x\n",
    "        mha_out = self.dropout1(mha_out, training = training) #they apply a small dropout of 0.1 after every residual step in the paper.\n",
    "\n",
    "        norm_out = self.norm1(x + mha_out) #first, add the vectors, then normalize them.\n",
    "\n",
    "        fnn_out = self.fnn(norm_out) #2nd sub-layer with fnn\n",
    "        fnn_out = self.dropout2(fnn_out, training = training) #again apply drop out\n",
    "\n",
    "        norm2_out = self.norm2(norm_out + fnn_out) #again add and norm\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042ae57b-c5b7-44f9-a681-4a929f99252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #amount of encoding layers\n",
    "        self.layers = [EncodingLayer(d_model, num_heads, d_ff, rate) for i in range(num_layers)] #define multiple diffferent encoding layers here.\n",
    "\n",
    "        self.dropout = Dropout(rate)\n",
    "            \n",
    "    def call(self, x, training, mask = None):\n",
    "        x = self.dropout(x, training = training) #we want to drop out before the first layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, training = training, mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dafb55e-1d0e-4338-a9ff-12023f91fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, embedding_layer, d_model, max_seq_len, num_heads, num_layers, d_ff, rate):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.d_model = d_model\n",
    "        self.pos_enc = posEncoding(max_seq_len, d_model)\n",
    "        self.Encoder = Encoder(d_model, num_heads, num_layers, d_ff, rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.finalDense = Dense(1, activation = \"linear\", kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "        \n",
    "    def call(self, x, training, mask = None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.expand_dims(x, axis=-1) #add a dimension to x\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #scale with √d_model\n",
    "        x += self.pos_enc[:, :seq_len, :]\n",
    "        \n",
    "        out_Encoder = self.Encoder(x, training = training, mask = mask)\n",
    "\n",
    "        output = out_Encoder[:,0,:] #pooling: to the first token.\n",
    "        output = self.dropout(output, training = training) #another dropout\n",
    "\n",
    "        final = self.finalDense(output) #now we can reduce back to a single neuron. This is the opposite of what we did in the embedding layer.\n",
    "\n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563b50-5de7-4e5f-a365-7e1daf5e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom learning rate schedule class with warmup and cosine decay\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    A custom learning rate schedule that implements a linear warmup\n",
    "    followed by a cosine decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, peak_lr, warmup_steps, decay_steps, alpha=0.0, name=None):\n",
    "        super().__init__()\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmupCosineDecay\"):\n",
    "            # Ensure step is a float for calculations\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            \n",
    "            # --- 1. Warmup Phase ---\n",
    "            # Linearly increase the learning rate from 0 to peak_lr\n",
    "            warmup_lr = self.peak_lr * (step / self.warmup_steps)\n",
    "\n",
    "            # --- 2. Cosine Decay Phase ---\n",
    "            # Define the cosine decay schedule\n",
    "            cosine_decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.peak_lr,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            # Calculate the learning rate for the decay phase.\n",
    "            # Note: The 'step' for the cosine part must be relative to its start.\n",
    "            decay_lr = cosine_decay_schedule(step - self.warmup_steps)\n",
    "\n",
    "            # --- 3. Choose the correct phase ---\n",
    "            # Use tf.where to select the learning rate based on the current step\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps,\n",
    "                warmup_lr,\n",
    "                decay_lr\n",
    "            )\n",
    "            return learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"peak_lr\": self.peak_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"name\": self.name\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97550c6-783b-4fa8-8f66-cd24fd6c3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=False>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "from tensorflow.keras import backend as K\n",
    "def build_model(hp):\n",
    "    K.clear_session()\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [128, 256, 512, 1024])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.05\n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = 25\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = 3\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse'\n",
    "    )\n",
    "    return transformer_model\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3343ff78-3947-49fa-935f-09dcb9c57d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from 2ndTuner/tuner_2/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=False,\n",
    "    directory=\"2ndTuner\",\n",
    "    project_name=\"tuner_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1895cb-38d5-4c54-b3ff-88a7624757d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "\n",
    "tuner.search(train_dataset, epochs = num_epochs, validation_data = (val_dataset), verbose = 1, callbacks = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2fbd62-fcb6-4f71-87ec-4608e98ccce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in 2ndTuner/tuner_2\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 38 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.000335475909883661\n",
      "Score: 0.04216651991009712\n",
      "\n",
      "Trial 44 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.0003474250203708101\n",
      "Score: 0.045826565474271774\n",
      "\n",
      "Trial 47 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.0002954682078714795\n",
      "Score: 0.047226257622241974\n",
      "\n",
      "Trial 32 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.00022857874505977143\n",
      "Score: 0.0673523098230362\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.00044234303994600396\n",
      "Score: 0.07036179304122925\n",
      "\n",
      "Trial 29 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.00019220184501492638\n",
      "Score: 0.08472851663827896\n",
      "\n",
      "Trial 43 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.00034234887278273505\n",
      "Score: 0.09732010215520859\n",
      "\n",
      "Trial 41 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.0003348040248349321\n",
      "Score: 0.10271967202425003\n",
      "\n",
      "Trial 40 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 6\n",
      "d_ff: 256\n",
      "dropout: False\n",
      "peak learning rate: 0.00028078218893211316\n",
      "Score: 0.11300205439329147\n",
      "\n",
      "Trial 21 summary\n",
      "Hyperparameters:\n",
      "num_heads: 8\n",
      "d_model: 64\n",
      "num_layers: 5\n",
      "d_ff: 512\n",
      "dropout: False\n",
      "peak learning rate: 0.0002059888145878046\n",
      "Score: 0.11380325257778168\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041e74da-7bcd-49e9-aa43-9c550f8a18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b6745e-8bcf-4610-90d8-5c7cacb0a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_best_model(hp, num_epochs):\n",
    "    # A smaller configuration to reduce overfitting\n",
    "    # Ensure compatibility\n",
    "    num_heads = hp.Choice('num_heads', [2, 4, 8])  # Powers of 2 work well\n",
    "    d_model = hp.Choice('d_model', [32, 64, 128])   # Also powers of 2\n",
    "    # This guarantees d_model % num_heads == 0\n",
    "    num_layers = hp.Int('num_layers', 2, 6)\n",
    "    d_ff = hp.Choice('d_ff', [64, 128, 256, 512])   # Multiples that work well\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        dropout_rate = 0.2 \n",
    "    else: \n",
    "        dropout_rate = 0\n",
    "    peak_lr = hp.Float(\"peak learning rate\", min_value = 1e-7, max_value = 1e-2, sampling=\"log\")\n",
    "\n",
    "    embedding_layer = Dense(d_model, kernel_initializer = \"glorot_uniform\", bias_initializer = \"zeros\")\n",
    "    batch_size = 32\n",
    "    num_epochs = num_epochs\n",
    "    max_seq_len = 16\n",
    "    warmup_epochs = np.floor(num_epochs/10) + 1\n",
    "    \n",
    "\n",
    "    \n",
    "    transformer_model = Transformer(\n",
    "        embedding_layer = embedding_layer, \n",
    "        d_model = d_model,\n",
    "        max_seq_len = max_seq_len,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        d_ff = d_ff,\n",
    "        rate = dropout_rate\n",
    "    )\n",
    "\n",
    "\n",
    "        # Calculate steps based on your data\n",
    "    # IMPORTANT: Use the actual length of your training data for this calculation\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    decay_steps = (num_epochs - warmup_epochs) * steps_per_epoch\n",
    "    \n",
    "    # Create an instance of our new scheduler\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        peak_lr=peak_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # This means the LR will decay to 10% of peak_lr\n",
    "    )\n",
    "    return transformer_model, lr_schedule\n",
    "num_epochs_best_model = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9ae6b1-f153-468f-8e80-04ee9224040a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759433554.794494  334898 service.cc:145] XLA service 0xfffe78004710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1759433554.794576  334898 service.cc:153]   StreamExecutor device (0): Orin, Compute Capability 8.7\n",
      "2025-10-02 21:32:34.843611: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-02 21:32:35.084289: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/60\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 29.1281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759433555.576901  334898 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 23.3455 - val_loss: 17.2313\n",
      "Epoch 2/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.2753 - val_loss: 14.4801\n",
      "Epoch 3/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.2188 - val_loss: 12.9862\n",
      "Epoch 4/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.3988 - val_loss: 10.8244\n",
      "Epoch 5/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7764 - val_loss: 7.8180\n",
      "Epoch 6/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.4109 - val_loss: 4.3211\n",
      "Epoch 7/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1744 - val_loss: 1.8494\n",
      "Epoch 8/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4296 - val_loss: 1.0797\n",
      "Epoch 9/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9658 - val_loss: 0.8898\n",
      "Epoch 10/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8114 - val_loss: 0.7633\n",
      "Epoch 11/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7019 - val_loss: 0.6645\n",
      "Epoch 12/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6140 - val_loss: 0.5936\n",
      "Epoch 13/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5436 - val_loss: 0.5390\n",
      "Epoch 14/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4832 - val_loss: 0.4863\n",
      "Epoch 15/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4281 - val_loss: 0.4347\n",
      "Epoch 16/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3776 - val_loss: 0.3834\n",
      "Epoch 17/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3289 - val_loss: 0.3363\n",
      "Epoch 18/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2835 - val_loss: 0.2928\n",
      "Epoch 19/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2459 - val_loss: 0.2555\n",
      "Epoch 20/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2154 - val_loss: 0.2253\n",
      "Epoch 21/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1904 - val_loss: 0.2009\n",
      "Epoch 22/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1694 - val_loss: 0.1795\n",
      "Epoch 23/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1513 - val_loss: 0.1614\n",
      "Epoch 24/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1372 - val_loss: 0.1465\n",
      "Epoch 25/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1258 - val_loss: 0.1336\n",
      "Epoch 26/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1159 - val_loss: 0.1223\n",
      "Epoch 27/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1072 - val_loss: 0.1129\n",
      "Epoch 28/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0998 - val_loss: 0.1047\n",
      "Epoch 29/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0926 - val_loss: 0.0973\n",
      "Epoch 30/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0865 - val_loss: 0.0906\n",
      "Epoch 31/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0810 - val_loss: 0.0844\n",
      "Epoch 32/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0763 - val_loss: 0.0790\n",
      "Epoch 33/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0719 - val_loss: 0.0745\n",
      "Epoch 34/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0677 - val_loss: 0.0704\n",
      "Epoch 35/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0639 - val_loss: 0.0666\n",
      "Epoch 36/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0603 - val_loss: 0.0635\n",
      "Epoch 37/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0569 - val_loss: 0.0602\n",
      "Epoch 38/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0539 - val_loss: 0.0573\n",
      "Epoch 39/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0507 - val_loss: 0.0544\n",
      "Epoch 40/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0477 - val_loss: 0.0519\n",
      "Epoch 41/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0450 - val_loss: 0.0497\n",
      "Epoch 42/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0473\n",
      "Epoch 43/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0453\n",
      "Epoch 44/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0388 - val_loss: 0.0433\n",
      "Epoch 45/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0370 - val_loss: 0.0416\n",
      "Epoch 46/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0353 - val_loss: 0.0396\n",
      "Epoch 47/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0336 - val_loss: 0.0380\n",
      "Epoch 48/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0322 - val_loss: 0.0365\n",
      "Epoch 49/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0308 - val_loss: 0.0353\n",
      "Epoch 50/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0297 - val_loss: 0.0340\n",
      "Epoch 51/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0285 - val_loss: 0.0327\n",
      "Epoch 52/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0276 - val_loss: 0.0315\n",
      "Epoch 53/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0266 - val_loss: 0.0304\n",
      "Epoch 54/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0257 - val_loss: 0.0294\n",
      "Epoch 55/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0249 - val_loss: 0.0282\n",
      "Epoch 56/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0240 - val_loss: 0.0272\n",
      "Epoch 57/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0233 - val_loss: 0.0261\n",
      "Epoch 58/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0226 - val_loss: 0.0250\n",
      "Epoch 59/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0219 - val_loss: 0.0243\n",
      "Epoch 60/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0213 - val_loss: 0.0235\n",
      "Epoch 61/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0208 - val_loss: 0.0229\n",
      "Epoch 62/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0202 - val_loss: 0.0224\n",
      "Epoch 63/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0198 - val_loss: 0.0219\n",
      "Epoch 64/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0193 - val_loss: 0.0214\n",
      "Epoch 65/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0187 - val_loss: 0.0208\n",
      "Epoch 66/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0181 - val_loss: 0.0203\n",
      "Epoch 67/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0177 - val_loss: 0.0197\n",
      "Epoch 68/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0171 - val_loss: 0.0192\n",
      "Epoch 69/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0167 - val_loss: 0.0188\n",
      "Epoch 70/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0164 - val_loss: 0.0185\n",
      "Epoch 71/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0160 - val_loss: 0.0181\n",
      "Epoch 72/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0156 - val_loss: 0.0177\n",
      "Epoch 73/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0153 - val_loss: 0.0174\n",
      "Epoch 74/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0149 - val_loss: 0.0169\n",
      "Epoch 75/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0145 - val_loss: 0.0164\n",
      "Epoch 76/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0140 - val_loss: 0.0159\n",
      "Epoch 77/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0136 - val_loss: 0.0153\n",
      "Epoch 78/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0131 - val_loss: 0.0149\n",
      "Epoch 79/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0128 - val_loss: 0.0145\n",
      "Epoch 80/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0123 - val_loss: 0.0141\n",
      "Epoch 81/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0119 - val_loss: 0.0137\n",
      "Epoch 82/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0115 - val_loss: 0.0133\n",
      "Epoch 83/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0111 - val_loss: 0.0131\n",
      "Epoch 84/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0108 - val_loss: 0.0128\n",
      "Epoch 85/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 86/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 87/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 88/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 89/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0093 - val_loss: 0.0120\n",
      "Epoch 90/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0090 - val_loss: 0.0121\n",
      "Epoch 91/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0088 - val_loss: 0.0123\n",
      "Epoch 92/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0086 - val_loss: 0.0124\n",
      "Epoch 93/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0084 - val_loss: 0.0124\n",
      "Epoch 94/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0082 - val_loss: 0.0126\n",
      "Epoch 95/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0081 - val_loss: 0.0126\n",
      "Epoch 96/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0079 - val_loss: 0.0127\n",
      "Epoch 97/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0078 - val_loss: 0.0129\n",
      "Epoch 98/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0077 - val_loss: 0.0130\n",
      "Epoch 99/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0076 - val_loss: 0.0130\n",
      "Epoch 100/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0075 - val_loss: 0.0128\n",
      "Epoch 101/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0074 - val_loss: 0.0130\n",
      "Epoch 102/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0074 - val_loss: 0.0134\n",
      "Epoch 103/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0074 - val_loss: 0.0134\n",
      "Epoch 104/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0073 - val_loss: 0.0134\n",
      "Epoch 105/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0074 - val_loss: 0.0137\n",
      "Epoch 106/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0075 - val_loss: 0.0141\n",
      "Epoch 1/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 21.5133 - val_loss: 18.1452\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16.9865 - val_loss: 15.2732\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.4003 - val_loss: 13.1675\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.9905 - val_loss: 10.4591\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.8529 - val_loss: 7.0289\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2252 - val_loss: 3.6530\n",
      "Epoch 7/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3694 - val_loss: 1.6793\n",
      "Epoch 8/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1955 - val_loss: 1.0881\n",
      "Epoch 9/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8859 - val_loss: 0.8972\n",
      "Epoch 10/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7451 - val_loss: 0.7873\n",
      "Epoch 11/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6488 - val_loss: 0.7035\n",
      "Epoch 12/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5677 - val_loss: 0.6299\n",
      "Epoch 13/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4968 - val_loss: 0.5580\n",
      "Epoch 14/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4338 - val_loss: 0.4959\n",
      "Epoch 15/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3810 - val_loss: 0.4445\n",
      "Epoch 16/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3404 - val_loss: 0.3985\n",
      "Epoch 17/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3061 - val_loss: 0.3571\n",
      "Epoch 18/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2769 - val_loss: 0.3212\n",
      "Epoch 19/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2503 - val_loss: 0.2903\n",
      "Epoch 20/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2280 - val_loss: 0.2648\n",
      "Epoch 21/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2082 - val_loss: 0.2420\n",
      "Epoch 22/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1905 - val_loss: 0.2221\n",
      "Epoch 23/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1745 - val_loss: 0.2049\n",
      "Epoch 24/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1603 - val_loss: 0.1882\n",
      "Epoch 25/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1473 - val_loss: 0.1724\n",
      "Epoch 26/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1352 - val_loss: 0.1581\n",
      "Epoch 27/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1236 - val_loss: 0.1445\n",
      "Epoch 28/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1127 - val_loss: 0.1320\n",
      "Epoch 29/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1036 - val_loss: 0.1208\n",
      "Epoch 30/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0954 - val_loss: 0.1106\n",
      "Epoch 31/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0880 - val_loss: 0.1021\n",
      "Epoch 32/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0816 - val_loss: 0.0945\n",
      "Epoch 33/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0754 - val_loss: 0.0878\n",
      "Epoch 34/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0699 - val_loss: 0.0813\n",
      "Epoch 35/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0650 - val_loss: 0.0757\n",
      "Epoch 36/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0606 - val_loss: 0.0708\n",
      "Epoch 37/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0568 - val_loss: 0.0659\n",
      "Epoch 38/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0534 - val_loss: 0.0620\n",
      "Epoch 39/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0506 - val_loss: 0.0589\n",
      "Epoch 40/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0480 - val_loss: 0.0563\n",
      "Epoch 41/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0458 - val_loss: 0.0543\n",
      "Epoch 42/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0438 - val_loss: 0.0525\n",
      "Epoch 43/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0419 - val_loss: 0.0508\n",
      "Epoch 44/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0494\n",
      "Epoch 45/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0386 - val_loss: 0.0481\n",
      "Epoch 46/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0373 - val_loss: 0.0472\n",
      "Epoch 47/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0359 - val_loss: 0.0460\n",
      "Epoch 48/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0347 - val_loss: 0.0450\n",
      "Epoch 49/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0335 - val_loss: 0.0437\n",
      "Epoch 50/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0323 - val_loss: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capybara/Desktop/matura_project_python/github/matura/FNN1_1.py:246: RuntimeWarning: divide by zero encountered in divide\n",
      "  relativeError = np.where(np.array(y_test) != 0, deviation.flatten() / np.abs(np.array(y_test)), deviation.flatten())\n"
     ]
    }
   ],
   "source": [
    "from FNN1_1 import baseline_deviation, baeline_out_deviation, baseline_long_deviation, baseline_relError, absSum\n",
    "baseline_out_deviation = baeline_out_deviation\n",
    "from GetXY import x_test, y_test, out_x_test, out_y_test, long_x_test, long_y_test, outsideExpr, absSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b70bcea-2acf-4d21-8894-9cee7fdc6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_value = 15\n",
    "x_test = np.pad(x_test, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "out_x_test = np.pad(out_x_test, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "long_x_test = np.pad(long_x_test, ((0, 0), (1, 0)), 'constant', constant_values=pad_value)\n",
    "\n",
    "x_test_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size)\n",
    "out_x_test_dataset = tf.data.Dataset.from_tensor_slices(out_x_test).batch(batch_size)\n",
    "long_x_test_dataset = tf.data.Dataset.from_tensor_slices(long_x_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd33590d-f683-4ef2-85cb-92f125512f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debuggng: \n",
    "# Add a custom callback to track the best epoch\n",
    "class BestEpochTracker(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_epoch = epoch + 1\n",
    "            print(f\"New best validation loss: {val_loss:.4f} at epoch {self.best_epoch}\")\n",
    "\n",
    "best_tracker = BestEpochTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc3e31fa-53d2-49e0-b5ee-c28036fe4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759433650.700856  340405 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433651.137123  340404 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 26.1216 - mse: 26.1216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759433675.861707  340688 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433676.627604  340690 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 308 bytes spill stores, 308 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433676.775417  340691 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433678.188799  340689 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_157', 8 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433678.788352  340687 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_157', 16 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433679.444939  340692 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_157', 180 bytes spill stores, 188 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - loss: 26.1123 - mse: 26.1123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759433694.913252  340938 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433695.258000  340935 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759433696.463669  340938 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 308 bytes spill stores, 308 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 25.9389 at epoch 1\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 575ms/step - loss: 25.5610 - mse: 25.5610 - val_loss: 25.9389 - val_mse: 25.9389\n",
      "Epoch 2/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 23.6191 - mse: 23.6191New best validation loss: 24.5387 at epoch 2\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 23.3334 - mse: 23.3334 - val_loss: 24.5387 - val_mse: 24.5387\n",
      "Epoch 3/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 23.5166 - mse: 23.5166New best validation loss: 23.5395 at epoch 3\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 22.2651 - mse: 22.2651 - val_loss: 23.5395 - val_mse: 23.5395\n",
      "Epoch 4/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 19.5409 - mse: 19.5409New best validation loss: 21.1664 at epoch 4\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 21.0517 - mse: 21.0517 - val_loss: 21.1664 - val_mse: 21.1664\n",
      "Epoch 5/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 18.9657 - mse: 18.9657New best validation loss: 19.9223 at epoch 5\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 20.1915 - mse: 20.1915 - val_loss: 19.9223 - val_mse: 19.9223\n",
      "Epoch 6/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 18.7299 - mse: 18.7299New best validation loss: 17.0319 at epoch 6\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 18.3984 - mse: 18.3984 - val_loss: 17.0319 - val_mse: 17.0319\n",
      "Epoch 7/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 16.7637 - mse: 16.7637 - val_loss: 19.6394 - val_mse: 19.6394\n",
      "Epoch 8/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 14.6497 - mse: 14.6497New best validation loss: 14.6446 at epoch 8\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 14.6347 - mse: 14.6347 - val_loss: 14.6446 - val_mse: 14.6446\n",
      "Epoch 9/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 14.1950 - mse: 14.1950New best validation loss: 11.5140 at epoch 9\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 13.6565 - mse: 13.6565 - val_loss: 11.5140 - val_mse: 11.5140\n",
      "Epoch 10/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 13.7194 - mse: 13.7194 - val_loss: 12.0797 - val_mse: 12.0797\n",
      "Epoch 11/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 14.2055 - mse: 14.2055 - val_loss: 12.1927 - val_mse: 12.1927\n",
      "Epoch 12/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 14.2905 - mse: 14.2905 - val_loss: 14.5323 - val_mse: 14.5323\n",
      "Epoch 13/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 16.9963 - mse: 16.9963 - val_loss: 20.3930 - val_mse: 20.3930\n",
      "Epoch 14/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 16.9272 - mse: 16.9272 - val_loss: 11.9898 - val_mse: 11.9898\n",
      "Epoch 15/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 12.6680 - mse: 12.6680 - val_loss: 12.1298 - val_mse: 12.1298\n",
      "Epoch 16/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 11.9369 - mse: 11.9369 - val_loss: 14.2014 - val_mse: 14.2014\n",
      "Epoch 17/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 12.8764 - mse: 12.8764 - val_loss: 13.0122 - val_mse: 13.0122\n",
      "Epoch 18/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 12.4441 - mse: 12.4441 - val_loss: 14.8001 - val_mse: 14.8001\n",
      "Epoch 19/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 16.4433 - mse: 16.4433 - val_loss: 18.7148 - val_mse: 18.7148\n",
      "Epoch 20/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 15.8190 - mse: 15.8190 - val_loss: 12.7837 - val_mse: 12.7837\n",
      "Epoch 21/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 13.2449 - mse: 13.2449 - val_loss: 12.3966 - val_mse: 12.3966\n",
      "Epoch 22/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 11.4703 - mse: 11.4703 - val_loss: 13.1554 - val_mse: 13.1554\n",
      "Epoch 23/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.8434 - mse: 9.8434New best validation loss: 5.5664 at epoch 23\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 9.6912 - mse: 9.6912 - val_loss: 5.5664 - val_mse: 5.5664\n",
      "Epoch 24/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.2055 - mse: 6.2055New best validation loss: 3.4926 at epoch 24\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 5.9935 - mse: 5.9935 - val_loss: 3.4926 - val_mse: 3.4926\n",
      "Epoch 25/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.8673 - mse: 6.8673New best validation loss: 1.9049 at epoch 25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 6.1345 - mse: 6.1345 - val_loss: 1.9049 - val_mse: 1.9049\n",
      "Epoch 26/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 2.3058 - mse: 2.3058 - val_loss: 6.5657 - val_mse: 6.5657\n",
      "Epoch 27/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.8341 - mse: 3.8341New best validation loss: 0.8818 at epoch 27\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 2.4162 - mse: 2.4162 - val_loss: 0.8818 - val_mse: 0.8818\n",
      "Epoch 28/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 5.8469 - mse: 5.8469 - val_loss: 2.2363 - val_mse: 2.2363\n",
      "Epoch 29/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.5798 - mse: 1.5798 - val_loss: 1.8912 - val_mse: 1.8912\n",
      "Epoch 30/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 2.5070 - mse: 2.5070 - val_loss: 4.8935 - val_mse: 4.8935\n",
      "Epoch 31/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.8393 - mse: 1.8393New best validation loss: 0.8375 at epoch 31\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.1141 - mse: 1.1141 - val_loss: 0.8375 - val_mse: 0.8375\n",
      "Epoch 32/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.8030 - mse: 0.8030New best validation loss: 0.6967 at epoch 32\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.6803 - mse: 0.6803 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 33/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.6448 - mse: 0.6448New best validation loss: 0.3552 at epoch 33\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.5641 - mse: 0.5641 - val_loss: 0.3552 - val_mse: 0.3552\n",
      "Epoch 34/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.7673 - mse: 0.7673 - val_loss: 0.8085 - val_mse: 0.8085\n",
      "Epoch 35/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.0782 - mse: 1.0782 - val_loss: 1.4862 - val_mse: 1.4862\n",
      "Epoch 36/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.1053 - mse: 1.1053 - val_loss: 0.4507 - val_mse: 0.4507\n",
      "Epoch 37/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.7332 - mse: 1.7332 - val_loss: 0.5359 - val_mse: 0.5359\n",
      "Epoch 38/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.9925 - mse: 1.9925 - val_loss: 2.8691 - val_mse: 2.8691\n",
      "Epoch 39/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.5506 - mse: 1.5506 - val_loss: 0.3715 - val_mse: 0.3715\n",
      "Epoch 40/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.5671 - mse: 1.5671 - val_loss: 0.7057 - val_mse: 0.7057\n",
      "Epoch 41/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3409 - mse: 0.3409New best validation loss: 0.2365 at epoch 41\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.3059 - mse: 0.3059 - val_loss: 0.2365 - val_mse: 0.2365\n",
      "Epoch 42/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.6167 - mse: 0.6167 - val_loss: 2.0677 - val_mse: 2.0677\n",
      "Epoch 43/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.3669 - mse: 1.3669 - val_loss: 0.4484 - val_mse: 0.4484\n",
      "Epoch 44/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.8207 - mse: 0.8207 - val_loss: 1.2200 - val_mse: 1.2200\n",
      "Epoch 45/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.4215 - mse: 1.4215 - val_loss: 0.3733 - val_mse: 0.3733\n",
      "Epoch 46/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.6011 - mse: 0.6011 - val_loss: 0.3034 - val_mse: 0.3034\n",
      "Epoch 47/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2431 - mse: 0.2431New best validation loss: 0.1384 at epoch 47\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.2128 - mse: 0.2128 - val_loss: 0.1384 - val_mse: 0.1384\n",
      "Epoch 48/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1483 - mse: 0.1483New best validation loss: 0.1354 at epoch 48\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.1354 - val_mse: 0.1354\n",
      "Epoch 49/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.2847 - mse: 0.2847 - val_loss: 0.2525 - val_mse: 0.2525\n",
      "Epoch 50/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2954 - mse: 1.2954New best validation loss: 0.1082 at epoch 50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.9938 - mse: 0.9938 - val_loss: 0.1082 - val_mse: 0.1082\n",
      "Epoch 51/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.1296 - mse: 0.1296 - val_loss: 0.1547 - val_mse: 0.1547\n",
      "Epoch 52/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1508 - mse: 0.1508New best validation loss: 0.1010 at epoch 52\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1313 - mse: 0.1313 - val_loss: 0.1010 - val_mse: 0.1010\n",
      "Epoch 53/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1042 - mse: 0.1042 - val_loss: 0.1095 - val_mse: 0.1095\n",
      "Epoch 54/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 1.8040 - mse: 1.8040 - val_loss: 0.3189 - val_mse: 0.3189\n",
      "Epoch 55/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1568 - mse: 0.1568 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 56/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.5838 - mse: 0.5838New best validation loss: 0.0989 at epoch 56\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.6165 - mse: 0.6165 - val_loss: 0.0989 - val_mse: 0.0989\n",
      "Epoch 57/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.2534 - mse: 0.2534 - val_loss: 0.4123 - val_mse: 0.4123\n",
      "Epoch 58/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1728 - mse: 0.1728 - val_loss: 0.1657 - val_mse: 0.1657\n",
      "Epoch 59/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1329 - val_mse: 0.1329\n",
      "Epoch 60/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.1054 - val_mse: 0.1054\n",
      "Epoch 61/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1360 - mse: 0.1360 - val_loss: 0.1572 - val_mse: 0.1572\n",
      "Epoch 62/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1062 - mse: 0.1062New best validation loss: 0.0938 at epoch 62\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.0938 - val_mse: 0.0938\n",
      "Epoch 63/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.1485 - val_mse: 0.1485\n",
      "Epoch 64/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0825 - mse: 0.0825New best validation loss: 0.0678 at epoch 64\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0663 - mse: 0.0663 - val_loss: 0.0678 - val_mse: 0.0678\n",
      "Epoch 65/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0657 - mse: 0.0657 - val_loss: 2.1129 - val_mse: 2.1129\n",
      "Epoch 66/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.3381 - mse: 0.3381 - val_loss: 0.2165 - val_mse: 0.2165\n",
      "Epoch 67/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1128 - mse: 0.1128New best validation loss: 0.0589 at epoch 67\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0772 - mse: 0.0772 - val_loss: 0.0589 - val_mse: 0.0589\n",
      "Epoch 68/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0406 - mse: 0.0406New best validation loss: 0.0587 at epoch 68\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0587 - val_mse: 0.0587\n",
      "Epoch 69/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 70/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0637 - mse: 0.0637 - val_loss: 0.1372 - val_mse: 0.1372\n",
      "Epoch 71/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.3730 - mse: 0.3730 - val_loss: 0.1075 - val_mse: 0.1075\n",
      "Epoch 72/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0813 - mse: 0.0813New best validation loss: 0.0443 at epoch 72\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0639 - mse: 0.0639 - val_loss: 0.0443 - val_mse: 0.0443\n",
      "Epoch 73/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0263 - mse: 0.0263New best validation loss: 0.0382 at epoch 73\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 74/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 75/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0475 - mse: 0.0475New best validation loss: 0.0312 at epoch 75\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 76/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0263 - mse: 0.0263New best validation loss: 0.0249 at epoch 76\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 77/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 78/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 79/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 80/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0194 - mse: 0.0194New best validation loss: 0.0239 at epoch 80\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 81/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 82/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0172 - mse: 0.0172New best validation loss: 0.0216 at epoch 82\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 83/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 84/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0538 - val_mse: 0.0538\n",
      "Epoch 85/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0535 - val_mse: 0.0535\n",
      "Epoch 86/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 87/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 88/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 89/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0109 - mse: 0.0109New best validation loss: 0.0207 at epoch 89\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 90/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0133 - mse: 0.0133New best validation loss: 0.0157 at epoch 90\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 91/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0098 - mse: 0.0098New best validation loss: 0.0154 at epoch 91\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 92/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 93/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0168 - mse: 0.0168New best validation loss: 0.0135 at epoch 93\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 94/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 95/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 96/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 97/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 98/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0071 - mse: 0.0071New best validation loss: 0.0127 at epoch 98\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 99/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 100/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 101/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0210 - val_mse: 0.0210\n",
      "Epoch 102/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 103/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 104/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 105/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 106/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 107/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 108/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0534 - val_mse: 0.0534\n",
      "Epoch 109/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 110/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0159 - mse: 0.0159New best validation loss: 0.0125 at epoch 110\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 111/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 112/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0124 - mse: 0.0124New best validation loss: 0.0114 at epoch 112\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 113/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 114/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 115/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0029 - mse: 0.0029New best validation loss: 0.0079 at epoch 115\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 116/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 117/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 118/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 119/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 120/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 121/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 122/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 123/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 124/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 125/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 126/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 127/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 128/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 129/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 130/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 131/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 132/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 133/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 134/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 135/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0017 - mse: 0.0017New best validation loss: 0.0067 at epoch 135\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 136/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 137/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 138/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 139/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 140/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 141/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 142/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 143/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 144/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 145/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 146/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 147/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0014 - mse: 0.0014New best validation loss: 0.0066 at epoch 147\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 148/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 149/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0026 - mse: 0.0026New best validation loss: 0.0061 at epoch 149\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 150/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 151/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 152/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0014 - mse: 0.0014New best validation loss: 0.0060 at epoch 152\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 153/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 7.1269e-04 - mse: 7.1269e-04 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 154/200\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 155/200\n",
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0015 - mse: 0.0015New best validation loss: 0.0059 at epoch 155\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 155: early stopping\n",
      "Restoring model weights from the end of the best epoch: 135.\n",
      "\u001b[1m40/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759434019.766833  347951 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759434020.563914  347950 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759434021.059417  347948 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 308 bytes spill stores, 308 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 162ms/step\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step\n",
      "Done: 0\n"
     ]
    }
   ],
   "source": [
    "n_bootstrap = 1\n",
    "count = 0\n",
    "bootstrap_predsInRange = []\n",
    "bootstrap_predsOutRange = []\n",
    "bootstrap_predsLongRange = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=20,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "        monitor='val_loss',\n",
    "        mode=\"min\", \n",
    "        verbose=1\n",
    "    )\n",
    "    tf.random.set_seed(i * 12345)  # Different seed each iteration\n",
    "    best_model, lr_schedule = build_best_model(best_hps, num_epochs_best_model)\n",
    "    best_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay = 4e-3,\n",
    "            beta_1=0.85,  \n",
    "            beta_2=0.999,  # Primary recommendation: lower this\n",
    "            clipnorm=1.0\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    best_model.fit(\n",
    "        train_dataset, # Pass the TensorFlow Dataset\n",
    "        validation_data=val_dataset, # Pass the TensorFlow Dataset\n",
    "        epochs=num_epochs_best_model,\n",
    "        callbacks=[early_stopping, best_tracker],\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    bootstrap_predsInRange.append(best_model.predict(x_test_dataset))\n",
    "    bootstrap_predsOutRange.append(best_model.predict(out_x_test_dataset))\n",
    "    bootstrap_predsLongRange.append(best_model.predict(long_x_test_dataset))\n",
    "    print(f\"Done: {count}\")\n",
    "    count += 1\n",
    "\n",
    "bootstrap_predsInRange = np.array(bootstrap_predsInRange)\n",
    "bootstrap_predsOutRange = np.array(bootstrap_predsOutRange)\n",
    "bootstrap_predsLongRange = np.array(bootstrap_predsLongRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cdc42eb-b271-4160-aea5-3dd9ef377a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">498,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m498,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,494,724</span> (5.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,494,724\u001b[0m (5.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">498,241</span> (1.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m498,241\u001b[0m (1.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">996,483</span> (3.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m996,483\u001b[0m (3.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3f1ea3-7b7c-40fb-99f2-41f68005243f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1457, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_predsInRange.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e632d837-05ff-46a3-bb6e-3f4e8e7ffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predsInRange = []\n",
    "predsOutRange = []\n",
    "predsLongRange = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    counter = 0\n",
    "    for j in range(n_bootstrap):\n",
    "        counter += bootstrap_predsInRange[j][i]\n",
    "    predsInRange.append(counter/n_bootstrap)\n",
    "\n",
    "for i in range(len(out_x_test)):\n",
    "    counter = 0\n",
    "    for j in range(n_bootstrap):\n",
    "        counter += bootstrap_predsOutRange[j][i]\n",
    "    predsOutRange.append(counter/n_bootstrap)\n",
    "\n",
    "for i in range(len(long_x_test)):\n",
    "    counter = 0\n",
    "    for j in range(n_bootstrap):\n",
    "        counter += bootstrap_predsLongRange[j][i]\n",
    "    predsLongRange.append(counter/n_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4fbd56c-6d64-41cf-8bef-13a06fb0737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457\n",
      "MAE in Range:  0.038198326\n",
      "MRE in Range:  0.015183598\n",
      "MAE longer Expressions:  6.0680327\n",
      "MAE out Range:  4.9287004\n",
      "MRE out Range:  0.54311574\n"
     ]
    }
   ],
   "source": [
    "reldiffInRange = []\n",
    "diffInRange = []\n",
    "safe_y_test = np.where(np.isclose(y_test,0.0), 1.0, y_test)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    diffInRange.append(abs(y_test[i] - predsInRange[i]))\n",
    "    reldiffInRange.append(abs(y_test[i] - predsInRange[i])/abs(safe_y_test[i]))\n",
    "print(len(diffInRange))\n",
    "print(\"MAE in Range: \", np.mean(diffInRange))\n",
    "print(\"MRE in Range: \", np.mean(reldiffInRange))\n",
    "\n",
    "diffLongRange = []\n",
    "for i in range(200, 300):\n",
    "    diffLongRange.append(np.array(np.abs(long_y_test[i]) - np.array(predsInRange[i])))\n",
    "    \n",
    "NEEDdiffLongRange = []\n",
    "for i in range(len(long_y_test)):\n",
    "    NEEDdiffLongRange.append(np.array(np.abs(long_y_test[i]) - np.array(predsInRange[i])))\n",
    "print(\"MAE longer Expressions: \", np.mean(NEEDdiffLongRange))\n",
    "\n",
    "diffOutRange = []\n",
    "for i in range(len(out_y_test)):\n",
    "    diffOutRange.append(abs(out_y_test[i] - predsOutRange[i]))\n",
    "safe_out_y_test = np.where(out_y_test == 0, 1, out_y_test)\n",
    "diff_out_relError = []\n",
    "for i in range(len(out_y_test)):\n",
    "    diff_out_relError.append(abs(diffOutRange[i] / safe_out_y_test[i]))\n",
    "print(\"MAE out Range: \", np.mean(diffOutRange))\n",
    "print(\"MRE out Range: \", np.mean(diff_out_relError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59665095-8f64-48a6-8126-757700d50aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder = absSum(outsideExpr)\n",
    "diffOutRange = []\n",
    "indices_with_placeholder_22 = [i for i, val in enumerate(placeholder) if val == 22] \n",
    "\n",
    "for i in indices_with_placeholder_22:\n",
    "    diffOutRange.append(np.abs(out_y_test[i]-predsOutRange[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b36b63-6c0a-43db-b375-97a8a7cb6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDiff_InRange = np.mean(diffInRange)\n",
    "meanDiff_OutRange = np.mean(diffOutRange)\n",
    "meanDiff_LongRange = np.mean(diffLongRange)\n",
    "meanDiff_OutRelRange = np.mean(diff_out_relError)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eae46e3-0413-4e6b-879b-2b280245d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066194249259755\n",
      "0.0612270529439841\n",
      "0.1327205835189104\n",
      "0.02949489364091677\n",
      "Benchmark: 5.289636779363566\n"
     ]
    }
   ],
   "source": [
    "benchmark = 0\n",
    "benchmark += baseline_deviation / (meanDiff_InRange**2) / 4\n",
    "print(baseline_deviation / (meanDiff_InRange**2) / 4)\n",
    "\n",
    "benchmark += baseline_out_deviation / (meanDiff_OutRange**2) / 4\n",
    "print(baseline_out_deviation / (meanDiff_OutRange**2) / 4)\n",
    "\n",
    "benchmark += baseline_long_deviation / (meanDiff_LongRange**2) / 4\n",
    "print(baseline_long_deviation / (meanDiff_LongRange**2) / 4)\n",
    "\n",
    "benchmark += baseline_relError / (meanDiff_OutRelRange**2) / 4\n",
    "print(baseline_relError / (meanDiff_OutRelRange**2) / 4)\n",
    "\n",
    "print(f\"Benchmark: {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fdf34-1945-4aae-b670-03c540df22f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e6d46-0876-4945-87e0-408346febc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a1dd0-e426-428e-8401-b7e52c16d815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old versions and stuff (TF GPU)",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
