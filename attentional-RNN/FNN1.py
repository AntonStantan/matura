# -*- coding: utf-8 -*-
"""FNN1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sek7rICQu7h8A-vxXGQ-8zIXsaD1NQKo
"""

import numpy as np
np.random.seed(42)

x1 = np.random.rand(4000)*10
x2 = np.random.rand(4000)*10
x3 = np.random.rand(4000)*10

x1_int = x1.astype(int) - 5
x2_int = x2.astype(int) - 5
x3_int = x3.astype(int) - 5

x1_str = x1_int.astype(str)
x2_str = x2_int.astype(str)
x3_str = x3_int.astype(str)

unique_expressions = set()

for i in range(len(x1)):
  n = np.random.rand(1)
  if n < 0.25:
    opp1 = " + "
    opp2 = " + "
  elif n > 0.25 and n < 0.5:
    opp1 = " + "
    opp2 = " - "
  elif n > 0.75:
    opp1 = " - "
    opp2 = " + "
  else:
    opp1 = " - "
    opp2 = " - "
  unique_expressions.add(x1_str[i] + opp1 + x2_str[i] + opp2 + x3_str[i])

x = list(unique_expressions) # Convert the set back to a list
print(x)
print(len(x))
diff_x = x

y = []

for expression in x:
  result = float(eval(expression))
  y.append(result)

print(y)
diff_y = y

def tokenizer(input_list): # Changed parameter name to avoid confusion with global x
  #tokenizer by hand
  #tokens = (len(input_list), 5)
  # Create a copy of the input list to avoid modifying the original
  tokenized_x = [expression.split(" ") for expression in input_list]


  for i in range(len(tokenized_x)):
    for j in range(len(tokenized_x[i])):
      if j % 2 == 0:  # Check if the index is even
        tokenized_x[i][j] = np.float32(tokenized_x[i][j])
      else:  # The index is uneven, it's an operator
        if tokenized_x[i][j] == "+":
          tokenized_x[i][j] = np.float32(1)
        else:
          tokenized_x[i][j] = np.float32(0)
    padding_count = 15 - len(tokenized_x[i])
    for _ in range(padding_count): # Use a throwaway variable
      tokenized_x[i].append(np.float32(0.5))
  tokenized_x = np.array(tokenized_x)
  return tokenized_x
x = tokenizer(x)

from transformers import BertTokenizer

# Load a pre-trained BERT tokenizer (e.g., 'bert-base-uncased')
# 'uncased' means the model treats "Hello" and "hello" the same.
# For case-sensitive models, use 'bert-base-cased'.
BERTtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# You can also use AutoTokenizer which automatically detects the correct tokenizer
# based on the model name. This is generally recommended.
# from transformers import AutoTokenizer
# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

BERTtokenizer.tokenize("1 + 2 + allo")

'''
def posTokenizer(input_list):
    tokenized_x = [expression.split(" ") for expression in input_list]

    for i in range(len(tokenized_x)):
        for j in range(len(tokenized_x[i])):
            # Get the numerical value of the token
            token_value = 0.0 # Default
            if j % 2 == 0:  # Check if the index is even (number)
                token_value = np.float32(tokenized_x[i][j])
            else:  # The index is uneven (operator)
                if tokenized_x[i][j] == "+":
                    token_value = np.float32(1)
                else:
                    token_value = np.float32(0)

            # Now store it as a [value, position] pair
            tokenized_x[i][j] = np.array([token_value, np.float32(j)]) # Store as a 2-element array

        # Padding
        padding_count = 15 - len(tokenized_x[i])
        for _ in range(padding_count):
            # For padding, you might just want a consistent padding value
            # or a specific padding for value and position.
            # Here, we'll use 0.5 for value and 0.0 for position for padding.
            # Or you could encode the padding position if needed.
            tokenized_x[i].append(np.array([np.float32(0.5), np.float32(0.0)])) # Padding as [0.5, 0.0]

    # Convert the list of lists of arrays into a 3D NumPy array
    # e.g., (batch_size, sequence_length, 2)
    tokenized_x = np.array(tokenized_x)
    return tokenized_x
diff_x = posTokenizer(diff_x)
'''

def posTokenizer(input_list): # Changed parameter name to avoid confusion with global x

  tokenized_x = [expression.split(" ") for expression in input_list]
  max_sequence_length = 15  # Define a max length for consistent padding and PE

  positionally_encoded_x = []

  for i in range(len(tokenized_x)):
      current_sequence = []
      for j in range(len(tokenized_x[i])):
          token_val = tokenized_x[i][j]
          if j % 2 == 0:  # Even index: number
              encoded_val = np.float32(token_val) # No change in value for numbers, only their position matters
          else:  # Uneven index: operator
              if token_val == "+":
                  encoded_val = np.float32(1)
              else:
                  encoded_val = np.float32(0)

          positional_component = np.sin(j / (10000**(0/1))) # Always 0 for even term, effectively sin(j)
          if j % 2 != 0:
              positional_component = np.cos(j / (10000**(0/1)))
          current_sequence.append([encoded_val, np.float32(positional_component)])



      # Padding
      padding_count = max_sequence_length - len(current_sequence)
      for _ in range(padding_count):
          # For padding, we can also add a positional encoding to the padding value (0.5)
          # or simply use 0.5 without further PE for padding.
          # Let's add PE to make it consistent
          padding_position = len(current_sequence) + _
          padding_positional_component = np.sin(padding_position / (10000**(0/1)))
          if padding_position % 2 != 0:
              padding_positional_component = np.cos(padding_position / (10000**(0/1)))

          current_sequence.append([np.float32(0.5), np.float32(padding_positional_component)])

      positionally_encoded_x.append(current_sequence)

  tokenized_x = np.array(positionally_encoded_x)
  return tokenized_x
print(diff_x[0])
diff_x = posTokenizer(diff_x)
print(diff_x[0])

print(posTokenizer(["1 + 2 + 3"]).shape)
print(tokenizer(["1 + 2 + 3"]).shape)

# Generate all possible expressions
all_possible_expressions = set()
for num1 in range(-5, 5): # Range -5 to 4
    for num2 in range(-5, 5): # Range -5 to 4
        for num3 in range(-5, 5): # Range -5 to 4
            for op1 in [" + ", " - "]:
                for op2 in [" + ", " - "]:
                    expression = str(num1) + op1 + str(num2) + op2 + str(num3)
                    all_possible_expressions.add(expression)

# Find expressions not in x
expressions_not_in_x = all_possible_expressions - unique_expressions
expressions_not_in_x = list(expressions_not_in_x)
print("\nExpressions not in x:")
print(list(expressions_not_in_x)) # Convert back to a list for printing
if len(expressions_not_in_x)+len(x) == 4000: print(True)
print(len(expressions_not_in_x))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import PReLU

tf.random.set_seed(42)


x_train, x_val, y_train, y_val = \
    train_test_split(x, y, train_size=0.75)
x_train, x_val, y_train, y_val = \
    np.array(x_train), np.array(x_val), np.array(y_train), np.array(y_val)



model = keras.Sequential([
    keras.Input(shape=(len(x[0]),)),
    layers.Dense(30),
    PReLU(),
    layers.Dense(30),
    PReLU(),
    layers.Dense(1,  activation = "linear")
])

model.compile(optimizer = "adam", loss = "mse", metrics = ["mae", "mse"])

early_stopping = keras.callbacks.EarlyStopping(
    patience=5,
    min_delta=0.001,
    restore_best_weights=True,
    monitor='mse'
)
# Convert numpy arrays to TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) # Use a batch size, e.g., 32
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32) # Use the same batch size


history = model.fit(
    train_dataset, # Pass the TensorFlow Dataset
    validation_data=val_dataset, # Pass the TensorFlow Dataset
    epochs=200,
    callbacks=[early_stopping],
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['mse', 'val_mse']].plot()
plt.xlabel("Epoch")
plt.ylabel("Mean Squared Error")
plt.title("Training and Validation MSE per Epoch")
plt.show()

model.predict(tokenizer(["1 + 1 + 1"]))

y_test = []
for i in expressions_not_in_x:
  result = float(eval(i))
  y_test.append(result)
x_test = tokenizer(expressions_not_in_x)
diff_x_test = posTokenizer(expressions_not_in_x)
diff_x_test = np.array(diff_x_test)
print(diff_x_test[0])
print(y_test[0])
x_test = np.array(x_test)
store = model.predict(x_test)
print(len(store))
differences = []
for i in range(len(store)):
  differences.append(abs(store[i] - y_test[i]))
print(np.mean(differences))
plt.figure(figsize=(10, 6))
plt.plot(differences)
plt.axhline(y = np.mean(differences), color='r', linestyle='-', label='Mean Difference') # Plot a horizontal line at the mean difference
plt.xlabel("Test Sample Index")
plt.ylabel("Absolute Difference (Predicted - Actual)")
plt.title("Absolute Differences Between Predicted and Actual Values for Test Set")
plt.show()

#a list of expressions outside the number range
outsideExpr = set()
range1= range(5, 9)
range2= range(-8, -4)
comboRange = list(range1) + list(range2)
for num1 in comboRange:
    for num2 in comboRange:
        for num3 in comboRange:
            for op1 in [" + ", " - "]:
                for op2 in [" + ", " - "]:
                    expression = str(num1) + op1 + str(num2) + op2 + str(num3)
                    outsideExpr.add(expression)

outsideExpr = list(outsideExpr)
print(len(tokenizer(outsideExpr)[0]))

out_x_test = tokenizer(outsideExpr)
diff_out_x_test = posTokenizer(outsideExpr)
out_y_test = []
for i in outsideExpr:
  result = float(eval(i))
  out_y_test.append(result)

minNums = 2
maxNums = 8
amountNums = 100
num_terms = np.arange(minNums-1,maxNums)
longer_Exps = []
for j in num_terms:
  for p in range(amountNums):
    longer_Exp = ""
    for i in range(j):
      longer_Exp += str(np.random.randint(-5,6))
      longer_Exp += np.random.choice([" + ", " - "])
    longer_Exp += str(np.random.randint(-5,6))
    longer_Exps.append(longer_Exp)
print(longer_Exps)

longer_Exps = list(longer_Exps)
long_x_test = tokenizer(longer_Exps)
diff_long_x_test = posTokenizer(longer_Exps)
long_y_test = []
for i in longer_Exps:
  result = float(eval(i))
  long_y_test.append(result)
print(long_y_test[0])
print(long_x_test[0])

# prompt: bootstrap the model 10 times and calculate a range for predictions

n_bootstrap = 10
predictions = []
out_predictions = []
long_predictions = []

for _ in range(n_bootstrap):
  # Create bootstrap sample
  sample_indices = np.random.choice(len(x_train), size=len(x_train), replace=True)
  x_train_bootstrap = x_train[sample_indices]
  y_train_bootstrap = np.array(y_train)[sample_indices]

  # Re-compile and train the model
  bootstrap_model = keras.models.clone_model(model)
  bootstrap_model.compile(optimizer="adam", loss="mse", metrics=["mae", "mse"])

  bootstrap_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_bootstrap, y_train_bootstrap)).batch(32)
  bootstrap_model.fit(
      bootstrap_train_dataset,
      epochs=50,  # Reduced epochs for quicker example, adjust as needed
      verbose=0, # Suppress output
      callbacks=[early_stopping]
  )

  # Make predictions on the test set
  predictions.append(bootstrap_model.predict(x_test))
  out_predictions.append(bootstrap_model.predict(out_x_test))
  long_predictions.append(bootstrap_model.predict(long_x_test))

# Convert predictions to a numpy array for easier processing
predictions = np.array(predictions)
out_predictions = np.array(out_predictions)
long_predictions = np.array(long_predictions)

#calculate the mean thingy
meanPredictions = []
for i in range(len(expressions_not_in_x)):
  calc = 0
  for j in range(n_bootstrap):
    calc += predictions[j][i]
  meanPredictions.append(calc/n_bootstrap)


# Calculate the lower and upper bounds of the prediction range
lower_bound = np.percentile(predictions, 5, axis=0)
upper_bound = np.percentile(predictions, 95, axis=0)

inRange = []
outOfBounds = 0
for i in range(len(x_test)):
  if y_test[i] >= lower_bound[i][0] and y_test[i] <= upper_bound[i][0]:
    inRange.append(True)
  else:
    inRange.append(False)
    outOfBounds += 1


print("Prediction Range (5th to 95th percentile):")
for i in range(len(x_test)):
  print(f"Input: {expressions_not_in_x[i]}, Actual: {y_test[i]:.2f}, Predicted Range: [{lower_bound[i][0]:.2f}, {upper_bound[i][0]:.2f}], bootstrapped prediction: {meanPredictions[i]}, in Range: {inRange[i]}")

print(f"Amount out of bounds: {outOfBounds}, Amount of expressions: {len(x_test)}, Percentage in bounds: {(1-(outOfBounds/len(x_test)))*100}%")

print(f"Amount out of bounds: {outOfBounds}, Amount of expressions: {len(x_test)}, Percentage in bounds: {(1-(outOfBounds/len(x_test)))*100}")

out_meanPredictions = []
for i in range(len(out_x_test)):
  calc = 0
  for j in range(n_bootstrap):
    calc += out_predictions[j][i]
  out_meanPredictions.append(calc/n_bootstrap)


# Calculate the lower and upper bounds of the prediction range
out_lower_bound = np.percentile(out_predictions, 5, axis=0)
out_upper_bound = np.percentile(out_predictions, 95, axis=0)


out_inRange = []
out_outOfBounds = 0
for i in range(len(out_y_test)):
  if out_y_test[i] >= out_lower_bound[i][0] and out_y_test[i] <= out_upper_bound[i][0]:
    out_inRange.append(True)
  else:
    out_inRange.append(False)
    out_outOfBounds += 1


print("Prediction Range (5th to 95th percentile):")
for i in range(len(out_x_test)):
  print(f"Input: {outsideExpr[i]}, Actual: {out_y_test[i]:.2f}, Predicted Range: [{out_lower_bound[i][0]:.2f}, {out_upper_bound[i][0]:.2f}], bootstrapped prediction: {out_meanPredictions[i]}, in Range: {out_inRange[i]}")

print(f"Amount out of bounds: {out_outOfBounds}, Amount of expressions: {len(out_x_test)}, Percentage in bounds: {(1-(out_outOfBounds/len(out_x_test)))*100}%")

long_meanPredictions = []
for i in range(len(long_x_test)):
  calc = 0
  for j in range(n_bootstrap):
    calc += long_predictions[j][i]
  long_meanPredictions.append(calc/n_bootstrap)


# Calculate the lower and upper bounds of the prediction range
long_lower_bound = np.percentile(long_predictions, 5, axis=0)
long_upper_bound = np.percentile(long_predictions, 95, axis=0)


long_inRange = []
long_outOfBounds = 0
for i in range(len(long_y_test)):
  if long_y_test[i] >= long_lower_bound[i][0] and long_y_test[i] <= long_upper_bound[i][0]:
    long_inRange.append(True)
  else:
    long_inRange.append(False)
    long_outOfBounds += 1


print("Prediction Range (5th to 95th percentile):")
for i in range(len(long_x_test)):
  print(f"Input: {longer_Exps[i]}, Actual: {long_y_test[i]:.2f}, Predicted Range: [{long_lower_bound[i][0]:.2f}, {long_upper_bound[i][0]:.2f}], bootstrapped prediction: {long_meanPredictions[i]}, in Range: {long_inRange[i]}")

print(f"Amount out of bounds: {long_outOfBounds}, Amount of expressions: {len(long_x_test)}, Percentage in bounds: {(1-(long_outOfBounds/len(long_x_test)))*100}%")

# Visualize the predictions and the range for a few samples
plt.figure(figsize=(10, 6))
# Calculate the positive error values for the lower and upper bounds
yerr_lower = abs(y_test[:11] - lower_bound[:11, 0])
yerr_upper = abs(upper_bound[:11, 0] - y_test[:11])
plt.errorbar(range(len(y_test[:11])), y_test[:11], yerr=[yerr_lower, yerr_upper], fmt='o', label='Actual with 90% Prediction Range')
plt.xlabel("Test Sample Index")
plt.ylabel("Value")
plt.title("Actual Values and 90% Prediction Range for Test Set (Bootstrap)")
plt.legend()
plt.show()

def absSum(x):
  expressions = x
  p = []
  for i in range(len(expressions)):
    components = []
    components = expressions[i].split(" ")
    p.append(abs(int(components[0])) + abs(int(components[2])) + abs(int(components[4])))
  return p
print(absSum(expressions_not_in_x))

deviation = []
relativeError = []
out_deviation = []
out_relativeError = []

for i in range(len(y_test)):
  deviation.append(abs(y_test[i]-meanPredictions[i]))
print(deviation)
for i in range(len(y_test)):
  if y_test[i] != 0:
    relativeError.append(deviation[i]/abs(y_test[i]))
  else:
    relativeError.append(deviation[i])
print(relativeError)
avDeviation = np.mean(deviation)
print(np.mean(deviation))

for i in range(len(out_y_test)):
  out_deviation.append(abs(out_y_test[i]-out_meanPredictions[i]))
print(out_deviation)
out_avDeviation = np.mean(out_deviation)
print(np.mean(out_deviation))

for i in range(len(out_y_test)):
  if out_y_test[i] != 0:
    out_relativeError.append(out_deviation[i]/abs(out_y_test[i]))
  else:
    out_relativeError.append(out_deviation[i])

import seaborn as sns

paired_data = sorted(zip(absSum(expressions_not_in_x), deviation))

# Unpack the sorted paired_data into separate lists for plotting
sorted_abs_sum = [item[0] for item in paired_data]
sorted_deviation = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_deviation, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")

x_values = np.arange(1,16)
y_values = np.full_like(x_values, avDeviation, dtype=np.float32)
plt.plot(x_values, y_values, color='green', linestyle='--', label='hypothesis')

plt.xlabel("Absolute Sum of Components (absSum)")
plt.ylabel("error")
plt.title("Deviation vs. Absolute Sum of Components for Test Set")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%", "hypothesis"])
plt.show()

paired_data = sorted(zip(absSum(expressions_not_in_x), relativeError))

# Separate the sorted absSum and relativeError values
sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")

x_values = np.arange(1,16)
y_values = np.full_like(x_values, np.mean(relativeError), dtype=np.float32)
plt.plot(x_values, y_values, color='green', linestyle='--', label='hypothesis')

print(np.mean(relativeError))
plt.xlabel("Absolute Sum of Components (absSum)")
plt.ylabel("Relative Error") # Changed label to reflect relativeError
plt.title("Relative Error vs. Absolute Sum of Components for Test Set")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%", "hypothesis"])
plt.show()

paired_data = sorted(zip(y_test, deviation))

# Separate the sorted absSum and relativeError values
sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")


plt.xlabel("actual value of the expression")
plt.ylabel("error") # Changed label to reflect relativeError
plt.title("error vs. actual value of the expressions")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%"])
plt.show()

paired_data = sorted(zip(y_test, relativeError))

# Separate the sorted absSum and relativeError values
sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")

print(np.mean(relativeError))
plt.xlabel("actual value of the expression")
plt.ylabel("Relative Error") # Changed label to reflect relativeError
plt.title("Relative Error vs. actual value of the expressions")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%"])
plt.show()

most_Expr =outsideExpr + expressions_not_in_x
most_deviation = out_deviation + deviation
most_absSum = absSum(outsideExpr) + absSum(expressions_not_in_x)

paired_data = sorted(zip(most_absSum, most_deviation))

# Separate the sorted absSum and relativeError values
sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")


# Define the x-coordinate for the vertical line
vertical_line_x = 15

# Define the y-range for the vertical line (adjust as needed)
# You can use the range of your data's y-values or a specific range
# For example, to cover the range of your plotted data:
min_y = min(plt.gca().get_ylim()) # Get the current y-axis limits
max_y = max(plt.gca().get_ylim())
vertical_line_y = np.linspace(min_y, max_y, 100) # Create a range of y values

plt.plot(np.full_like(vertical_line_y, vertical_line_x), vertical_line_y, color='black', linestyle='--', label='training range limit')


plt.xlabel("Absolute Sum of Components (absSum)")
plt.ylabel("Deviation (abs(Actual - Mean Prediction))")
plt.title("Deviation vs. Absolute Sum of Components for Test Set")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%", "training range limit"])
plt.show()

most_Expr =outsideExpr + expressions_not_in_x
most_deviation = out_deviation + deviation
most_y = out_y_test + y_test

paired_data = sorted(zip(most_y, most_deviation))

sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")

plt.xlabel("Actual y values")
plt.ylabel("error")
plt.title("Deviation vs. y test")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%"])
plt.show()

most_Expr =outsideExpr + expressions_not_in_x
most_deviation = out_relativeError + relativeError
most_y = out_y_test + y_test
avRelError = np.mean(most_deviation)

print(avRelError)
paired_data = sorted(zip(most_y, most_deviation))

sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")

plt.xlabel("Actual y values")
plt.ylabel("relative Errors")
plt.title("relative Errors vs. y test")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%"])
plt.show()

most_Expr =outsideExpr + expressions_not_in_x
most_deviation = out_relativeError + relativeError
most_absSum = absSum(outsideExpr) + absSum(expressions_not_in_x)

paired_data = sorted(zip(most_absSum, most_deviation))

sorted_abs_sum = [item[0] for item in paired_data]
sorted_relative_error = [item[1] for item in paired_data]

plt.figure(figsize=(10, 6))
# Use the unpacked lists for regplot
sns.regplot(x=sorted_abs_sum, y=sorted_relative_error, order = 5, x_jitter=0.1, color = "red", scatter_kws ={"color": None}, label="data points")


# Define the x-coordinate for the vertical line
vertical_line_x = 15

# Define the y-range for the vertical line (adjust as needed)
# You can use the range of your data's y-values or a specific range
# For example, to cover the range of your plotted data:
min_y = min(plt.gca().get_ylim()) # Get the current y-axis limits
max_y = max(plt.gca().get_ylim())
vertical_line_y = np.linspace(min_y, max_y, 100) # Create a range of y values

plt.plot(np.full_like(vertical_line_y, vertical_line_x), vertical_line_y, color='black', linestyle='--', label='training range limit')


plt.xlabel("Absolute Sum of Components (absSum)")
plt.ylabel("relative Errors")
plt.title("relative Errors vs. Absolute Sum of Components for Test Set")
plt.legend(["data points", "polynomial regression with order 5", "confidence interval 95%", "training range limit"])
plt.show()

long_deviation = []
for i in range(len(long_meanPredictions)): long_deviation.append(abs(long_meanPredictions[i] - long_y_test[i]))
print(len(long_deviation))
print(np.mean(long_deviation))

repeated_integers = []
for i in range(minNums, maxNums + 1):
  repeated_integers.extend([i] * amountNums)
print(len(long_deviation))
repeated_integers = np.array(repeated_integers)

long_coef = np.polyfit(repeated_integers, long_deviation, 5)


plt.figure(figsize=(10, 6))
plt.scatter(repeated_integers, long_deviation,  label = "data points")
plt.plot(repeated_integers, np.polyval(long_coef, repeated_integers), color='red', label=("trend-line (fitted polynomial)"))
plt.xlabel("Number of Terms")
plt.ylabel("Deviation (abs(Actual - Mean Prediction))")
plt.title("Deviation vs. Number of Terms for Test Set")
plt.legend()
plt.show()

long_deviations = np.array(long_deviation)
long_deviations = long_deviations.reshape(maxNums-minNums+1,amountNums)
long_meanDeviations = []
for i in range(len(long_deviations)):
  long_meanDeviations.append(np.mean(long_deviations[i]))
print(long_meanDeviations)
print(out_deviation)
placeholder = absSum(outsideExpr)
print(placeholder)
paired = []
for i in range(len(out_deviation)):
  paired.append([out_deviation[i], placeholder[i]])
print(len(paired))
paired_sorted = sorted(paired, key=lambda item: item[1])
print(paired_sorted)

from collections import defaultdict

# Group deviations by placeholder value
grouped_deviations = defaultdict(list)
for deviation, placeholder_value in paired_sorted:
    grouped_deviations[placeholder_value].append(deviation)

# Calculate the mean deviation for each placeholder value
mean_deviations_by_placeholder = {}
for placeholder_value, deviations in grouped_deviations.items():
    mean_deviations_by_placeholder[placeholder_value] = np.mean(deviations)

print("Mean deviation for each placeholder value:")
for placeholder_value, mean_deviation in mean_deviations_by_placeholder.items():
    print(f"Placeholder value {placeholder_value}: {mean_deviation}")


print((avDeviation)**2)
print((long_meanDeviations[2])**2)
print((mean_deviations_by_placeholder[22])**2)


baseline_deviation = (avDeviation)**2
baeline_out_deviation = (mean_deviations_by_placeholder[22])**2
baseline_long_deviation = (long_meanDeviations[2])**2
baseline_relError = (avRelError)**2

if __name__ == '__main__':
    print("This runs only when my_module.py is executed directly.")
    
    model_config = model.get_config()
    print(model_config)
    import pprint
    pprint.pprint(model_config)
    
    base_layers = model_config['layers']
    input_layer = base_layers[0]
    output_layer = base_layers[-1]
    hidden_layer = [base_layers[1], base_layers[2]]
    base_layers[1]
    
    def build_model(input_shape, num_hidden_layers, units_per_layer):
      lil_model = keras.Sequential()
      lil_model.add(keras.Input(shape=input_shape))
      lil_model.add(layers.Flatten())
      for i in range(num_hidden_layers):
        lil_model.add(layers.Dense(units_per_layer, kernel_regularizer=regularizers.l2(0.01))),
        lil_model.add(PReLU())
      lil_model.add(layers.Dense(1, activation='linear'))
      lil_model.compile(optimizer="adam", loss="mse", metrics=["mae", "mse"])
      return lil_model
    
    diff_x_train, diff_x_val, diff_y_train, diff_y_val = \
        train_test_split(diff_x, diff_y, train_size=0.75)
    diff_x_train, diff_x_val, diff_y_train, diff_y_val = \
        np.array(diff_x_train), np.array(diff_x_val), np.array(diff_y_train), np.array(diff_y_val)
    
    different_models = []
    max_layers = 5
    min_Neurons = 5
    max_Neurons = 55
    small_n_bootstrap = 5
    input_shape = diff_x[0].shape
    Neuron_step = 5
    
    print(input_shape)
    
    
    diff_predictions = []
    diff_out_predictions = []
    diff_long_predictions = []
    
    counter = 0
    max_counter = ((max_Neurons-min_Neurons)/Neuron_step + 1)*(max_layers+1)*small_n_bootstrap
    for i in range(max_layers+1):
      for j in range(min_Neurons, max_Neurons+1,5):
        for p in range(small_n_bootstrap):
          sample_indices = np.random.choice(len(diff_x_train), size=len(diff_x_train), replace=True)
          x_train_bootstrap = diff_x_train[sample_indices]
          y_train_bootstrap = np.array(diff_y_train)[sample_indices]
          bootstrap_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_bootstrap, y_train_bootstrap)).batch(32)
    
          this_model = build_model(input_shape, i, j)
          different_models.append(this_model)
    
          this_model.fit(
              bootstrap_train_dataset,
              epochs=50,  # Reduced epochs for quicker example, adjust as needed
              verbose=0, # Suppress output
              callbacks=[early_stopping]
          )
          diff_predictions.append(this_model.predict(diff_x_test))
          diff_out_predictions.append(this_model.predict(diff_out_x_test))
          diff_long_predictions.append(this_model.predict(diff_long_x_test))
          counter += 1
          print(f"counter: {counter} / {max_counter}")
          print(f"percentage: {counter/max_counter*100:.2f}%")
    
    diff_predictions = np.array(diff_predictions)
    diff_out_predictions = np.array(diff_out_predictions)
    diff_long_predictions = np.array(diff_long_predictions)
    
    print(len(different_models))
    
    diff_predictions.shape
    
    diff_meanPredictions = []
    num_models = diff_predictions.shape[0] // small_n_bootstrap # 66
    print(num_models)
    num_test_samples = diff_predictions.shape[1] # 1457
    print(num_test_samples)
    
    for model_index in range(num_models):
      model_predictions = diff_predictions[model_index * small_n_bootstrap : (model_index + 1) * small_n_bootstrap]
      # model_predictions now contains the 5 prediction sets for this specific model
      mean_prediction_for_model = np.mean(model_predictions, axis=0) # Average across the bootstrap axis
      diff_meanPredictions.append(mean_prediction_for_model)
    
    diff_meanPredictions = np.array(diff_meanPredictions)
    # diff_meanPredictions should now have shape (66, 1457, 1)
    print(diff_meanPredictions.shape)
    
    diff_deviation = []
    #diff_out_deviation = []
    
    for j in range(diff_meanPredictions.shape[0]):
      calc = 0
      for i in range(len(y_test)):
        calc += abs(y_test[i]-diff_meanPredictions[j][i])
      diff_deviation.append(calc/len(y_test))
    print(diff_deviation)
    
    '''
    for i in range(len(out_y_test)):
      out_deviation.append(abs(out_y_test[i]-out_meanPredictions[i]))
    print(out_deviation)
    out_avDeviation = np.mean(out_deviation)
    print(np.mean(out_deviation))
    '''
    
    # prompt: plot diff_deviation on a graph
    
    plt.figure(figsize=(10, 6))
    plt.plot(diff_deviation)
    plt.xlabel("Model Index")
    plt.ylabel("Mean Absolute Deviation on Test Set")
    plt.title("Mean Absolute Deviation for Different Models")
    plt.show()
    
    diff_out_meanPredictions = []
    num_models = diff_out_predictions.shape[0] // small_n_bootstrap # 36
    num_test_samples = diff_out_predictions.shape[1] # 1457
    
    for model_index in range(num_models):
      model_predictions = diff_out_predictions[model_index * small_n_bootstrap : (model_index + 1) * small_n_bootstrap]
      # model_predictions now contains the 5 prediction sets for this specific model
      mean_prediction_for_model = np.mean(model_predictions, axis=0) # Average across the bootstrap axis
      diff_out_meanPredictions.append(mean_prediction_for_model)
    
    diff_out_meanPredictions = np.array(diff_out_meanPredictions)
    # diff_meanPredictions should now have shape (36, 1457)
    print(diff_out_meanPredictions.shape)
    
    diff_out_deviation = []
    #diff_out_deviation = []
    indices_with_placeholder_22 = [i for i, val in enumerate(placeholder) if val == 22]
    print(indices_with_placeholder_22)
    for j in range(diff_out_meanPredictions.shape[0]):
      calc = 0
      for i in indices_with_placeholder_22:
        calc += abs(out_y_test[i]-diff_out_meanPredictions[j][i])
      diff_out_deviation.append(calc/len(indices_with_placeholder_22))
    print(diff_out_deviation)
    
    plt.figure(figsize=(10, 6))
    plt.plot(diff_out_deviation[6:])
    plt.xlabel("Model Index")
    plt.ylabel("Mean Absolute Deviation for Placeholder 22")
    plt.title("Mean Absolute Deviation for Placeholder 22 for Different Models")
    plt.show()
    
    diff_out_relError = []
    for j in range(diff_out_meanPredictions.shape[0]):
      calc = 0
      for i in range(diff_out_meanPredictions.shape[1]):
        calc += abs((out_y_test[i]-diff_out_meanPredictions[j][i])/out_y_test[i])
      diff_out_relError.append(calc/diff_out_meanPredictions.shape[1])
    
    diff_relError = []
    for j in range(diff_meanPredictions.shape[0]):
      calc = 0
      for i in range(diff_meanPredictions.shape[1]):
        if y_test[i] == 0:
          calc += y_test[i]-diff_meanPredictions[j][i]
          continue
        calc += abs((y_test[i]-diff_meanPredictions[j][i])/y_test[i])
      diff_relError.append(calc/diff_meanPredictions.shape[1])
    
    diff_relErrors = (np.array(diff_out_relError) + np.array(diff_relError))/2.0
    diff_avRelError = np.mean(diff_relErrors)
    print(diff_avRelError)
    print(len(diff_relErrors))
    
    diff_long_meanPredictions = []
    num_models = diff_long_predictions.shape[0] // small_n_bootstrap # 36
    num_test_samples = diff_long_predictions.shape[1] # 1457
    
    for model_index in range(num_models):
      model_predictions = diff_long_predictions[model_index * small_n_bootstrap : (model_index + 1) * small_n_bootstrap]
      # model_predictions now contains the 5 prediction sets for this specific model
      mean_prediction_for_model = np.mean(model_predictions, axis=0) # Average across the bootstrap axis
      diff_long_meanPredictions.append(mean_prediction_for_model)
    
    diff_long_meanPredictions = np.array(diff_long_meanPredictions)
    # diff_meanPredictions should now have shape (36, 1457)
    print(diff_long_meanPredictions.shape)
    
    print(np.arange(100,200,1))
    diff_long_deviations = []
    
    for j in range(diff_long_meanPredictions.shape[0]):
      calc = 0
      for i in range(200,300):
        calc += abs(long_y_test[i]-diff_long_meanPredictions[j][i])
      diff_long_deviations.append(calc/100)
    print(len(diff_long_deviations))
    
    # prompt: plot of diff_long_deviations
    
    plt.figure(figsize=(10, 6))
    plt.plot(diff_long_deviations)
    plt.xlabel("Model Index")
    plt.ylabel("Mean Absolute Deviation for Expressions with 4 Terms")
    plt.title("Mean Absolute Deviation for Expressions with 4 Terms for Different Models")
    plt.show()
    
    import seaborn as sns
    num_neurons = (max_Neurons - min_Neurons) // Neuron_step + 1
    diff_deviation_reshaped = np.array(diff_deviation).reshape(max_layers + 1, num_neurons)
    plt.figure(figsize=(8, 6)) # Optional: Adjust figure size
    sns.heatmap(diff_deviation_reshaped, annot=True, fmt=".2f", cmap="plasma",
                xticklabels=np.arange(min_Neurons, max_Neurons + 1, Neuron_step),
                yticklabels=np.arange(max_layers + 1))
    plt.title('Simple Heatmap')
    plt.show()
    
    print((avDeviation)**2)
    print((long_meanDeviations[2])**2)
    print((mean_deviations_by_placeholder[22])**2)
    
    
    benchmark = []
    for i in range(len(diff_deviation)):
      calc = 0
      calc += baseline_deviation / (diff_deviation[i]**2)
      calc += baeline_out_deviation / (diff_out_deviation[i]**2)
      calc += baseline_long_deviation / (diff_long_deviations[i]**2)
      calc += baseline_relError / (diff_relErrors[i]**2)
      benchmark.append(calc/4)
    print(benchmark)
    
    
    benchmark_array = np.array(benchmark).reshape(max_layers + 1, num_neurons)
    
    # Create a new array to include the means
    # Add a column for row means and a row for column means (and a corner for the overall mean)
    extended_benchmark_array = np.zeros((max_layers + 2, num_neurons + 1))
    
    Neurons_means = np.mean(benchmark_array, axis=0)
    Layers_means = np.mean(benchmark_array, axis=1)
    
    # Copy the original benchmark data
    extended_benchmark_array[:max_layers + 1, :num_neurons] = benchmark_array
    
    # Add the row means
    extended_benchmark_array[:max_layers + 1, num_neurons] = Layers_means
    
    # Add the column means
    extended_benchmark_array[max_layers + 1, :num_neurons] = Neurons_means
    
    # Calculate and add the overall mean
    overall_mean = np.mean(benchmark_array)
    extended_benchmark_array[max_layers + 1, num_neurons] = overall_mean
    
    
    # Create updated tick labels for the heatmap
    xticklabels_extended = list(np.arange(min_Neurons, max_Neurons + 1, Neuron_step)) + ['Row Mean']
    yticklabels_extended = list(np.arange(max_layers + 1)) + ['Column Mean']
    
    
    plt.figure(figsize=(14, 10)) # Adjust figure size for the extra row/column
    ax = sns.heatmap(extended_benchmark_array, annot=True, fmt=".2f", cmap="inferno",
                xticklabels=xticklabels_extended,
                yticklabels=yticklabels_extended)
    ymin, ymax = ax.get_ylim()
    xmin, xmax = ax.get_xlim()
    ax.hlines(y=6, xmin=xmin, xmax=xmax, colors='lightblue', lw=2, linestyle='-')
    ax.vlines(x=11, ymin=ymin, ymax=ymax, colors='lightblue', lw=2, linestyle='-')
    plt.ylabel('Number of Layers')
    plt.xlabel('Number of Neurons per Layer')
    plt.title('Heatmap of the Benchmarks with Row and Column Means')
    plt.savefig("bigHeatmap.png")
    plt.show()
    
