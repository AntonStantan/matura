# -*- coding: utf-8 -*-
"""TheFirst.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sek7rICQu7h8A-vxXGQ-8zIXsaD1NQKo
"""

import numpy as np
np.random.seed(42)

x1 = np.random.rand(4000)*10
x2 = np.random.rand(4000)*10
x3 = np.random.rand(4000)*10

x1_int = x1.astype(int) - 5
x2_int = x2.astype(int) - 5
x3_int = x3.astype(int) - 5

x1_str = x1_int.astype(str)
x2_str = x2_int.astype(str)
x3_str = x3_int.astype(str)

unique_expressions = set()

for i in range(len(x1)):
  n = np.random.rand(1)
  if n < 0.25:
    opp1 = " + "
    opp2 = " + "
  elif n > 0.25 and n < 0.5:
    opp1 = " + "
    opp2 = " - "
  elif n > 0.75:
    opp1 = " - "
    opp2 = " + "
  else:
    opp1 = " - "
    opp2 = " - "
  unique_expressions.add(x1_str[i] + opp1 + x2_str[i] + opp2 + x3_str[i])

x = list(unique_expressions) # Convert the set back to a list
print(x)
print(len(x))

y = []

for expression in x:
  result = float(eval(expression))
  y.append(result)

print(y)

def tokenizer(input_list): # Changed parameter name to avoid confusion with global x
  #tokenizer by hand
  #tokens = (len(input_list), 5)
  # Create a copy of the input list to avoid modifying the original
  tokenized_x = [expression.split(" ") for expression in input_list]


  for i in range(len(tokenized_x)):
    for j in range(len(tokenized_x[i])):
      if j % 2 == 0:  # Check if the index is even
        tokenized_x[i][j] = np.float32(tokenized_x[i][j])
      else:  # The index is uneven, it's an operator
        if tokenized_x[i][j] == "+":
          tokenized_x[i][j] = np.float32(1)
        else:
          tokenized_x[i][j] = np.float32(0)
    padding_count = 15 - len(tokenized_x[i])
    for _ in range(padding_count): # Use a throwaway variable
      tokenized_x[i].append(np.float32(0.5))
  tokenized_x = np.array(tokenized_x)
  return tokenized_x
x = tokenizer(x)

# Generate all possible expressions
all_possible_expressions = set()
for num1 in range(-5, 5): # Range -5 to 4
    for num2 in range(-5, 5): # Range -5 to 4
        for num3 in range(-5, 5): # Range -5 to 4
            for op1 in [" + ", " - "]:
                for op2 in [" + ", " - "]:
                    expression = str(num1) + op1 + str(num2) + op2 + str(num3)
                    all_possible_expressions.add(expression)

# Find expressions not in x
expressions_not_in_x = all_possible_expressions - unique_expressions
expressions_not_in_x = list(expressions_not_in_x)
print("\nExpressions not in x:")
print(list(expressions_not_in_x)) # Convert back to a list for printing
if len(expressions_not_in_x)+len(x) == 4000: print(True)
print(len(expressions_not_in_x))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import PReLU

tf.random.set_seed(42)


x_train, x_val, y_train, y_val = \
    train_test_split(x, y, train_size=0.75)
x_train, x_val, y_train, y_val = \
    np.array(x_train), np.array(x_val), np.array(y_train), np.array(y_val)



model = keras.Sequential([
    keras.Input(shape=(len(x[0]),)),
    layers.Dense(32),
    PReLU(),
    layers.Dense(32),
    PReLU(),
    layers.Dense(1,  activation = "linear")
])

model.compile(optimizer = "adam", loss = "mse", metrics = ["mae", "mse"])

early_stopping = keras.callbacks.EarlyStopping(
    patience=5,
    min_delta=0.001,
    restore_best_weights=True,
    monitor='mse'
)
# Convert numpy arrays to TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) # Use a batch size, e.g., 32
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32) # Use the same batch size


history = model.fit(
    train_dataset, # Pass the TensorFlow Dataset
    validation_data=val_dataset, # Pass the TensorFlow Dataset
    epochs=200,
    callbacks=[early_stopping],
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['mse', 'val_mse']].plot()
plt.xlabel("Epoch")
plt.ylabel("Mean Squared Error")
plt.title("Training and Validation MSE per Epoch")
plt.show()

model.predict(tokenizer(["1 + 1 + 1"]))

y_test = []
for i in expressions_not_in_x:
  result = float(eval(i))
  y_test.append(result)
x_test = tokenizer(expressions_not_in_x)
print(y_test[0])
x_test = np.array(x_test)
store = model.predict(x_test)
print(len(store))
differences = []
for i in range(len(store)):
  differences.append(abs(store[i] - y_test[i]))
print(np.mean(differences))
plt.figure(figsize=(10, 6))
plt.plot(differences)
plt.axhline(y = np.mean(differences), color='r', linestyle='-', label='Mean Difference') # Plot a horizontal line at the mean difference
plt.xlabel("Test Sample Index")
plt.ylabel("Absolute Difference (Predicted - Actual)")
plt.title("Absolute Differences Between Predicted and Actual Values for Test Set")
plt.show()

#a list of expressions outside the number range
outsideExpr = set()
range1= range(5, 9)
range2= range(-8, -4)
comboRange = list(range1) + list(range2)
for num1 in comboRange:
    for num2 in comboRange:
        for num3 in comboRange:
            for op1 in [" + ", " - "]:
                for op2 in [" + ", " - "]:
                    expression = str(num1) + op1 + str(num2) + op2 + str(num3)
                    outsideExpr.add(expression)

outsideExpr = list(outsideExpr)
print(len(tokenizer(outsideExpr)[0]))

out_x_test = tokenizer(outsideExpr)
out_y_test = []
for i in outsideExpr:
  result = float(eval(i))
  out_y_test.append(result)

minNums = 2
maxNums = 8
amountNums = 100
num_terms = np.arange(minNums-1,maxNums)
longer_Exps = []
for j in num_terms:
  for p in range(amountNums):
    longer_Exp = ""
    for i in range(j):
      longer_Exp += str(np.random.randint(-5,6))
      longer_Exp += np.random.choice([" + ", " - "])
    longer_Exp += str(np.random.randint(-5,6))
    longer_Exps.append(longer_Exp)
print(longer_Exps)

longer_Exps = list(longer_Exps)
long_x_test = tokenizer(longer_Exps)
long_y_test = []
for i in longer_Exps:
  result = float(eval(i))
  long_y_test.append(result)
print(long_y_test[0])
print(long_x_test[0])

# prompt: bootstrap the model 10 times and calculate a range for predictions

n_bootstrap = 10
predictions = []
out_predictions = []
long_predictions = []

for _ in range(n_bootstrap):
  # Create bootstrap sample
  sample_indices = np.random.choice(len(x_train), size=len(x_train), replace=True)
  x_train_bootstrap = x_train[sample_indices]
  y_train_bootstrap = np.array(y_train)[sample_indices]

  # Re-compile and train the model
  bootstrap_model = keras.models.clone_model(model)
  bootstrap_model.compile(optimizer="adam", loss="mse", metrics=["mae", "mse"])

  bootstrap_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_bootstrap, y_train_bootstrap)).batch(32)
  bootstrap_model.fit(
      bootstrap_train_dataset,
      epochs=50,  # Reduced epochs for quicker example, adjust as needed
      verbose=0, # Suppress output
      callbacks=[early_stopping]
  )

  # Make predictions on the test set
  predictions.append(bootstrap_model.predict(x_test))
  out_predictions.append(bootstrap_model.predict(out_x_test))
  long_predictions.append(bootstrap_model.predict(long_x_test))

# Convert predictions to a numpy array for easier processing
predictions = np.array(predictions)
out_predictions = np.array(out_predictions)
long_predictions = np.array(long_predictions)

#calculate the mean thingy
meanPredictions = []
for i in range(len(expressions_not_in_x)):
  calc = 0
  for j in range(n_bootstrap):
    calc += predictions[j][i]
  meanPredictions.append(calc/n_bootstrap)


# Calculate the lower and upper bounds of the prediction range
lower_bound = np.percentile(predictions, 5, axis=0)
upper_bound = np.percentile(predictions, 95, axis=0)

inRange = []
outOfBounds = 0
for i in range(len(x_test)):
  if y_test[i] >= lower_bound[i][0] and y_test[i] <= upper_bound[i][0]:
    inRange.append(True)
  else:
    inRange.append(False)
    outOfBounds += 1


print("Prediction Range (5th to 95th percentile):")
for i in range(len(x_test)):
  print(f"Input: {expressions_not_in_x[i]}, Actual: {y_test[i]:.2f}, Predicted Range: [{lower_bound[i][0]:.2f}, {upper_bound[i][0]:.2f}], bootstrapped prediction: {meanPredictions[i]}, in Range: {inRange[i]}")

print(f"Amount out of bounds: {outOfBounds}, Amount of expressions: {len(x_test)}, Percentage in bounds: {(1-(outOfBounds/len(x_test)))*100}%")

print(f"Amount out of bounds: {outOfBounds}, Amount of expressions: {len(x_test)}, Percentage in bounds: {(1-(outOfBounds/len(x_test)))*100}")

out_meanPredictions = []
for i in range(len(out_x_test)):
  calc = 0
  for j in range(n_bootstrap):
    calc += out_predictions[j][i]
  out_meanPredictions.append(calc/n_bootstrap)


# Calculate the lower and upper bounds of the prediction range
out_lower_bound = np.percentile(out_predictions, 5, axis=0)
out_upper_bound = np.percentile(out_predictions, 95, axis=0)


out_inRange = []
out_outOfBounds = 0
for i in range(len(out_y_test)):
  if out_y_test[i] >= out_lower_bound[i][0] and out_y_test[i] <= out_upper_bound[i][0]:
    out_inRange.append(True)
  else:
    out_inRange.append(False)
    out_outOfBounds += 1


print("Prediction Range (5th to 95th percentile):")
for i in range(len(out_x_test)):
  print(f"Input: {outsideExpr[i]}, Actual: {out_y_test[i]:.2f}, Predicted Range: [{out_lower_bound[i][0]:.2f}, {out_upper_bound[i][0]:.2f}], bootstrapped prediction: {out_meanPredictions[i]}, in Range: {out_inRange[i]}")

print(f"Amount out of bounds: {out_outOfBounds}, Amount of expressions: {len(out_x_test)}, Percentage in bounds: {(1-(out_outOfBounds/len(out_x_test)))*100}%")

long_meanPredictions = []
for i in range(len(long_x_test)):
  calc = 0
  for j in range(n_bootstrap):
    calc += long_predictions[j][i]
  long_meanPredictions.append(calc/n_bootstrap)


# Calculate the lower and upper bounds of the prediction range
long_lower_bound = np.percentile(long_predictions, 5, axis=0)
long_upper_bound = np.percentile(long_predictions, 95, axis=0)


long_inRange = []
long_outOfBounds = 0
for i in range(len(long_y_test)):
  if long_y_test[i] >= long_lower_bound[i][0] and long_y_test[i] <= long_upper_bound[i][0]:
    long_inRange.append(True)
  else:
    long_inRange.append(False)
    long_outOfBounds += 1


print("Prediction Range (5th to 95th percentile):")
for i in range(len(long_x_test)):
  print(f"Input: {longer_Exps[i]}, Actual: {long_y_test[i]:.2f}, Predicted Range: [{long_lower_bound[i][0]:.2f}, {long_upper_bound[i][0]:.2f}], bootstrapped prediction: {long_meanPredictions[i]}, in Range: {long_inRange[i]}")

print(f"Amount out of bounds: {long_outOfBounds}, Amount of expressions: {len(long_x_test)}, Percentage in bounds: {(1-(long_outOfBounds/len(long_x_test)))*100}%")

# Visualize the predictions and the range for a few samples
plt.figure(figsize=(10, 6))
# Calculate the positive error values for the lower and upper bounds
yerr_lower = abs(y_test[:11] - lower_bound[:11, 0])
yerr_upper = abs(upper_bound[:11, 0] - y_test[:11])
plt.errorbar(range(len(y_test[:11])), y_test[:11], yerr=[yerr_lower, yerr_upper], fmt='o', label='Actual with 90% Prediction Range')
plt.xlabel("Test Sample Index")
plt.ylabel("Value")
plt.title("Actual Values and 90% Prediction Range for Test Set (Bootstrap)")
plt.legend()
plt.show()

def absSum(x):
  expressions = x
  p = []
  for i in range(len(expressions)):
    components = []
    components = expressions[i].split(" ")
    p.append(abs(int(components[0])) + abs(int(components[2])) + abs(int(components[4])))
  return p
print(absSum(expressions_not_in_x))

deviation = []
out_deviation = []

for i in range(len(y_test)):
  deviation.append(abs(y_test[i]-meanPredictions[i]))
print(deviation)
avDeviation = np.mean(deviation)
print(np.mean(deviation))

for i in range(len(out_y_test)):
  out_deviation.append(abs(out_y_test[i]-out_meanPredictions[i]))
print(out_deviation)
out_avDeviation = np.mean(out_deviation)
print(np.mean(out_deviation))

coef = np.polyfit(absSum(expressions_not_in_x), deviation, 5)


plt.figure(figsize=(10, 6))
plt.scatter(absSum(expressions_not_in_x), deviation,  label = "data points")
sorted_abs_sum = sorted(absSum(expressions_not_in_x))
plt.plot(sorted_abs_sum, np.polyval(coef, sorted_abs_sum), color='red', label=("trend-line (fitted polynomial)"))

x_values = np.arange(1,16)
y_values = np.full_like(x_values, avDeviation, dtype=np.float32)
plt.plot(x_values, y_values, color='green', linestyle='--', label='hypothesis')


plt.xlabel("Absolute Sum of Components (absSum)")
plt.ylabel("Deviation (abs(Actual - Mean Prediction))")
plt.title("Deviation vs. Absolute Sum of Components for Test Set")
plt.legend()
plt.show()

most_Expr =outsideExpr + expressions_not_in_x
most_deviation = out_deviation + deviation
most_absSum = absSum(outsideExpr) + absSum(expressions_not_in_x)


out_coef = np.polyfit(most_absSum, most_deviation, 5)

plt.figure(figsize=(10, 6))
plt.scatter(most_absSum, most_deviation,  label = "data points")
out_sorted = sorted(most_absSum)
plt.plot(out_sorted, np.polyval(out_coef, out_sorted), color='red', label=("trend-line (fitted polynomial)"))

x_for_vertical_line = np.full_like(y_values, 15, dtype=np.float32)

plt.plot(x_for_vertical_line, y_values, color='black', linestyle='--', label='training range limit')


plt.xlabel("Absolute Sum of Components (absSum)")
plt.ylabel("Deviation (abs(Actual - Mean Prediction))")
plt.title("Deviation vs. Absolute Sum of Components for Test Set")
plt.legend()
plt.show()

long_deviation = []
for i in range(len(long_meanPredictions)): long_deviation.append(abs(long_meanPredictions[i] - long_y_test[i]))
print(len(long_deviation))
print(np.mean(long_deviation))

repeated_integers = []
for i in range(minNums, maxNums + 1):
  repeated_integers.extend([i] * amountNums)
print(len(long_deviation))
repeated_integers = np.array(repeated_integers)

long_coef = np.polyfit(repeated_integers, long_deviation, 5)


plt.figure(figsize=(10, 6))
plt.scatter(repeated_integers, long_deviation,  label = "data points")
plt.plot(repeated_integers, np.polyval(long_coef, repeated_integers), color='red', label=("trend-line (fitted polynomial)"))
plt.xlabel("Number of Terms")
plt.ylabel("Deviation (abs(Actual - Mean Prediction))")
plt.title("Deviation vs. Number of Terms for Test Set")
plt.legend()
plt.show()

long_deviations = np.array(long_deviation)
long_deviations = long_deviations.reshape(maxNums-minNums+1,amountNums)
long_meanDeviations = []
for i in range(len(long_deviations)):
  long_meanDeviations.append(np.mean(long_deviations[i]))
print(long_meanDeviations)
print(out_deviation)
placeholder = absSum(outsideExpr)
print(placeholder)
paired = []
for i in range(len(out_deviation)):
  paired.append([out_deviation[i], placeholder[i]])
print(len(paired))
paired_sorted = sorted(paired, key=lambda item: item[1])
print(paired_sorted)

from collections import defaultdict

# Group deviations by placeholder value
grouped_deviations = defaultdict(list)
for deviation, placeholder_value in paired_sorted:
    grouped_deviations[placeholder_value].append(deviation)

# Calculate the mean deviation for each placeholder value
mean_deviations_by_placeholder = {}
for placeholder_value, deviations in grouped_deviations.items():
    mean_deviations_by_placeholder[placeholder_value] = np.mean(deviations)

print("Mean deviation for each placeholder value:")
for placeholder_value, mean_deviation in mean_deviations_by_placeholder.items():
    print(f"Placeholder value {placeholder_value}: {mean_deviation}")


print((avDeviation)**2)
print((long_meanDeviations[2])**2)
print((mean_deviations_by_placeholder[22])**2)

"""About 20 absSum of the numbers in a expression is the same mean deviation as 4 numbers instead of 3.

benchmark for a model has to scale with:
*   MSE inside number range
*   MSE outside number range
*   MSE for longer expressions

the results of this FNN will be used as baseline.


MSE inside number range: 1457 expressions in x_test: 0.007094925

MSE outside number range: Placeholder value 22: 9.788455

Deviation for longer expressions: Deviation for expressions with 4 numbers: 9.551048


all 3 Values will be weighed equally as important.
"""

model_config = model.get_config()
print(model_config)
import pprint
pprint.pprint(model_config)

base_layers = model_config['layers']
input_layer = base_layers[0]
output_layer = base_layers[-1]
hidden_layer = [base_layers[1], base_layers[2]]
base_layers[1]

def build_model(input_shape, num_hidden_layers, units_per_layer):
  lil_model = keras.Sequential()
  lil_model.add(keras.Input(shape=input_shape))
  for i in range(num_hidden_layers):
    lil_model.add(layers.Dense(units_per_layer)),
    lil_model.add(PReLU())
  lil_model.add(layers.Dense(1, activation='linear'))
  lil_model.compile(optimizer="adam", loss="mse", metrics=["mae", "mse"])
  return lil_model

different_models = []
max_layers = 5
min_Neurons = 5
max_Neurons = 55
small_n_bootstrap = 5
input_shape = (len(x[0]),)
Neuron_step = 5

print(input_shape)


diff_predictions = []
diff_out_predictions = []
diff_long_predictions = []

for i in range(max_layers+1):
  for j in range(min_Neurons, max_Neurons+1,5):
    for p in range(small_n_bootstrap):
      sample_indices = np.random.choice(len(x_train), size=len(x_train), replace=True)
      x_train_bootstrap = x_train[sample_indices]
      y_train_bootstrap = np.array(y_train)[sample_indices]
      bootstrap_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_bootstrap, y_train_bootstrap)).batch(32)

      this_model = build_model(input_shape, i, j)
      different_models.append(this_model)

      this_model.fit(
          bootstrap_train_dataset,
          epochs=50,  # Reduced epochs for quicker example, adjust as needed
          verbose=0, # Suppress output
          callbacks=[early_stopping]
      )
      diff_predictions.append(this_model.predict(x_test))
      diff_out_predictions.append(this_model.predict(out_x_test))
      diff_long_predictions.append(this_model.predict(long_x_test))

diff_predictions = np.array(diff_predictions)
diff_out_predictions = np.array(diff_out_predictions)
diff_long_predictions = np.array(diff_long_predictions)

print(len(different_models))

diff_predictions.shape[1]

diff_meanPredictions = []
num_models = diff_predictions.shape[0] // small_n_bootstrap # 36
num_test_samples = diff_predictions.shape[1] # 1457

for model_index in range(num_models):
  model_predictions = diff_predictions[model_index * small_n_bootstrap : (model_index + 1) * small_n_bootstrap]
  # model_predictions now contains the 5 prediction sets for this specific model
  mean_prediction_for_model = np.mean(model_predictions, axis=0) # Average across the bootstrap axis
  diff_meanPredictions.append(mean_prediction_for_model)

diff_meanPredictions = np.array(diff_meanPredictions)
# diff_meanPredictions should now have shape (36, 1457)
print(diff_meanPredictions.shape)

diff_deviation = []
#diff_out_deviation = []

for j in range(diff_meanPredictions.shape[0]):
  calc = 0
  for i in range(len(y_test)):
    calc += abs(y_test[i]-diff_meanPredictions[j][i])
  diff_deviation.append(calc/len(y_test))
print(diff_deviation)

'''
for i in range(len(out_y_test)):
  out_deviation.append(abs(out_y_test[i]-out_meanPredictions[i]))
print(out_deviation)
out_avDeviation = np.mean(out_deviation)
print(np.mean(out_deviation))
'''

# prompt: plot diff_deviation on a graph

plt.figure(figsize=(10, 6))
plt.plot(diff_deviation)
plt.xlabel("Model Index")
plt.ylabel("Mean Absolute Deviation on Test Set")
plt.title("Mean Absolute Deviation for Different Models")
plt.show()

diff_out_meanPredictions = []
num_models = diff_out_predictions.shape[0] // small_n_bootstrap # 36
num_test_samples = diff_out_predictions.shape[1] # 1457

for model_index in range(num_models):
  model_predictions = diff_out_predictions[model_index * small_n_bootstrap : (model_index + 1) * small_n_bootstrap]
  # model_predictions now contains the 5 prediction sets for this specific model
  mean_prediction_for_model = np.mean(model_predictions, axis=0) # Average across the bootstrap axis
  diff_out_meanPredictions.append(mean_prediction_for_model)

diff_out_meanPredictions = np.array(diff_out_meanPredictions)
# diff_meanPredictions should now have shape (36, 1457)
print(diff_out_meanPredictions.shape)

diff_out_deviation = []
#diff_out_deviation = []
indices_with_placeholder_22 = [i for i, val in enumerate(placeholder) if val == 22]
print(indices_with_placeholder_22)
for j in range(diff_out_meanPredictions.shape[0]):
  calc = 0
  for i in indices_with_placeholder_22:
    calc += abs(out_y_test[i]-diff_out_meanPredictions[j][i])
  diff_out_deviation.append(calc/len(indices_with_placeholder_22))
print(diff_out_deviation)

plt.figure(figsize=(10, 6))
plt.plot(diff_out_deviation[6:])
plt.xlabel("Model Index")
plt.ylabel("Mean Absolute Deviation for Placeholder 22")
plt.title("Mean Absolute Deviation for Placeholder 22 for Different Models")
plt.show()

'''
#placeholder = absSum(outsideExpr)
#print(placeholder)
diff_paired = []
for i in range(len(diff_out_deviation)):
  diff_paired.append([diff_out_deviation[i], placeholder[i]])
diff_paired_sorted = sorted(diff_paired, key=lambda item: item[1])
print(diff_paired_sorted)

# Group deviations by placeholder value
diff_grouped_deviations = defaultdict(list)
for deviation, placeholder_value in diff_paired_sorted:
    diff_grouped_deviations[placeholder_value].append(deviation)

# Calculate the mean deviation for each placeholder value
diff_mean_deviations_by_placeholder = {}
for placeholder_value, deviations in diff_grouped_deviations.items():
    diff_mean_deviations_by_placeholder[placeholder_value] = np.mean(deviations)

print((diff_mean_deviations_by_placeholder[22])**2)
'''

diff_long_meanPredictions = []
num_models = diff_long_predictions.shape[0] // small_n_bootstrap # 36
num_test_samples = diff_long_predictions.shape[1] # 1457

for model_index in range(num_models):
  model_predictions = diff_long_predictions[model_index * small_n_bootstrap : (model_index + 1) * small_n_bootstrap]
  # model_predictions now contains the 5 prediction sets for this specific model
  mean_prediction_for_model = np.mean(model_predictions, axis=0) # Average across the bootstrap axis
  diff_long_meanPredictions.append(mean_prediction_for_model)

diff_long_meanPredictions = np.array(diff_long_meanPredictions)
# diff_meanPredictions should now have shape (36, 1457)
print(diff_long_meanPredictions.shape)

print(np.arange(100,200,1))
diff_long_deviations = []

for j in range(diff_long_meanPredictions.shape[0]):
  calc = 0
  for i in range(200,300):
    calc += abs(long_y_test[i]-diff_long_meanPredictions[j][i])
  diff_long_deviations.append(calc/100)
print(len(diff_long_deviations))

# prompt: plot of diff_long_deviations

plt.figure(figsize=(10, 6))
plt.plot(diff_long_deviations)
plt.xlabel("Model Index")
plt.ylabel("Mean Absolute Deviation for Expressions with 4 Terms")
plt.title("Mean Absolute Deviation for Expressions with 4 Terms for Different Models")
plt.show()

import seaborn as sns
num_neurons = (max_Neurons - min_Neurons) // Neuron_step + 1
diff_deviation_reshaped = np.array(diff_deviation).reshape(max_layers + 1, num_neurons)
plt.figure(figsize=(8, 6)) # Optional: Adjust figure size
sns.heatmap(diff_deviation_reshaped, annot=True, fmt=".2f", cmap="plasma",
            xticklabels=np.arange(min_Neurons, max_Neurons + 1, Neuron_step),
            yticklabels=np.arange(max_layers + 1))
plt.title('Simple Heatmap')
plt.show()

print((avDeviation)**2)
print((long_meanDeviations[2])**2)
print((mean_deviations_by_placeholder[22])**2)

baseline_deviation = (avDeviation)**2
baeline_out_deviation = (mean_deviations_by_placeholder[22])**2
baseline_long_deviation = (long_meanDeviations[2])**2

benchmark = []
for i in range(len(diff_deviation)):
  calc = 0
  calc += baseline_deviation / (diff_deviation[i]**2)
  calc += baeline_out_deviation / (diff_out_deviation[i]**2)
  calc += baseline_long_deviation / (diff_long_deviations[i]**2)
  benchmark.append(calc)
print(benchmark)


benchmark_array = np.array(benchmark).reshape(max_layers + 1, num_neurons)

# Create a new array to include the means
# Add a column for row means and a row for column means (and a corner for the overall mean)
extended_benchmark_array = np.zeros((max_layers + 2, num_neurons + 1))

Neurons_means = np.mean(benchmark_array, axis=0)
Layers_means = np.mean(benchmark_array, axis=1)

# Copy the original benchmark data
extended_benchmark_array[:max_layers + 1, :num_neurons] = benchmark_array

# Add the row means
extended_benchmark_array[:max_layers + 1, num_neurons] = Layers_means

# Add the column means
extended_benchmark_array[max_layers + 1, :num_neurons] = Neurons_means

# Calculate and add the overall mean
overall_mean = np.mean(benchmark_array)
extended_benchmark_array[max_layers + 1, num_neurons] = overall_mean


# Create updated tick labels for the heatmap
xticklabels_extended = list(np.arange(min_Neurons, max_Neurons + 1, Neuron_step)) + ['Row Mean']
yticklabels_extended = list(np.arange(max_layers + 1)) + ['Column Mean']


plt.figure(figsize=(14, 10)) # Adjust figure size for the extra row/column
sns.heatmap(extended_benchmark_array, annot=True, fmt=".2f", cmap="inferno",
            xticklabels=xticklabels_extended,
            yticklabels=yticklabels_extended)
plt.ylabel('Number of Layers')
plt.xlabel('Number of Neurons per Layer')
plt.title('Heatmap of the Benchmarks with Row and Column Means')
plt.show()

